% !TEX root = ../main.tex
\section{Algebra}
The field of abstract algebra can classically be defined as the study of sets with operations and the study of maps between such objects. I.e. an object of study in algebra would be a set with some non-empty collection of functions, which somehow act upon elements in the set. An example of such an operation would be a \textit{binary operation} on a set, $M$ say, i.e. a function taking a pair in $M\times M$ to an element in $M$. Such a pair of a set and a binary operation is called a \textit{magma}. It could also be that another set $S$ acts on $M$, i.e. there is a function taking a pair of elements in $S\times M$/$M\times S$ to an element in $M$. We call such a structure an \textit{$S$-act}{\LARGE DO WE?}. When we ask that such operation or the overlying sets adhere to certain axioms we obtain a rich family of sub-classes of objects having a certain kind of algebraic structure which will be preserved by certain maps. We will in the following be focusing broadly on the classes of rings and modules. However, it will be useful to also introduce groups and monoid in this context.\\ Before we begin, consider the following comment on the nature of much of this theory: There are often very few restrictions on the sets being considered in algebra while the operations will have many more restrictions. This means that to get from $A$ to $B$ in a proof it feels like one has to move through a very rigid structure that doesn't allow many choices or much creativity. Sometimes the right path from $A$ to $B$ will require small clever tricks, but ultimately the algebraic structure will dictate a fixed path for how a proof will go. This the opposite of what of what analysis feels like. In analysis one has a lot of freedom in constructing the functions, sequences, choice of $\epsilon$'s etc. that will do the trick, which can both result really clever and elegant uses solution or solutions that are less well thought out or elegant.\\
The creative aspect of algebra, is that of giving of giving good definitions and having a natural eye for the most natural constructions. With the right definition it can become clearer how a difficult question should be answered. Somehow what is important in algebra, lives among the objects of study and not among the elements in these object, hence the objective is more so to find the right (class of) object(s). This may not ever become very clear from these notes, but the hope is that some shadows of this fact(?) will be present.         
\subsection{Monoids}
\subsubsection{Definitions and Basic Properties}
\begin{definition}
    A \textit{monoid} is a set $M$ with an operation $\circ : M\times M \rightarrow M$ where $m_1m_2 := m_1 \circ m_2 := \circ(m_1,m_2)$ for $m_1,m_2\in M$ that satisfy the following two axioms
    \begin{enumerate}
        \item The operation $\circ$ satisfies the \textit{associative law}, i.e. for every $m_1,m_2,m_3\in M$, 
        $$m_1(m_2m_3) = (m_1m_2)m_3.$$
        \item There is an element $e\in M$ such that for every $m\in M$,
        $$me = em = m.$$
        The element $e$ is referred to as the \textit{neutral element with respect to $\circ$}. 
    \end{enumerate}
    The data specifying a monoid is often written as the tuple $(M,\circ)$.
\end{definition}
\begin{remark}\label{UniquenessOfNeutralElement}
    The neutral element with respect to $\circ$ is unique. Indeed, suppose $e,e'\in M$ are neutral with respect to $\circ$. Then 
    $$e = e e' = e'.$$
    For an element $m\in M$ and a non-negative integer $n$ we define 
    $$m^n = \underbrace{m\cdots m}_{n}$$
    with the convention that $m^0 = e$.
\end{remark}
\begin{definition}
    A \textit{commutative monoid} is a monoid $M$ such that for every $m_1,m_2\in M$,
    $$m_1m_2 = m_2m_1.$$
\end{definition}
\begin{definition}
    Let $(M,\circ)$ be a monoid. A subset $N\subset M$ is called a \textit{submonoid (of $M$)} if 
    \begin{enumerate}
        \item $e\in N$
        \item For every $n_1,n_2\in N$,
        $$n_1n_2\in N.$$
    \end{enumerate}
\end{definition}
\begin{remark}\label{ASubmonoidIsAMonoid}
     $(N,\circ|_N)$ is a monoid. Indeed, Since $n_1n_2\in N$ for every $n_1,n_2\in N$, the operation $\circ\mid_{N\times N} : N\times N\rightarrow N$ is well-defined. The operation $\circ$ is associative on $N$ since it is associative on $M$. By the definition of a submonoid $e\in N$ and again clearly the property of being the neutral element with respect to $\circ$ on $N$ is inherited by $e$ being so with respect to $\circ$ on $M$.  
\end{remark}
\begin{example}\label{SubMonoidsExample}
    \begin{enumerate}
        \item The non-negative integers $\N$ is a monoid with respect to addition and multiplication. 
        \item $(\Z,+),(\Z,\cdot),(\Q,+),(\Q,\cdot),(\R,+),(\R,\cdot),(\C,+),(\C,\cdot)$ are monoids.
        \item Let $A$ be a set. Consider $\text{Fun}(A,A) := \{f:A\rightarrow A\}$. This a monoid under function composition.
        \item Given a non-empty set $X$ and a monoid $M$ the set $$\Fun(X,M):=\left\{ f: X\rightarrow M\right\}$$
        with $fg\in \text{Fun}(X,M)$ defined by $fg(x) := f(x)g(x)$ for $f,g\in \text{Fun}(X,M)$ and $x\in X$
        with $fg\in\Fun(X,M) $ defined by $fg(x) = f(x)g(x)$. Indeed, given $f,g,h\in \Fun(X,R)$ and $x\in M$
        $$(fg)h(x)=(fg)(x)h(x)=(f(x)g(x))h(x)= f(x)(g(x)h(x))=f(x)(gh)(x)=f(gh)(x).$$
        And for the function $e : X \rightarrow M$, mapping every element in $X$ to $e_M$ we have that 
        $$ef(x) = e(x)f(x) = e_Mf(x) = f(x) \text{ and } fe(x)=f(x)e(x)=f(x)e_M = f(x).$$
        \item Let $M$ be a monoid. Then $M\subset M$ is a submonoid. 
        \item Let $M$ be a monoid. Then $\{e\}\subset M$ is a submonoid.
        \item Let $M$ be a monoid and $L\subset N\subset M$ be submonoids of $M$. Then $L$ is a submonoid of $N$. Similarly if $N\subset M$ is a submonoid and $L\subset N$ is a submonoid, then $L\subset M$ is a submonoid. 
        
    \end{enumerate}
\end{example}
\subsubsection{Morphisms of Monoids}
\begin{definition}
    Let $M,N$ be monoids. A \textit{monoid homomorphism/map of monoids/morphism of monoids} is a map $\rho : M \rightarrow N$ such that 
    \begin{enumerate}
        \item For every $m_1,m_2\in M$
        $$\rho(m_1m_2)=\rho(m_1)\rho(m_2).$$
        \item 
        $$\rho(e_M) = e_N.$$
    \end{enumerate}
    Denote the set of homomorphisms from $M$ to $N$ by $\Hom^{\text{Mon}}(M,N)$.
\end{definition}
\begin{remark}
    Let $\mathrm{Monoid}$ be the class of monoids and $\Hom^{\mathrm{Mon}}$ the class of monoid homomorphisms. One readily verifies that $\left(\mathrm{Monoid}, \Hom^\mathrm{Mon}\right)$ is a category. {\LARGE Potential to write more}. 
\end{remark}
\begin{remark}
    By a prior example (cf. Example~\ref{SubMonoidsExample}) we have seen that $\Fun(M,N)$ is a monoid. $\Hom^{\mathrm{Mon}}(M,N)\subset \Fun(M,N)$ is a submonoid if $N$ is commutative. Indeed, for $f,g\in \Hom^{\mathrm{Mon}}(M,N)$ and $x,y\in M$. Then 
    $$fg(xy)=f(xy)g(xy) = f(x)f(y)g(x)g(y) = f(x)g(x)f(y)g(y)=fg(x)fg(y)\implies fg\in \Hom^\mathrm{Mon}(M,N).$$
    Furthermore we have 
    $$e(xy)=e_M(xy)=xy=(e_Nx)(e_Ny)=e(x)e(y)\implies e\in \Hom^\mathrm{Mon}(M,N).$$
\end{remark}
\begin{lemma}\label{MonoidHomomorphismImageOfSubmonoidIsSubmonoid}
    Let $\rho : M\rightarrow N$ be a monoid homomorphism and $L\subset M$ a submonoid. Then $\rho(L) \subset N$ is a submonoid.   
\end{lemma}
\begin{proof}
    Let $\rho(l_1),\rho(l_2)\in \rho(L)$. Then since $l_1l_2\in L$,
    $$\rho(l_1)\rho(l_2)=\rho(l_1l_2)\in \rho(L).$$
    Clearly $e_N = \rho(e_M)\in \rho(L)$. 
\end{proof}
\begin{corollary}\label{ImageOfMonoidHomomorphismIsSubmonoid}
    The image of a monoid homomorphism $\rho:M\rightarrow N$ is a submonoid of $N$.
\end{corollary}
\begin{proof}
    This follows from the above lemma (cf. Example~\ref{SubMonoidsExample}). 
\end{proof}
\begin{definition}
    Let $\rho: M\rightarrow N$ be a monoid homomorphism. We define \textit{the kernel of $\rho$} to be the set 
    $$\ker \rho := \rho^{-1}(e_N) =\left\{ m\in M : \rho(m) = e_N\right\}\subset M$$
\end{definition}
\begin{lemma}\label{MonoidHomomorphismPreImageOfSubmonoidIsSubmonoid}
    Let $\rho: M\rightarrow N$ be a monoid homomorphism, and $L\subset N$ a submonoid. Then $\rho^{-1}(L)\subset M$ is a submonoid. 
\end{lemma}
\begin{proof}
    Let $m_1,m_2\in \rho^{-1}(L)$. Then since $\rho(m_1),\rho(m_2)\in L$,
    $$\rho(m_1m_2)=\rho(m_1)\rho(m_2)\in L,$$
    hence $m_1m_2\in \rho^{-1}(L)$. Since $\rho(e_M)=e_N \in L$, it follows that $\rho^{-1}(L)$ is a submonoid of $N$.
\end{proof}
\begin{corollary}\label{MonoidHomomorphismKernelIsSubmonoid}
    The kernel of a monoid homomorphism $\rho: M\rightarrow N$ is a submonoid of $M$.
\end{corollary}
\begin{proof}
    Since $\{e_N\}$ is a submonoid of $N$ it follows by the above lemma that $\ker\ \rho = \rho^{-1}(\{e_N\})$ (cf. Example~\ref{SubMonoidsExample}) is a submonoid.
\end{proof}
\begin{lemma}\label{SubmonoidOfCommutativeMonoidIsCommutativeMonoid}
    Let $M$ be a commutative monoid and $N\subset M$ a submonoid. Then $N$ is a commutative monoid.
\end{lemma}
\begin{proof}
    By Lemma~\ref{ASubmonoidIsAMonoid} $N$ is a monoid. Let $n_1,n_2\in N$, then since $n_1,n_2\in M$, $n_1n_2= n_2n_1$. 
\end{proof}
\begin{lemma}
    Let $\rho : M \rightarrow N$ be a monoid homomorphism. Let $L\subset M$ be a submonoid. If $M$ is commutative, then $\rho(L)\subset N$ is a commutative monoid. 
\end{lemma}
\begin{proof}
    Since $\rho(L)\subset N$ is a submonoid, it is a monoid. Let $\rho(l_1),\rho(l_2)\in \rho(L)$. Then since $L$ is a commutative by Lemma~\ref{SubmonoidOfCommutativeMonoidIsCommutativeMonoid} it follows that  
    $$\rho(l_1)\rho(l_2) = \rho(l_1l_2) = \rho(l_2l_1) = \rho(l_2)\rho(l_1).$$
\end{proof}
\subsubsection{Product Monoids \& Restricted Product of Monoids}
\begin{theorem}\label{DirectProductOfMonoidsIsAMonoid}
    Let $A$ be a set and $\{M_\alpha\}_{\alpha\in A}$ a family of monoids. We define a binary operation on the product $\prod_{\alpha\in A} M_\alpha$ by $(m_\alpha)(m_\alpha') = (m_\alpha m_\alpha')$ for $(m_\alpha),(m_\alpha')\in \prod_{\alpha\in A} M_\alpha$. With this operation $\prod_{\alpha\in A} M_\alpha$ becomes a monoid. If $M_\alpha$ is commutative for every $\alpha \in A$ so is $\prod_{\alpha\in A} M_\alpha$.
\end{theorem}
\begin{proof}
    We define $e := (e):= (e_\alpha)$ where $e_\alpha$ is the neutral element in $M_\alpha$ for each $\alpha$. Let $(m_\alpha),(m_\alpha'),(m_\alpha'')\in \prod_{\alpha\in A} M_\alpha$. We then have that
    \begin{align*}
        (m_\alpha)\left((m_\alpha') (m_\alpha'') \right) = (m_\alpha)(m_\alpha'm_\alpha'')  = (m_\alpha(m_\alpha'm_\alpha'')) = ((m_\alpha m_\alpha')m_\alpha'') =(m_\alpha m_\alpha')(m_\alpha'') = \left((m_\alpha)(m_\alpha')\right)(m_\alpha'')
    \end{align*}
    and that
    \begin{align*}
        e(m_\alpha) = (e)(m_\alpha) = (e_\alpha m_\alpha) = (m_\alpha) \text{ and } (m_\alpha)e= (m_\alpha)(e) = (m_\alpha e_\alpha) = (m_\alpha),
    \end{align*}
    hence $\left(\prod_{\alpha\in A} M_\alpha, \cdot \right)$ is a monoid. Suppose $M_\alpha$ is commutative for each $\alpha\in A$. Then 
    $$(m_\alpha)(m_\alpha') = (m_\alpha m_\alpha') = (m_\alpha' m_\alpha) = (m_\alpha)(m_\alpha').$$
\end{proof}
\begin{lemma}\label{ProductMonoidIsAProductInTheCategoryOfMonoids}
    Let $A$ be a set and $\{M_\alpha\}_{\alpha\in A}$ a family of monoids. Then 
    $$\pi_\beta : \prod_{\alpha\in A} M_\alpha \rightarrow M_\beta$$
    is monoid homomorphism. Given a monoid $N$ and a family monoid homomorphisms $\{f_\alpha : N\rightarrow M_\alpha\}_{\alpha\in A}$ then the unique map $f: N\rightarrow \prod_{\alpha\in A}$ (cf. Proposition~\ref{DirectProductIsADirectProductInTheCategoryOfSets}) such that $\pi_\alpha \circ f = f_\alpha$ for every $\alpha \in A$ is monoid homomorphism. 
\end{lemma}
\begin{proof}
    Let $(m_\alpha),(m_\alpha')\in \prod_{\alpha\in A} M_\alpha$ and fix $\beta \in A$. Then 
    $$\pi_\beta((m_\alpha)(m_\alpha'))= \pi_\beta((m_\alpha m_\alpha')) = m_\beta m_\beta' = \pi_\beta((m_\alpha))\pi_\beta((m_\alpha')).$$
    Lastly 
    $$\pi_\beta(e)=\pi_\beta((e_\alpha))=e_\beta.$$
    Let $n,n' \in N$. Then 
    $$f(nn') = (f_\alpha(nn'))=(f_\alpha(n)f_\alpha(n'))=(f_\alpha(n))(f_\alpha(n'))=f(n)f(n'),$$
    and 
    $$f(e_N)=(f_\alpha(e_N))=(e_\alpha)=e$$
\end{proof}
\begin{proposition}\label{CertainSubmonoidsOfProductMonoid}
    Let $A$ be a set and $\{M_\alpha\}_{\alpha\in A}$ a family of monoids. Consider a family of sets $\{N_\alpha\}_{\alpha\in A}$ such that $N_\alpha\subset M_\alpha$ is a submonoid for each $\alpha\in A$. Then $$\prod_{\alpha\in A} N_\alpha \subset \prod_{\alpha \in A} M_\alpha$$
    is a submonoid.
\end{proposition}
\begin{proof}
    Since $e_\alpha \in N_\alpha$ for each $\alpha \in A$. Then $e=(e_\alpha)\in \prod_{\alpha\in A} N_\alpha.$ Let $(n_\alpha),(n_\alpha')\in \prod_{\alpha\in A} N_\alpha$. Then since $n_\alpha n_\alpha'\in N_\alpha$ for each $\alpha \in A$. This implies that $(n_\alpha)(n_\alpha') = (n_\alpha n_\alpha')\in \prod_{\alpha\in A} N_\alpha$.
\end{proof}
\begin{example}
    Not every submonoid of a monoid arises in this fashion. For instance consider $N=\{(n,n)\in\N\times \N \}$ which is a proper submonoid of $(\N\times \N,+)$. Indeed, $0_{\N\times \N} = (0,0)\in N$ and if $(n_1,n_1),(n_2,n_2)\in N$, then $(n_1,n_1)+(n_2,n_2)=(n_1+n_2,n_1+n_2)\in N$. {\Large Show it is not product of submonoids}
\end{example}
\begin{remark}\label{BinaryOperationOverArbitrarySetIsWellDefinedWhenAllButFinitelyManyEntriesAreZero}
    With products introduced, at this point we will introduce some notation. Consider a monoid $M$. Let $A$ be a non-empty set, $\{M_\alpha\}$ a family of submonoids of $M$ and suppose we are given $(m_\alpha)\in \prod_{\alpha \in A} M_\alpha$ such that $m_\alpha = 0$ for all but finitely many $\alpha\in A$. Then there are $\alpha_1,\dots,\alpha_n\in A$ such that $m_\alpha=0$ for every $\alpha\in A\setminus\{\alpha_1,\dots,\alpha_n\}$. We then define 
    $$\prod_{\alpha\in A} m_\alpha := \prod_1^n m_{\alpha_i}.$$
    We first note that $\prod_{\alpha\in A} m_\alpha$ is an element of $M$. Suppose $\{\beta_1,\dots,\beta_m\}\subset A$ is another subset such that $m_\alpha = 0$ for all $\alpha \in A\setminus \{\beta_1,\dots,\beta_m\}$. If $m_\alpha = e$ for all $\alpha\in A$, then clearly 
    $$\prod_1^m m_{\beta_j} = e = \prod_1^n m_{\alpha_i}.$$
    If there is an $i\in \{1,\dots,n\}$ such that $m_{\alpha_i} \neq e$, then $\alpha_i = \beta_{j(i)}$ for some $j(i)\in \{1,\dots,m\}$, for if not, $\alpha_i \in X\setminus \{\beta_1,\dots,\beta_m\}$, which would imply $m_{\alpha_i} = e$. We can show that $i\mapsto j(i)$ is a bijection using the same argument for the non-zero $m_{\beta_j}$ to show that there is a $i(j)$ such that $\beta_j = \alpha_{i(j)}$. It then follows that 
    $$\prod_1^n m_{\alpha_i} = \prod_{i\in \{1,\dots,n\} : m_{\alpha_i} \neq e} m_{\alpha_i} = \prod_{i\in \{1,\dots,m\} : m_{\beta_i}\neq e} m_{\beta_i} = \prod_1^m m_{\beta_i},$$
    hence the notion is independent of the choice of the elements of $A$ corresponding to possibly non-zero entries of $(m_\alpha)$.\\
    A postemptive note after the above construction: The author of these notes, realizes that it is intuitively rather obvious, what is to be understood by $\prod_{\alpha\in A} m_\alpha$ and that it makes sense (is well-defined). It might it even be obvious - PERIOD! Somehow this construction just feels like a notational trick. If anyone should, by some weird coincidence, read these notes, note that the author being fixated on being (overly) precise in some instances, is a result of wanting to make sure that their understanding of what is going on, is precise AND EVEN FORMALISABLE - in some instances at least. The other instances where this seems not to be the case, it is either because the author doesn't care or that they have postponed it. Care is often given when the answer to the question seems easy enough to be done in LEAN. For example, if we knew what a monoid $M$ and $\prod_{\alpha \in A} \bullet$ is in LEAN, it seem rather easy(?) to prove that the function 
    $$\prod_{\alpha\in A} M_\alpha \rightarrow M, (m_\alpha)\mapsto \prod_{\alpha\in A} m_\alpha,$$
    is well-defined in a set-theoretic sense in LEAN or it should at least be easy to see how $\prod_{\alpha \in A} m_\alpha$ should be defined in LEAN from what has been written in this remark. 
\end{remark}
\begin{definition}
    Let $A$ be a set and $\{M_\alpha\}_{\alpha\in A}$ a family of monoids. We define the \textit{restricted direct product} of $M_\alpha$ over $A$ as the set 
    $$\prod_{\alpha\in A}' M_\alpha := \left\{(m_\alpha) \in \prod_{\alpha\in A} M_\alpha : m_\alpha = e_\alpha \text{ for all but finitely many } \alpha \in A \right\}$$
\end{definition}
\begin{lemma}\label{RestrictedProductMonoidIsAMonoid}
    Let $A$ be a set and $\{M_\alpha\}_{\alpha\in A}$ a family of monoids.  $\prod'_{\alpha\in A} M_\alpha$ is a submonoid $\prod_{\alpha \in A} M_\alpha$.
\end{lemma}
\begin{proof}
    Let $(m_\alpha),(m_\alpha')\in \prod'_{\alpha\in A} M_\alpha$. For some distinct $\alpha_1,\dots,\alpha_r\in A$ and $\beta_1,\dots,\beta_p\in A$, $m_\alpha = e_\alpha$ for every $\alpha \in A\setminus \{\alpha_1,\dots,\alpha_r\}$ and $m_\alpha' = e_\alpha$ for every $\alpha \in A \setminus \{\beta_1,\dots,\beta_p\}$. Then $m_\alpha m'_\alpha = e_\alpha$ for every $\alpha \in A\setminus \{\alpha_1,\dots,\alpha_r,\beta_1,\dots,\beta_p\}$ hence $(m_\alpha)(m'_\alpha)=(m_\alpha m'_\alpha) \in \prod'_{\alpha\in A} M_\alpha$. Clearly $e=(e_\alpha)\in \prod'_{\alpha\in A} M_\alpha$. 
\end{proof}
\subsection{Groups}
\subsubsection{Definition \& Basic Properties}
\begin{definition}
    A \textit{group} is a monoid $(G,\circ)$ where for every $g\in G$ there is an element $g^{-1}\in G$ such that
    $$gg^{-1}=g^{-1}g = e.$$
    For $g\in G$ we refer to $g^{-1}$ as the \textit{inverse of $g$ with respect to $i$}. The data specifying a group is also often written as the tuple $(G,\circ)$.
\end{definition}
\begin{remark}
    For an element $g\in G$ and a non-negative integer $n$, we define $g^{-n} = \left(g^{-1}\right)^n$. It is easy to check that $\left( g^n\right)^{-1}=g^{-n}$.
\end{remark}
\begin{definition}
    A group $(G,+)$ is called \textit{abelian} or \textit{additive}, if it is also a commutative monoid. We denote the inverse of $g\in G$ with respect to addition by $-g$, and for $g_1,g_2\in G$ we define 
    $$g_1-g_2 := g_1+(-g_2).$$
    and $ng_1 = \underbrace{g_1+\dots + g_1}_{n\text{ times}}$
\end{definition}
\begin{lemma}\label{LeftAndRightMultiplicationIsInjective}
    Let $(G,\circ)$ be a group. Let $g,g',a\in G$. If $ag = ag'$, then $g=g'$. Similary, if $ga=g'a$, then $g=g'$.
\end{lemma}
\begin{proof}
    We have that $(a^{-1},ag)=(a^{-1},ag')$, hence 
    $$g = eg = (a^{-1}a)g = a^{-1}(ag) = a^{-1}(ag') = (a^{-1}a)g' = eg' = g.$$
    The proof of the other statement is dual. 
\end{proof}
\begin{lemma}\label{InverseElementResults}
    Let $(G,\circ)$ be a group. The following is true
    \begin{enumerate}
        \item Inverse elements are unique
        \item For every $g,g'\in G$,
        $$(gg')^{-1} = g'^{-1}g^{-1}$$
        \item For every $g\in G$, 
        $$(g^{-1})^{-1} = g$$
    \end{enumerate}
\end{lemma}
\begin{proof}
    1. Let $g\in G$ and consider $g',g''$ such that $g'g=gg'=e$ and $g''g=gg''=e$. Then 
    $$gg' = e = gg'',$$
    hence $g' =g''$ by the prior lemma.\\ 
    2. One easily check that both $(gg')^{-1}$ and $g'^{-1}g^{-1}$ are inverse elements of $gg'$. It then follows from 1. that $(gg')^{-1} = g'^{-1}g^{-1}$.\\
    3. 
    One easily sees that $\left(g^{-1}\right)^{-1}$ and $g$ are inverse elements of $g^{-1}$. It follows from 1. that $\left(g^{-1}\right)^{-1}=g$.
\end{proof}
\begin{remark}
    One should note that if we in 1. for $g\in G$ only proved that elements $g'\in G$ satisfying $gg' = e$ were unique, this would still be sufficient to prove 2. and 3. This in addition means that if $gg^{-1} =e$ then 
    $$g^{-1}g = g^{-1}\left(g^{-1}\right)^{-1} = (g^{-1}g)^{-1} = e.$$
    Since the first statement in Lemma~\ref{LeftAndRightMultiplicationIsInjective} only uses $eg = g$ for every $g\in G$ and we only ever make use first statement of this lemma in 1. then we can prove that that if $eg = g$ for every $g\in G$, then
    $$ge = \left(g^{-1}\right)^{-1} \left(e^{-1}\right)^{-1} =\left(e^{-1}g^{-1}\right)^{-1} = \left(e g^{-1}\right)^{-1} = \left( g^{-1}\right)^{-1} =g,$$
    for every $g\in G$.
    In other words it is sufficient to check that $eg = g$ and $gg^{-1} = e$ for every $g\in G$, when checking the group axioms under the assumption that axiom 1. is already fulfilled.
\end{remark}
\begin{definition}
    Let $G$ be a group. A subset $H\subset G$ is called a \textit{subgroup} if it is a submonoid of $G$ and for every $h \in H$ we have that $h^{-1} \in H$.
\end{definition}
\begin{remark}\label{ASubgroupIsAGroup}
    $(H,\circ)$ is a group. Indeed, $(H,\circ)$ is a monoid since $H\subset G$ is a submonoid. Since $h^{-1}\in H$ for every $h\in H$, it follows that every element in $H$ has in inverse in $H$, hence $H$ is a group.
\end{remark}
\begin{example}
    \begin{enumerate}
        \item For $n\in \Z$, the set $n\Z := \{nm : m\in \Z\}$ is a subgroup of $(\Z,+)$.
        \item $\R\subset \C$ is a subgroup of $(\C,+)$. $\R\setminus \{0\} \subset \C\setminus \{0\}$ is a subgroup of $(\C\setminus \{0\},\cdot)$
        \item Let $G$ be a group. Then $G$ and $\{e\}$ are subgroups of $G$.
        \item Given a set $A$. The set of invertible maps $A\rightarrow A$ forms a submonoid of $\Fun(A,A)$ under composition, furthermore it is a group, when picking the inverse elements to be inverse maps and the neutral to be the identity on $A$. 
        \item Given a non-empty set $X$ and a group $G$ the monoid $\Fun(X,G)$ forms a group. Indeed for $f\in \Fun(X,G)$, define $f^{-1}:X\rightarrow G$ by $f^{-1}(x) = \left(f(x)\right)^{-1}$. Then 
        $$ff^{-1}(x) = f(x)\left(f(x)\right)^{-1}= e_N = e(x).$$
        \item Let $G$ be a group and consider $I\subset H \subset G$. Then $I,H\subset G $ are subgroups if and only if $I\subset H$ and $H\subset G$ are subgroups.
        \item Let $X$ be any non-empty set and $G$ a group. The set 
    \end{enumerate}
\end{example}
\begin{definition}
    Let $G$ be a group and $S\subset G$. Then we define \textit{the subgroup generated by $S$} to be the set 
    $$\langle S \rangle = \left\{ s_1^{v_1}\cdots s_n^{v_n}\in G : n\geq 1, s_1,\dots,s_n\in S, v_1,\dots,v_n\in \{\pm 1\}\right\}.$$
    Our convention will be that $\langle \emptyset\rangle = \{e\}$
\end{definition}
\begin{remark}
    Disallowing negative exponents in $\langle S\rangle$ gives the definition of \textit{the submonoid generated by $S$}. From the following, we can derive that this is a submonoid even if we allow $G$ to just be a monoid. If $S$ is empty, it is clearly a subgroup. So suppose $S$ is non-empty. Let $s_1^{v_1}\cdots s_n^{v_n}, t_1^{w_1}\cdots t_m^{w_m} \in \langle S\rangle$. Then if we define $s_i = t_{i-n}$ and $v_i = w_{i-n}$ for $i\in\{n+1,\dots,n+m\}$. Then 
    $$s_1^{v_1}\cdots s_n^{v_n}t_1\cdots^{w_1}\cdots t_m^{w_m} = s_1^{v_1}\cdots s_{n+m}^{v_{n+m}} \in \langle S\rangle$$
    Clearly $e \in \langle S\rangle$. We also have 
    $$\left(s_1^{v_1}\cdots s_n^{v_n} \right)^{-1} = s_n^{-v_n}\cdots s_1^{-v_1} \in \langle S\rangle.$$
    It follows that $\langle S \rangle$ is a subgroup. Let $H\subset G$ be a subgroup containing $S$. Then clearly $s_1^{v_1}\cdots s_n^{v_n}\in H$. Thus $\langle S\rangle$ is the smallest subgroup containing $S$.\\
    $S,T\subset G$ such that $S\subset T \subset G$. Then $\langle S\rangle \subset \langle T\rangle$. If $H\subset G$ is a subgroup, then $\langle H\rangle = H$, since $H$ is the smallest subgroup containing $H$.
\end{remark}
\begin{definition}
    A group $G$ is \textit{finitely generated} if $G= \langle g_1,\dots,g_n\rangle$ for some $g_1,\dots,g_n\in G$. If $G$ is generated by one element it is called \textit{cyclic}.
\end{definition}
\begin{lemma}\label{EverySubgroupOfCyclicGroupIsCyclic}
    Let $G$ be a cyclic group. Then any subgroup of $G$ is cyclic.
\end{lemma}
\begin{proof}
    Let $H\subset G$ be a subgroup. If $H=\{e\}$ we are done, so suppose it is not. We have that $G = \langle g \rangle$ for some $g $. The set $\{n>0 : g^n \in H\}$ is non-empty and thus have a minimum by the well-ordering of the natural numbers. Call this number $m$. We claim that $\left\langle g^m\right\rangle = H$. The first inclusion is trivial. Let $h \in H$. Then $h= g^l$ for some $l\in \Z$. It is sufficient to check that $h\in \left\langle g^m\right\rangle $ the case where $l>0$, so we assume this. By minimality $l\geq m$. Then $h = g^l= g^{qm+r}$ for some $q,r\geq 0$ and $r< m$. Then $g^r= g^{qm+r-qm} = q^{qm+r}q^{-qm} \in H$, hence by minimality $r =0$, hence $h = g^{qm} \in \left\langle g^m\right\rangle $
\end{proof}
\subsubsection{Morphisms of groups}
\begin{definition}
    Let $G,H$ be groups. A map $\rho : G\rightarrow H$ is called a \textit{group homomorphism/map of groups/morphism of groups}, if for every $g_1,g_2\in G$,
    $$\rho(g_1g_2)=\rho(g_1)\rho(g_2).$$
    Denote the set of group homomorphism between $G$ and $H$ by $\Hom^\text{Grp}(G,H)$
\end{definition}
\begin{remark}\label{GroupHomomorphismSendsInversesToInversesAndNeutralElementsToNeutralElements}
    \begin{enumerate}
    \item Denote the neutral elements of $G$ and $H$ by $e_G$ and $e_H$ respectively. Then $\rho(e_G)=e_H$. Indeed, 
    $$\rho(e_G)e_H=\rho(e_Ge_G)=\rho(e_G)\rho(e_G),$$
    hence by Lemma~\ref{LeftAndRightMultiplicationIsInjective}, $e_H= \rho(e_G)$.\\ 
    We also have that $\rho\left(g^{-1}\right) = \rho(g)^{-1}$. Indeed,
    $$\rho(g)\rho(g)^{-1} = e_H = \rho(e_G)=\rho\left(gg^{-1}\right)=\rho(g)\rho\left(g^{-1}\right),$$
    hence by uniqueness of inverse elements $\rho\left(g^{-1}\right) = \rho(g)^{-1}$. Thus a group homomorphism is a monoid homomorphism.
    \item Suppose $H$ is commutative. Let $\rho \in \Hom^\text{Grp}(G,H)$ and $x\in M$. Then 
    $$\rho(x)\left(\rho(x)\right)^{-1}=e_N = e(x),$$
    implying that $\Hom^\text{Grp}(G,H)\subset \Fun(G,H)$ is a subgroup. Let $f\in \Hom^\text{Grp}(G,H)$ and $x,y\in G$, then 
    $$f^{-1}(xy) = \left(f(xy)\right)^{-1} = \left(f(x)f(y)\right)^{-1} = \left(f(x)\right)^{-1}\left(f(y)\right)^{-1} = f^{-1}(x)f^{-1}(y) \implies f^{-1}\in \Hom^\text{Grp}(G,H).$$
    \end{enumerate}
\end{remark}
The following lemma follows directly from Lemmas~\ref{MonoidHomomorphismImageOfSubmonoidIsSubmonoid} and \ref{MonoidHomomorphismPreImageOfSubmonoidIsSubmonoid}
\begin{lemma}\label{KernelAndImageOfGroupHomomorphismIsSubgroup}
    Let $\rho : G\rightarrow H$ be a group homomorphism and $I\subset G$, $J\subset H$ be subgroups. Then $\rho(I)\subset H$ and $\rho^{-1}(J)\subset G$ are subgroups. In particular, the kernel and image of a $\rho$ are subgroups of $G$ and $H$ respectively. 
\end{lemma}
\begin{proof}
    By Lemma~\ref{MonoidHomomorphismImageOfSubmonoidIsSubmonoid} and Lemma~\ref{MonoidHomomorphismPreImageOfSubmonoidIsSubmonoid} both sets in question are submonoids of $H$ and $G$ respectively. Let $g\in \rho^{-1}(J)$. Then by Remark~\ref{GroupHomomorphismSendsInversesToInversesAndNeutralElementsToNeutralElements},
    $$\rho\left(g^{-1}\right)  = \rho(g)^{-1} \in J \implies g^{-1}\in \rho^{-1}(J),$$
    hence $\rho^{-1}(J)$ is a subgroup of $G$. Let $\rho(i)\in \rho(I)$. Then by Remark~\ref{GroupHomomorphismSendsInversesToInversesAndNeutralElementsToNeutralElements}
    $$\rho(i)^{-1}=\rho\left(i^{-1}\right) \in \rho(I),$$
    hence $\rho(I)$ is a subgroup of $H$.
\end{proof}
\subsubsection{Product Groups, Direct Sums \& and Other Enumerated Constructions}
\begin{definition}
    Let $A$ be a set and $\{G_\alpha\}$ a family of additive groups. We define the \textit{direct sum of $G_\alpha$ over $A$} as 
    $$\bigoplus_{\alpha\in A} G_\alpha = \prod'_{\alpha\in A} G_\alpha.$$
\end{definition}
\begin{remark}\label{SumsOfInfinitelyOftenZeroGroupElements}
    Let $A$ be a set and $\{G_\alpha\}$ a family of subgroups of $G$. Then 
    $$\sum_{\alpha\in A} g_\alpha \in G,$$
    for $(g_\alpha)\in \prod_{\alpha\in A}'$ is a well-defined construction (cf. Remark~\ref{BinaryOperationOverArbitrarySetIsWellDefinedWhenAllButFinitelyManyEntriesAreZero})
\end{remark}
\begin{lemma}\label{DirectProductRestrictedDirectProductOfGroupsAreGroups}
    Let $A$ be a set and $\{G_\alpha\}_{\alpha \in A}$ a family of groups. The direct product of $G_\alpha$ over $A$ is a group. If each $G_\alpha$ is additive, then so is the direct product. The restricted direct product is a subgroup of the direct product, hence the direct sum is an additive group. 
\end{lemma}
\begin{proof}
    All of these constructions are monoids by Theorem~\ref{DirectProductOfMonoidsIsAMonoid} and Lemma~\ref{RestrictedProductMonoidIsAMonoid} is follows that the direct product is a monoid, that when the groups are additive that this is also the case for the direct product and lastly the restricted direct product is a submonoid of the product monoid, hence also the direct sum when the groups are additive. For the first statement it thus suffices to check that each element of $\prod_{\alpha\in A} G_\alpha$ has an inverse. Let $(g_\alpha)\in \prod_{\alpha\in A} G_\alpha$. We define $(g_\alpha)^{-1}:= (g_\alpha^{-1})$. It then follows that 
    $$(g_\alpha)^{-1}(g_\alpha) = (g_\alpha^{-1})(g_\alpha) = (g_\alpha^{-1}g_\alpha) = (e_\alpha)= e.$$
    For the last two statements it suffices to check that $\prod_{\alpha \in A}' G_\alpha$ is closed under inversion of elements. Let $(g_\alpha ) \in \prod_{\alpha \in A}' G_\alpha.$  Then there $g_\alpha = e_\alpha$ for each $\alpha \in A\setminus B$ for some finite subset $B$ of $A$. Hence for $\alpha \in A\setminus B$, $g_\alpha^{-1} = e_\alpha$. It follows that $(g_\alpha)^{-1} =\left(g_\alpha^{-1}\right)$.
\end{proof}
\begin{proposition}\label{ProductGroupIsProductInCategoryOfGroups}
    Let $A$ be a set and $\{G_\alpha\}_{\alpha \in A}$ a family of groups.Then 
    $$\pi_\beta : \prod_{\alpha\in A} G_\alpha \rightarrow G_\beta$$
    is group homomorphism. Given a group $H$ and a family of group homomorphisms $\{f_\alpha : H\rightarrow G_\alpha\}_{\alpha\in A}$ then the unique group homomorphism $f: H\rightarrow \prod_{\alpha\in A} G_\alpha$ (cf. Lemma~\ref{ProductMonoidIsAProductInTheCategoryOfMonoids}) such that $\pi_\alpha \circ f = f_\alpha$ for every $\alpha \in A$ is group homomorphism. 
\end{proposition}
\begin{proof}
    $\pi_\beta$ and $f$ being monoid homomorphism they are automatically group homomorphisms.
\end{proof}
\begin{proposition}\label{CertainSubgroupsOfProductGroup}
    Let $A$ be a set and $\{G_\alpha\}_{\alpha\in A}$ a family of groups. Consider a family of sets $\{H_\alpha\}_{\alpha\in A}$ such that $H_\alpha\subset G_\alpha$ is a subgroup for each $\alpha\in A$. Then $$\prod_{\alpha\in A} H_\alpha \subset \prod_{\alpha \in A} G_\alpha$$
    is a subgroup.
\end{proposition}
\begin{proof}
    By Proposition~\ref{CertainSubmonoidsOfProductMonoid} it follows that $\prod_{\alpha\in A} H_\alpha$ is a submonoid. It thus suffices to check that it is closed under inversion of elements. Let $(h_\alpha)\in\prod_{\alpha\in A} H_\alpha$. Then $h_\alpha^{-1}\in H_\alpha$ for each $\alpha \in A$, hence $(h_\alpha)^{-1} = \left( h_\alpha^{-1} \right)\in \prod_{\alpha\in A} H_\alpha$.
\end{proof}
\begin{proposition}
    Let $G$ be an additive group and $H\subset G$ a subgroup. Then $H$ is an additive subgroup.
\end{proposition}
\begin{proof}
    This follows from Lemma~\ref{ASubgroupIsAGroup} and Lemma~\ref{SubmonoidOfCommutativeMonoidIsCommutativeMonoid}.
\end{proof}
\begin{lemma}
    Let $\rho : G\rightarrow H$ be a group homomorphism where $G$ is an abelian group and $J\subset G$ be a subgroup. Then $\rho(J)$ is an abelian group
\end{lemma}
\begin{proof}
    Since $\rho(J)\subset H$ is a subgroup it is a group. It remains to check that $\rho(J)$ is abelian. Let $\rho(g_1)\rho(g_2)\in \rho(J)$. Then 
    $$\rho(g_1)\rho(g_2)= \rho(g_1g_2)= \rho(g_2g_1) = \rho(g_2)\rho(g_1).$$
\end{proof}
\begin{proposition}\label{IntersectionsOfSubgroupIsSubgroup}
    Let $A$ be a set, $G$ a group and $\{H_\alpha\}_{\alpha\in A}$ be a family of subgroups of $G$. Then 
    \begin{gather*}
        \bigcap_{\alpha\in A} H_\alpha 
    \end{gather*}
    is a subgroup of $G$
\end{proposition}
\begin{proof}
    Clearly $e\in H_\alpha$ for each $\alpha\in A$, hence $e\in \bigcap_{\alpha\in A} H_\alpha$. Fix a $\beta\in A$. Let $h,h'\in \bigcap_{\alpha\in A} H_\alpha$. Then $hh',h^{-1}\in H_\alpha$ for each $\alpha \in A$, hence $hh',h^{-1}\in \bigcap_{\alpha\in A} H_\alpha$.
\end{proof}
\begin{proposition}\label{CanonicalGroupHomomorphismBetweenDirectSumAndSum}
    Let $A$ be a set, $G$ an additive group and $\{H_\alpha\}_{\alpha\in A}$ be a family of subgroups of $G$.
    \begin{gather*}
        s : \bigoplus_{\alpha\in A} H_\alpha \rightarrow G\\
        (h_\alpha) \mapsto \sum_{\alpha \in A} h_\alpha
    \end{gather*}
\end{proposition}
\begin{proof}
    Let $(h_\alpha),(h_\alpha')\in \bigoplus_{\alpha\in A} H_\alpha$.
    For suitable $\{\alpha_1,\dots,\alpha_n\}\subset A$, $h_\alpha = h_\alpha'=0$ and hence $h_\alpha+h_\alpha' = 0$ for every $\alpha\in A\setminus \{\alpha_1,\dots,\alpha_n\}$. We thus find that
    \begin{align*}
        s((h_\alpha)+(h_\alpha')) &= s((h_\alpha+h_\alpha'))= \sum_{\alpha\in A}\left( h_{\alpha_i} + h_{\alpha_i}'\right) = \sum_1^n\left( h_{\alpha_i} + h_{\alpha_i}'\right)
        = \sum_1^n h_{\alpha_i} +\sum_1^n h_{\alpha_i}'\\
        &= \sum_{\alpha \in A} h_\alpha + \sum_{\alpha\in A} h_\alpha' = s((h_\alpha)) + s((h_\alpha')).
    \end{align*}
\end{proof}
\begin{definition}
    Let $A$ be a set, $G$ an additive group and $\{H_\alpha\}_{\alpha\in A}$ be a family of subgroups of $G$. We define \textit{the sum of $H_\alpha$ over $A$} set 
    $$\sum_{\alpha \in A} H_\alpha := s\left(\bigoplus_{\alpha\in A} H_\alpha\right) =  \left\{ \sum_{\alpha\in A} h_\alpha : (h_\alpha)\in \bigoplus_{\alpha\in A} H_\alpha \right\},$$
    which by the above proposition and Lemma~\ref{KernelAndImageOfGroupHomomorphismIsSubgroup} is a subgroup of $G$. 
\end{definition}
\begin{remark}\label{SumOfSubgroupsIsSubgroupGeneratedByUnion}
    \begin{enumerate}
    \item The kernel of $s$ is contained in 
    $$\left\{ (h_\alpha)\in \bigoplus_{\alpha\in A} : h\alpha \in H_\alpha \cap \sum_{\beta\in A\setminus\{\alpha\}} H_\beta \text{ for each } \alpha\in A\right\}.$$
    Indeed, let $(h_\alpha)\in \ker \ s$. Then $\sum_1^n h_{\alpha_i} = \sum_{\alpha} h_\alpha = 0$. Let $\alpha \in A$. If $h_\alpha =0$, then it is trivially in $H_\alpha \cap \sum_{\beta\in A\setminus\{\alpha\}} H_\beta$. Otherwise $\alpha = \alpha_i$ for some $i\in\{1,\dots,n\}$. Then $h_{\alpha_i}=\sum_{\beta\in A\setminus \{\alpha_i\}} h_\alpha \in \sum_{\beta\in A\setminus\{\alpha_i\}} H_\beta$, hence $h_{\alpha_i}\in H_{\alpha_i}\cap \sum_{\beta\in A\setminus\{\alpha_i\}} H_\beta$.
    \item One should note that $$\sum_{\alpha\in A} H_\alpha = \left\langle \bigcup_{\alpha\in A} H_\alpha \right\rangle.$$
    Indeed, $h_\alpha\in \bigcup_{\alpha\in A} H_\alpha$ for every $\alpha\in A$, hence 
    $$\sum_{\alpha\in A} h_\alpha = \sum_1^n h_\alpha \in \left\langle \bigcup_{\alpha\in A} H_\alpha \right\rangle.$$
    Let $\sum_1^n m_ih_{\alpha_i}\in \left\langle \bigcup_{\alpha\in A} H_\alpha \right\rangle$ where $m_i\geq 0$, $\alpha_i\in A$, $h_{\alpha_i}\in H_{\alpha_i}$. For each $i$ we then have that 
    $$m_ih_{\alpha_i} = \sum_{j=1}^{m_i} h_{\alpha_i}\in H_{\alpha_i},$$
    Hence upon putting $h_\alpha' = 0$ for $\alpha\in A\setminus\{\alpha_1,\dots,\alpha_n\}$ and $h_{\alpha_i}' = m_ih_{\alpha_i}$, implying 
    $$\sum_1^n m_ih_{\alpha_i} = \sum_{\alpha\in A} h_\alpha'\in \sum_{\alpha\in A} H_\alpha.$$
    \end{enumerate}
\end{remark}
\subsubsection{Quotient Groups}
\begin{definition}
    Let $G$ be a group and $X,Y\subset G$ we then define 
    $$XY := \circ (X\times Y) = \left\{xy \in G : (x,y)\in X\times Y \right\},$$
    and 
    $$X^{-1} := i(X) = \left\{ x^{-1} \in G : x \in X \right\}.$$
\end{definition}
\begin{remark}\label{AssociativityOfMultiplicationSets}
    One easily sees for $X,Y,Z\subset G$ that $X(YZ) = (XY)Z$ and that $(XY)^{-1} = Y^{-1}X^{-1}$.
\end{remark}
\begin{definition}
    Let $G$ be a group, $H\subset G$ be a subgroup and $g\in G$. The \textit{left coset of $H$ with respect to $g$} is defined to be the set 
    $$gH := \{g\}H =  \left\{ gh : h\in H \right\}$$
    The \textit{right coset of $H$ with respect to $g$} is defined to be the set
    $$Hg := H\{g\} =  \left\{ hg : h\in H\right\}.$$
\end{definition}
\begin{remark}\label{StabOfSubgroupIsSubgroup}
    Note that clearly $hH = H = Hh$ for any $h\in H$. If $gH \neq H$, then $gh \notin H$ for some $h\in H$, meaning $g \notin H$. Since $eg = g = ge$, it also follows that $g\in gH$ and $g\in Hg$ for every $g\in G$. It is also easy to check that $H^{-1}=H$.
\end{remark}
\begin{proposition}
    Let $G$ be a group and $H\subset G$ a subgroup. Then the sets 
    $$\sim_l := \left\{ (g_1,g_2)\in G\times G : g_1H = g_2H \right\} \ \& \ \sim_r := \left\{ (g_1,g_2)\in G\times G : Hg_1 = Hg_2 \right\}$$
    define equivalence relations. We define $G/H := G/\sim_l$ and $G\setminus H := G/\sim_r$.
\end{proposition}
\begin{proof}
    We only check the left case, since the right case is dual. Let $g_1,g_2,g_3\in G$. Obviously $g_1H = g_1H$, hence $g_1\sim_l g_1$. Suppose $g_1\sim_l g_2$. Then $g_1H = g_2H$, hence $g_2H=g_1H$, meaning $g_2\sim_l g_1$. Suppose $g_1\sim_l g_2$ and $g_2\sim_l g_3$. Then $g_1H = g_2H = g_3H$, hence $g_1\sim_l g_3$. 
\end{proof}
\begin{lemma}
    Let $G$ be a group and $H\subset G$. Then for $g,g'\in G$
    $$g\sim_l g' \iff g^{-1}g'\in H.$$
\end{lemma}
\begin{proof}
    Indeed, 
    \begin{gather*}
        g \sim_l g' \iff gH = g'H \iff g^{-1}(gH) = g^{-1}(g'H) \iff  H = \left(g^{-1}g \right)H = \left(g^{-1}g'\right)H\\ \iff g^{-1}g'\in H, 
    \end{gather*}
    where the last bi-implication follows from Remark~\ref{StabOfSubgroupIsSubgroup}.
\end{proof}
\begin{proposition}
    A group $G$ with a subgroup $H\subset $ is the disjoint union of the elements of $G/H$ respectively $G\setminus H$.
\end{proposition}
\begin{proof}
    We check the left case. The right case is dual. Since $g\in gH$ for every $g\in G$, it follows that 
    $$G= \bigcup_{gH \in G/H} gH.$$
    Let $g_1,g_2\in G$. Suppose $g_1H\cap g_2H \neq \emptyset$. Then there is an element $x\in g_1H\cap g_2H$, hence $g_1h_1 = x =g_2h_2$ for suitable $h_1,h_2\in H$. This implies that 
    $$g_1^{-1}g_2 = h_1h_2^{-1} \in H \implies g_1H = g_2H.$$
    Thus if $g_1H \neq g_1H$, then $g_1H\cap g_2H = \emptyset$.
\end{proof}
\begin{definition}
    A subgroup $H\subset G$ is \textit{normal} if 
    $$g N g^{-1} = N$$
    for every $g\in G$.
\end{definition}
\begin{remark}\label{ReformulationOfNormalSubgroupAndAllSubgroupsOfAbelianGroupsAreNormal}
    Note that 
    $$gNg^{-1} = N \iff gN = gN(g^{-1}g)=(gNg^{-1})g = Ng,$$
    Hence any subgroup of an abelian group is normal. Furthermore $\sim_l = \sim_r$. Thus we may define $\sim = \sim_l= \sim_r$.
    Let $X\subset G$. Then $XN = NX$. Indeed, if $xn \in XN$, then $xn \in xN=Nx$, hence $XN\subset NX$. The other inclusion is shown in a similar way. 
\end{remark}
\begin{lemma}\label{KernelIsNormalSubgroup}
    The kernel of a group homomorphism $\rho : G \rightarrow H$ is a normal subgroup of $G$.
\end{lemma}
\begin{proof}
    Let $g\in G$ and $k \in \ker \ \rho$. Then 
    $$\rho\left(gkg^{-1}\right) = \rho(g)\rho(k)\rho(g)^{-1} = \rho(g)e\rho(g)^{-1} = e,$$
    hence $g(\ker \ \rho)g^{-1}\subset \ker \ \rho$. Conversely $k\in g (\ker \ \rho) g^{-1}$ since $k = g\left(g^{-1}kg\right)g^{-1}$ and $g^{-1}kg\in \ker \ \rho$ by the above computation.
\end{proof}
\begin{proposition}
    Let $G$ be a group and $H,N\subset G$ be subgroup, where $N$ is normal. Then $HN,NH\subset G$ are subgroups of $G$. 
\end{proposition}
\begin{proof}
    Let $h_1n_1,h_2n_2\in HN$. Then 
    $$h_1n_1h_2n_2 \in (HN)(HN)=(NH)(HN) = (N(HH))N=(NH)N=(HN)N=H(NN) = HN. $$
    Since $e\in H$ and $e\in N$, $ee\in HN$. Thus $HN$ is a subgroup of $G$. Since $HN=NH$, it follows that $NH$ is also a subgroup of $G$. 
\end{proof}
\begin{proposition}
    Let $G$ be a group and $H\subset G$ a normal subgroup. Then the operation
    \begin{align*}
        \cdot : G/H \times G/H \rightarrow G/H
    \end{align*}
    given by $(g_1H)(g_2H) := g_1g_2H := (g_1g_2)H$ for $g_1H,g_2H\in G/H$ is well-defined and $(G/H,\cdot)$ is a group. 
\end{proposition}
\begin{proof}
    We first need to check that the group operation is well-defined. Let $g_1,g_2\in G$. We need to check that $(g_1H)(g_2H) = (g_1g_2) H$. Let $g_1h_1g_2h_2\in g_1Hg_2H$. Then $h_1g_2\in Hg_2 = g_2H$, hence $h_1g_2h_2\in g_2Hh = g_2H$, hence $g_1(h_1g_2h_2) = g_1(g_2H)=(g_1g_2)H.$ If $g_1g_2h \in g_1g_2H$, then $g_1g_2h = g_1eg_2h\in (g_1H)(g_2H)$. Thus the operation is well-defined, since if $(g_1H,g_2H)=(g_1'H,g_2'H)$, then trivially 
    $$(g_1g_2)H=(g_1H)(g_2H) =(g_1'H)(g_2'H) = (g_1'g_2')H.$$
    For $g_1H,g_2H,g_3H\in G/H$, it follows by Remark~\ref{AssociativityOfMultiplicationSets} that 
    $$(g_1H)((g_2H)(g_3H)) = (g_1Hg_2H)(g_3H).$$
    Define the neutral element with respect to $\cdot$ to be $eH$. Indeed
    $$(eH)(gH) = (eg)H=gH,$$
    for every $gH\in G/H$. We inverse element of $gH\in G/H$ to be $(g^{-1}H) = H^{-1}g^{-1} = Hg^{-1} = g^{-1}H$. Indeed 
    $$(g^{-1}H)(gH) = (g^{-1}g)H= eH.$$
\end{proof}
\begin{corollary}\label{QuotientGroupOfAdditiveGroupIsAdditive}
    If $(G,+)$ is an additive group and $H\subset G$ is a subgroup, then $(G/H,+)$ (where $+$ is defined as in the above proposition) is an additive group
\end{corollary}
\begin{proof}
    By Remark~\ref{ReformulationOfNormalSubgroupAndAllSubgroupsOfAbelianGroupsAreNormal} $H$ is normal and by the above proposition $G/H$ is a group. Let $g_1+H,g_2+H\in G/H$ be given. Then
    $$(g_1+H)+(g_2+H) = (g_1+g_2)+H = (g_2+g_1)+H = (g_2+H)+(g_1+H).$$
\end{proof}
\begin{lemma}\label{SubgroupOfQuoutientGroup}
    Let $G$ be a group and $N$ a normal subgroup. Let $N\subset H\subset G$ a subgroup. Then $N\subset H$ is normal and $H/N \subset G/N$ is a subgroup. 
\end{lemma}
\begin{proof}
    For every $h\in H$, $hNh^{-1} = N$, since $h\in G$. Let $h_1N,h_2N\in H/N$. Then since $h_1h_2\in H$, 
    $$(h_1N)(h_2N)=h_1h_2N \in H/N.$$
    Furthermore since $e\in H$, $eN \in H/N.$
\end{proof}
\begin{proposition}\label{ClassificationOfSubgroupsOfQuotientGroup}
    Let $G$ be a group and $N$ a normal subgroup. Then \\ $S =\{H\subset G : H \text{ is a subgroup of } G, N\subset H\}$ is in one-to-one correspondence with the set $S' = \{ K \subset G/N : K \text{ a subgroup of } G/N\}.$ Any subgroup $K\subset G/N$ is of the form $H/N$ for some $H\in S$.
\end{proposition}
\begin{proof}
    We show that $u : S \rightarrow S',$ $H \mapsto H/N$ is a bijection. This map is well-defined by the above lemma. For $K \in S'$, let $H(K) = \{ g\in G : gN \in K\}$. We check that $H(K)$ is a subgroup $G$ containing $N$. Let $h_1,h_2\in H(K)$. Then $h_1N,h_2N\in K$, hence $h_1h_2N\in K$, implying $h_1h_2\in H(K)$. Clearly $eN \in K$, hence $e \in H(K)$. Let $n\in N$. Then $nN = eN \in K$, hence $n\in H(K)$. Then the map $u' : S' \rightarrow S$, $K \rightarrow H(K)$ is well-defined. We check that $u$ and $u'$ are mutual inverses. Let $K \in S'$. We need to check that $uu'(K)=H(K)/N = K$. Let $k \in K$, then $k = gN$ for some $g\in G$, then $g\in H(K)$, hence $k=gN \in H(K)/N$. Let $hN \in H(K)/N$. Then by definition $hN \in K$. Let $H \in S$. Then we need to check that $u'u(H) = H(H/N) = H$. Let $h \in H(H/N)$, then $hN \in H/N$, hence $h\in H$. Let $h \in H$. Then $hN\in H/N$, hence $h \in H(H/N).$ 
\end{proof}
\begin{proposition}
    Let $G$ be a group and $N\subset G$ a normal subgroup. The surjection $\pi : G\rightarrow G/N, g\mapsto gN$ is a group map
\end{proposition}
\begin{proof}
    Let $g_1,g_2\in G$. Then $\pi(g_1g_2)=g_1g_2N= g_1Ng_2N=\pi(g_1)\pi(g_2)$.
\end{proof}    
\subsection{Rings}
\subsubsection{Definition \& Basic Properties}
\begin{definition}
    A \textit{ring (with unity)} is a set $R$ with operations $+: R\times R \rightarrow R$ and $\cdot: R\times R \rightarrow R$ called \textit{multiplication} such that $(R,+)$ is an additive group, $(R,\cdot)$ is a monoid and for $r_1,r_2,r_3\in R$
    $$r_1(r_2+r_3) = r_1r_2+r_1r_3 \ \& \ (r_1+r_2)r_3 = r_1r_3+r_2r_3.$$
    We denote the neutral element with respect to multiplication by $1$. The data specifying a ring is often written $(R,+,\cdot)$. 
\end{definition}
\begin{lemma}\label{ImportantRingIdentities}
    Let $R$ be a ring and $r\in R$. The following identities are true for rings
    \begin{enumerate}
        \item $0\cdot r = 0$.
        \item $(-1)r = -r $, $r(-1) = -r$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    1. follows from the following computation.
    \begin{align*}
        0r = 0r + 0 = 0r+r-r = 0r+1r-r=(0+1)r-r = 1r-r=r-r=0
    \end{align*}
    2. follows from the following computation.
    \begin{align*}
       (-1)r = (-1)r+0 = (-1)r+r-r = (-1)r+1r-r=(-1+1)r-r = 0r-r=0-r=-r
    \end{align*}
    the other statement is proven similarly.  
\end{proof}
\begin{definition}
    Let $R$ be a ring. If $(R,\cdot)$ is a commutative monoid, then $R$ is called a \textit{commutative ring}.  
\end{definition}
\begin{definition}
    Let $R$ be a ring. A \textit{subring} is a subset $S\subset R$ such that $S$ is a subgroup of $(R,+)$ and a submonoid of $(R,\cdot)$. 
\end{definition}
\begin{remark}\label{CommutativeRingRemark}
    $(S,+,\cdot)$ is a ring. Indeed, clearly $(S,+)$ is an additive group since $S\subset R$ is a subgroup and $(S,\cdot)$ is a monoid since $S\subset R$ is a submonoid. Let $r_1,r_2,r_3\in S$, then since $S\subset R$,
    $$r_1(r_2+r_3)= r_1r_2+r_1r_2 \ \& \ (r_1+r_2)r_3=r_1r_3+r_2r_3.$$
    One should also note that $R$ is commutative then $S$ is commutative
\end{remark}
\begin{example}
    \begin{enumerate}
        \item $(\Z,+,\cdot)$, $(\Q,+,\cdot)$, $(\R,+,\cdot)$, $(\C,+,\cdot)$ are rings.
        \item Given a ring $R$ we may form the opposite ring $(R^{(\text{op})},+,\ast)$ where $(R^{(\text{op})},+)=(R,+)$ and multiplication is defined by $r\ast r' = r' \cdot r$ for $r,r'\in R$. checking that this is a ring is easy. Clearly $(R^{(\text{op})},+)$ is an additive group. Let $r_1,r_2,r_3\in R^{(\text{op})}$. Then 
        $$r_1\ast (r_2\ast r_3) = (r_3r2)r_1 = r_3(r_2r_1) = (r_1\ast r_2)\ast r_3$$
        and 
        $$r_1 \ast 1= 1 r_1 = r_1= r_1 1= 1\ast r_1$$
        and lastly 
        $$r_1\ast(r_2+r_3)= (r_2+r_3)r_1 = r_2r_1+r_3r_1 = r_1\ast r_2+r_1\ast r_3,$$
        where last identity to be checked is omitted as it is dual to the one above.
        One also easily verifies that $\left(R^{(\text{op})}\right)^{(\text{op})} = R$
        \item For a non-empty set $X$ and a ring $R$, the set $\Fun(X,R)$ is a monoid and an additive group with respect to multiplication and addition defined earlier. One easily verifies that it is also a ring.
        \item For rings $R,S$, if $\Hom^\text{Ring}(R,S)\neq \emptyset$ is never a subring of $\Fun(R,S)$. Indeed, note that the zero map is never a ring homomorphism since it maps $1$ to $0$.  
    \end{enumerate}
\end{example}
\subsubsection{Morphisms of Rings}
\begin{definition}
    Let $R,S$ be rings. A map $\sigma : R \rightarrow S$ is called a \textit{ring homomorphism/map of rings/morphism of rings} if $\sigma$ is a group homomorphism between $(R,+)$ and $(S,+)$ and a monoid homomorphism between $(R,\cdot)$ and $(S,\cdot)$. The set of ring homomorphisms between $R$ to $S$ is denoted $\Hom^\text{Ring}(R,S)$.
\end{definition}
Here are some examples of rings 
\begin{lemma}
    Let $\sigma : R\rightarrow S$ be a ring homomorphism and $T\subset R$, $U\subset S$ be subrings. Then $\sigma(T)\subset S$ and $\sigma^{-1}(U)$ are subrings. If $T$ is commutative then so is $\sigma(T)$.
\end{lemma}
\begin{proof}
    Prior lemmas ensure that these sets are appropriate additive subgroups and submonoids.
\end{proof}
\subsubsection{Product Rings}
\begin{proposition}\label{ProductRingIsRing}
    Let $A$ be a set, $\{R_\alpha\}_{\alpha\in A}$ a family of rings. Then $\left(\prod_{\alpha\in A} R_\alpha,\cdot \right)$ is a monoid and $\left(\prod_{\alpha\in A} R_\alpha,+ \right)$ an additive group by Theorem~\ref{DirectProductOfMonoidsIsAMonoid} resp. Proposition~\ref{DirectProductRestrictedDirectProductOfGroupsAreGroups}. In addition $\left(\prod_{\alpha\in A} R_\alpha , +,\cdot\right)$ is a ring.
\end{proposition}
\begin{proof}
    It remains to check that multiplication distributes over addition. Let $(r_\alpha),(r_\alpha'),(r_\alpha'') \in \prod_{\alpha\in A} R_\alpha$. Then
    \begin{align*}
        (r_\alpha)\left((r_\alpha')+(r_\alpha'')\right) &= (r_\alpha)(r_\alpha'+r_\alpha'') = (r_\alpha (r_\alpha'+r_\alpha'')) = (r_\alpha r_\alpha' + r_\alpha r_\alpha'') = (r_\alpha r_\alpha') + (r_\alpha r_\alpha'')\\
        &= (r_\alpha)(r_\alpha') + (r_\alpha)(r_\alpha'').
    \end{align*}
\end{proof}
\begin{remark}
    The above ring is called \textit{the product ring (of $\{R_\alpha\}_{\alpha\in A}$ over $A$)}.
\end{remark}
\begin{corollary}
    The direct sum of rings is a subring of the direct product.
\end{corollary}
\begin{proof}
    This follows from Lemma~\ref{RestrictedProductMonoidIsAMonoid} and Lemma~\ref{DirectProductRestrictedDirectProductOfGroupsAreGroups}.
\end{proof}
\begin{proposition}\label{ProductRingIsProductInTheCategoryOfRings}
    Let $A$ be a set and $\{R_\alpha\}_{\alpha \in A}$ a family of rings. Then 
    $$\pi_\beta : \prod_{\alpha\in A} R_\alpha \rightarrow R_\beta$$
    is ring homomorphism. Given a ring $S$ and a family of ring homomorphisms $\{f_\alpha : S\rightarrow R_\alpha\}_{\alpha\in A}$ then the unique group and monoid homomorphism $f: S\rightarrow \prod_{\alpha\in A} R_\alpha$ (cf. Lemma~\ref{ProductMonoidIsAProductInTheCategoryOfMonoids} and Proposition~\ref{ProductGroupIsProductInCategoryOfGroups}) such that $\pi_\alpha \circ f = f_\alpha$ for every $\alpha \in A$ is a ring homomorphism. 
\end{proposition}
\begin{proof}
    This follows immediately from the fact both $\pi_\beta$ and $f$ are both group and monoid homomorphisms.
\end{proof}
\subsubsection{The Set of Integers: $\Z$}
    \begin{definition}
        For $(a,b),(c,d)\in \N$ we define $(a,b)\sim (c,d)$ if $a+d=b+c$. On easily checks that this is an equivalence relation. We define 
        $$\Z := \N^2/\sim.$$
    \end{definition}
    \begin{proposition}
        On $\Z$ we define
        $$[(a,b)]+[(c,d)] := [(a+c,b+d)]$$
        and 
        $$[(a,b)][(c,d)] := [(ac+bd,ad+bc)].$$
        Moreover we define $0:=[(0,0)]$, $-[(a,b)]:=[(b,a)]$ and $1:= [(1,0)]$. With these definitions, $\Z$ becomes a commutative ring. The $\{[(a,0)]\in \Z :a\in \N\}$ is a sub semi-ring isomorphic to $\N$.
    \end{proposition}
    \begin{proof}
        Suppose first that $([(a,b)],[(c,d)])=([(x,y)],[(v,w)])$. Then 
        $$a+y=b+x,\quad c+w=d+v$$
        hence
        $$(a+c)+(y+w)=(a+y)+(c+w)=(b+x)+(d+v)=(b+d)+(x+v) \implies [(a+c,b+d)]=[(x+v,y+w)].$$
        So addition is well-defined. We also have that  We check that $\Z$ is a group. Associativity of addition on $\Z$ readily follows from associativity of addition on $\N$. Let $[(a,b)],[(c,d)]\in\Z$ be arbitrary. Then 
        $$[(a,b)] + [(0,0)]= [(a+0,b+0)]=[(a,b)]$$
        and 
        $$[(a,b)]+(-[(a,b)])=[(a,b)]+[(b,a)]=[(a+b,a+b)] = [(0,0)]=0$$
        and 
        $$[(a,b)]+[(c,d)] = [(a+c,b+d)]=[(c+a,d+b)]=[(c,d)]+[(a,b)]$$
        hence $\Z$ is a commutative group. It is easy to check that $\{[(a,0)]\in \Z: a\in\N\}$ is a submonoid isomorphic to $\N$. Suppose again $([(a,b)],[(c,d)])=([(x,y)],[(v,w)])$. We find that
        \begin{align*}
            ac+bd+xw+yv+yc+xd+xc+yd &=\\
            &= c(a+y)+d(b+x)+x(c+w)+y(d+v)\\
            &= c(b+x)+d(a+y)+x(d+v)+y(c+w)\\
            &= ad+bc+xv+yw+yc+xd+xc+yd.
        \end{align*}
        implying that, using a fact from group theory,
        $$ac+bd+xw+yv=ad+bc+xv+yw \implies [(ac+bd,ad+bc)]=[(xv+yw,xw+yv)].$$
        We now find that
        \begin{align*}
            ([(a,b)][(c,d)])[(e,f)] &= [(ac+bd,ad+bc)][(e,f)]\\ &=[(ace+bde+adf+bcf,acf+bdf+ade+bce)]\\
            &= [(a(ce+df)+b(cf+de),a(cf+de)+b(ce+df)]\\
            &= [(a,b)][(ce+df,cf+de)] = [(a,b)]([(c,d)][(e,f)]).
        \end{align*}
        and
        $$[(a,b)][(1,0)]=[(a+b\cdot 0,a\cdot0+b)]=[(a,b)]$$
        and easily we check that
        $$[(a,b)][(c,d)]=[(c,d)][(a,b)].$$
        Lastly,
        \begin{align*}
            [(a,b)]([(c,d)]+[(e,f)]) &= [(a,b)][(c+e,d+f)]=[(ac+ae+bd+bf,ad+af+bc+be)]\\
            &= [(ac+bd,ad+bc)]+[(ae+bf,af+be)]\\
            &=[(a,b)][(c,d)]+[(a,b)][(e,f)],
        \end{align*}
        making $\Z$ a commutative ring.
        One readily verifies that $\{[(a,0)]\in\Z:a\in\N\}$ is a submonoid of $\Z$ with respect to multiplication. The isomorphism from $\N$ is given by $a\mapsto [(a,0)]$.
        \end{proof}
\subsection{Modules}
\subsubsection{Initial Definitions, Basic Properties \& Constructions}
\begin{definition}
    Let $R$ be a ring. A \textit{left $R$-module} is an additive group $(M,+)$ with a \textit{left scalar multiplication} $\cdot : R \times M \rightarrow M$, where $rm := r\cdot m := \cdot(r,m)$ for $(r,m)\in R\times M$ satisfying the following axioms
    \begin{enumerate}
        \item For every $r\in R$, $m,m'\in M$,
        $$r(m+m')=rm+rm'.$$
        \item For every $r,r'\in R$, $m\in M$,
        $$(r+r')m = rm+r'm.$$
        \item For every $r,r'\in R$, $x\in M$,
        $$(rr')m=r(r'm).$$
        \item For every $m\in M$,
        $$1m = m.$$
    \end{enumerate}
    To emphasise that a module $M$ is a left $R$-module, we may write $_RM:= M$.\\
    A \textit{right $R$-module} is an additive group $(M,+)$ with a \textit{right scalar multiplication} $\cdot : M\times R \rightarrow M$, where $mr := m\cdot r := \cdot(m,r)$, satisfying axioms dual to ones for left scalar multiplication.  To emphasise that a module $M$ is a right $R$-module, we may write $M_R := M$\\
    Let $S$ be a ring. An $(R,S)$-bimodule is an additive group $(M,+)$, that is a left $R$-module and a right $S$-module satisfying 
    $$(rm)s = r(ms),$$
    for every $r\in R$,$s\in S$, $m\in M$. To emphasise that a module $M$ is an $(R,S)$-bimodule, we may write $_RM_S := M$.
\end{definition}
\begin{lemma}\label{SufficientToCheckLeftCase}
    Let $M$ be an additive group and $R$ a ring. Then $M$ is a left $R$-module if and only if $M$ is a right $R^{(\text{op})}$-module.
\end{lemma}
\begin{proof}
    "$\implies$": 'pose $M$ is a left $R$-module. We define a right scalar multiplication of $R^{(\text{op})}$ on $M$ by $mr = rm$. Checking the first 3 axioms is straight forward. For the 4th axiom, let $r_1,r_2\in R^{(\text{op})}$ and $m\in M$ be given. Then 
    $$m(r_1\ast r_2) = (r_2r_1)m=r_2(r_1m)=r_2(mr_1) = (mr_1)r_2.$$
    "$\impliedby$": This is very similar.
\end{proof}
The consequence of the above lemma is that any theorem about right $R$-modules that is true for left $R$-modules, can be proven by applying said left case theorem to $R^{(\mathrm{op})}$. Using the fact that $R^{(\text{op})}=R$ when $R$ is commutative, implies that left/right $R$-modules coincide. In this case left/right $R$-modules will be referred to simply as \textit{$R$-modules}.\\
For a field $K$, we call a $K$-module a \textit{vector space over $K$}
We give simple initial examples of modules.
\begin{definition}
    Let $M$ be a left/right $R$-module. A left/right $R$-submodule is a subset $N\subset M$ such that $N$ is a subgroup of $(M,+)$ and for every $r\in R$, $n\in N$ we have that $rn\in N$ resp. $nr \in N$.
\end{definition}
\begin{remark}
    A left/right $R$-submodule $N\subset M$ is a left/right $R$-module. Indeed $(N,+)$ is group since it is a subgroup of $(M,+)$. $N$ being closed under left/right scalar multiplication ensures that $\cdot := \cdot\mid_{R\times N} : R\times N \rightarrow N$ respectively $\cdot := \cdot\mid_{N \times R}: N\times R\rightarrow N$ are well-defined and $M$ being a left/right $R$-module, these left/right actions respect the axioms for left/right scalar multiplication. If $M$ is a $(R,S)$-bimodule and $N$ is a left $R$-submodule and a right $S$-submodule of $M$ then it is also an $(R,S)$-bimodule. Indeed, if $r\in R$, $n\in N$ and $s\in S$, then since $n\in M$, $r(ns)=(rn)s$.
\end{remark}
\begin{example}
    \begin{enumerate}
        \item Let $(G,+)$ be an additive group. Then $G$ is a $\Z$-module under the left/right scalar multiplication
        $$ng = \sum_1^n g \ \& \ gn = \sum_1^n g.$$
        Under this definition $gn = ng$ hence if $G$ is a left $\Z$-module, it is automatically a $\Z$-module. Let $n,m\in \Z$ and $g,g'\in G$. Then 
        \begin{align*}
            &1. \quad  n(g+g') = \sum_1^n (g+g') = \sum_1^n g + \sum_1^n g' = ng+ng' \\
            &2. \quad (n+m)g = \sum_1^{n+m} g = \sum_1^n g + \sum_{n+1}^{n+m} g = ng + \sum_1^m g = ng +mg \\
            &3. \quad n(mg) = n\sum_1^m g = \sum_1^n\sum_1^m g =  \sum_1^{nm} g = (nm)g\\
            &4. \quad 1g = \sum_1^1 g = g
        \end{align*}
        To prove the second to last equality 3. one should really use induction in $m$. Note that the induction start uses 4.
        \item Let $(R,+,\cdot)$ be a ring. Then $(R,+)$ is an additive group, which becomes an $(R,R)$-bimodule under the action $rx := r\cdot x$ and $xs := x\cdot s$ for $r,s,x\in R$.
        \item A left/right $R$-submodule of $I\subset R$ is called a \textit{left/right ideal in $R$}. If $I$ is an $(R,R)$-bimodule, it is called a \textit{both-sided ideal in $R$}. If $R$ is commutative a left/right ideal is simply referred to as an \textit{ideal in $R$}. 
        \item Let $A$ be a set and $R$ a ring, then $\prod_{\alpha \in A} R$ is an $R$-module, under the left/right scalar multiplication given by $a(r_\alpha) := (ar_\alpha)$/$(r_\alpha)a=(r_\alpha a)$ for $a\in R$ and $(r_\alpha)\in \prod_{\alpha \in A} R$. This is easily checked using 2. together with the fact that $(a)(r_\alpha) = a(r_\alpha)$ and $(r_\alpha)(a) = (r_\alpha)a$ for every $a\in R$, $(r_\alpha)\in \prod_{\alpha \in A} R$. It in particular follows that the matrix ring is an $R$-module.
        \item Let $R$ be a ring and $I\subset R$ an ideal. Then $R/I$ is an $R$-module, when equipped with the left/right scalar multiplication $r(a+I) := (ra+I)$. One sees this from the fact that $(r+I)(a+I) = r(a+I)$. 
        \item Let $S$ be a ring and $R$ a subring of $S$. Then $ R\times S\ni(r,s) \mapsto rs\in S $ defines a left scalar multiplication of $R$ on $S$, hence $S$ is a left $R$-module. One turns $S$ into a right $R$-module via $S\times R \ni (s,r) \mapsto sr \in S$ 
    \end{enumerate}
\end{example}
\begin{lemma}
    Let $R$ be a ring and $M$ a left/right $R$-module. 
    \begin{enumerate}
        \item $0m = 0$ for every $m\in M$.
        \item $(-1)m = -m$ for every $m\in M$.
        \item $r(-m) = -rm$ for every $r\in R$, $m\in M$.
        \item $r0 = 0$ for every $r\in R$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    1. Really this is just a generalization of Lemma~\ref{ImportantRingIdentities} 1. Indeed,
    $$0m = 0m + m -m = 0m+1m-m = (0+1)m-m = 1m -m =m-m = 0.$$
    2. Really this is just a generalization of Lemma~\ref{ImportantRingIdentities} 2. Indeed,
    $$(-1)m=(-1)m +m-m = (-1)m+1m-m = (-1+1)m-m = 0m-m=0-m=-m$$
    3. Indeed,
    $$r(-m) = r((-1)m) = (r(-1))m=((-1)r)m=-rm.$$
    4. Indeed,
    $$r0 = r(0-0)=r0+r(-0)=r0-r0=(r-r)0=0\cdot 0 = 0.$$
\end{proof}
\begin{definition}
    Let $R$ be a ring. An element $m$ of a left/right $R$-module $M$ is called a torsion \textit{torsion element} if there is an $r\in R$  that is not a left/right zero-divisor satisfying $rm =0$ (resp. $mr=0$). A module is a torsion module if every element is a torsion element and \textit{torsion free} if the only torsion element is $0$.
\end{definition}
\begin{remark}
    An example of a torsion free module is a domain $R$.
\end{remark}
\begin{definition}
    Let $_RM, _RN$ be left $R$-modules. A map $\rho : M \rightarrow N$ is a \textit{left $R$-module homomorphism/map of left $R$-modules/morphism of left $R$-modules} if for every $r\in R$, $m,m'\in M$,
    $$\rho(rm+m') = r\rho(m)+\rho(m').$$
    \textit{right $R$-module homomorphisms} are defined in a dual manner. \textit{$(R,S)$-bimodule homomorphisms} is a map that is both a left $R$-module homomorphism and a right $S$-module homomorphism. 
\end{definition}
\begin{remark}
    A left/right/bimodule module homomorphisms $(M,+)\rightarrow (N,+)$ are automatically group homomorphisms. Similarly for every $r\in R$ and $m\in M$,
    $$\rho(rm) = r\rho(m),$$
    which is seen by setting $m' = 0$ when $\rho: M\rightarrow N$ is a left $R$-module. This is also true from the right if $\rho$ was a right $R$-module.
\end{remark}
\begin{lemma}
    Let $M$,$N$ be additive groups, $R$ a ring and $\rho : M \rightarrow N$ a group homomorphism. Then $\rho$ is a left $R$-module homomorphism if and only if $\rho$ is a right $R^{(\text{op})}$-module. 
\end{lemma}
\begin{proof}
    "$\implies$": We make $M$ and $N$ right $R^{(\text{op})}$-modules as in Lemma~\ref{SufficientToCheckLeftCase}. Let $r\in R^{(\text{op})}$ and $m\in M$. It is sufficient to check that $\rho(mr)=\rho(m)r$. Indeed, 
    $$\rho(mr) = \rho(rm) = r\rho(m) = \rho(m)r.$$
    "$\impliedby$": This is proven by using $\left(R^{(\text{op})}\right)^{(\text{op})}=R$ and applying "$\implies$".
\end{proof}
The consequence of the above lemma is that we get a way of automatically check a theorem for $R$-module homomorphisms, whenever we have a proof of the left case. This akin to what we gained in Lemma~\ref{SufficientToCheckLeftCase}.

\begin{lemma}
    If $\rho : M\rightarrow  N$ is a left $R$-module/right $S$-module homomorphism, then for a left $R$-submodule/right $S$-module $L\subset M$, $\sigma(L)\subset N$ is a left $R$-submodule/ right $R$-submodule. If $M$ and $N$ are $(R,S)$-bimodules and $\rho$ is an $(R,S)$-bimodule homomorphism, then $\sigma(L)$ is an $(R,S)$-bimodule. The two statements present are thus in particular true for the image of $\rho$.
\end{lemma}
\begin{proof}
    We only check left case, as the right case is dual. We already know that $\sigma(L)$ is a subgroup. Let $r\in R$ and $\sigma(l)\in\sigma(L)$. Then
    $$r\sigma(l)=\sigma(rl) \in \sigma(L).$$
    Let $r\in R$, $\sigma(l)\in \sigma(L)$, $s\in S$. Then 
    $$r(\sigma(l)s)= r\sigma(ls)= \sigma(r(ls)) = \sigma((rl)s) = \sigma(rl)s=(r\sigma(l))s.$$
\end{proof}
\begin{lemma}
    Let $R,S$ be rings, $A$ a set and $\{M_\alpha\}_{\alpha\in A}$ be a family of left $R$-modules/right $S$-modules/$(R,S)$-bimodules. Then 
    $$\prod_{\alpha\in A} M_\alpha$$
    is a left $R$-modules/right $S$-module/$(R,S)$-bimodule.
\end{lemma}
\begin{proof}
    Note that by Proposition~\ref{CertainSubgroupsOfProductGroup} the direct product of a family of left/right modules is an additive group. We check the left case. Let $r_1,r_2\in R$, $(m_\alpha),(m_\alpha')\in \prod_{\alpha \in A} M_\alpha$. Then 
    \begin{align*}
        &1. \quad (r_1+r_2)(m_\alpha)=((r_1+r_2)m_\alpha)=(r_1m_\alpha+ r_2m_\alpha)=(r_1m_\alpha)+(r_2m_\alpha)=r_1(m_\alpha)+r_2(m_\alpha).\\
        &2. \quad r_1((m_\alpha)+(m_\alpha')) = r_1(m_\alpha+m_\alpha')=(r_1m_\alpha+r_1m_\alpha')=(r_1m_\alpha)+(r_1m_\alpha')=r_1(m_\alpha)+r_1(m_\alpha').\\
        &3. \quad 1(m_\alpha) = (1m_\alpha)=(m_\alpha).\\
        &4. \quad (r_1r_2)(m_\alpha)=((r_1r_2)m_\alpha)=(r_1(r_2m_\alpha))=r_1(r_2m_\alpha)=r_1(r_2(m_\alpha)).
    \end{align*}
    Suppose $\{M_\alpha\}_{\alpha\in A}$ is a family of $(R,S)$-modules and let $r\in R$, $s\in S$. Then 
    $$r((m_\alpha)s)= r(m_\alpha s) = (r(m_\alpha s))=((rm_\alpha)s) = (rm_\alpha)s=(r(m_\alpha))s.$$
\end{proof}
\begin{proposition}
    Let $A$ be a set, $R$ a ring and $\{M_\alpha\}_{\alpha \in A}$ a family of left/right modules. Then 
    $$\pi_\beta : \prod_{\alpha\in A} M_\alpha \rightarrow M_\beta$$
    is a left/right $R$-module homomorphism. Given a left/right $R$-module $N$ and a family of left/right $R$-module homomorphisms $\{f_\alpha : N \rightarrow M_\alpha\}_{\alpha\in A}$ then the unique group homomorphism $f: N\rightarrow \prod_{\alpha\in A} M_\alpha$ (cf.  Proposition~\ref{ProductGroupIsProductInCategoryOfGroups}) such that $\pi_\alpha \circ f = f_\alpha$ for every $\alpha \in A$ is a left/right $R$-module homomorphism. 
\end{proposition}
\begin{proof}
    Let $r\in R$ and $(m_\alpha)\in \prod_{\alpha \in A} M_\alpha$, $n\in N$. Then for $\beta \in A$, 
    $$\pi_\beta(r(m_\alpha))= \pi_\beta((rm_\alpha)) = rm_\beta = r \pi_\beta((m_\alpha)).$$
    Furthermore, we have that 
    $$f(rn)=(f_\alpha(rn))=(rf_\alpha(n))=r(f_\alpha(n))=rf(n).$$
\end{proof}
\begin{lemma}
    Let $R$ be a ring, $M$ a left/right $R$-module, $A$ a set and $\{M_\alpha\}_{\alpha\in A}$ a family of left/right $R$-submodules. Then 
    $$\bigcap_{\alpha\in A} M_\alpha$$
    is a left/right $R$-submodule.
\end{lemma}
\begin{proof}
    From Proposition~\ref{IntersectionsOfSubgroupIsSubgroup} we already know that $\bigcap_{\alpha\in A} M_\alpha$ is an additive subgroup. Let $r\in R$ and $m\in \bigcap_{\alpha\in A} M_\alpha$ then $rm\in M_\alpha$ for every $\alpha \in A$, meaning $rm \in \bigcap_{\alpha \in A} M_\alpha$.
\end{proof}
\begin{proposition}
    Let $A$ be a set, $R$ a ring and $\{M_\alpha\}_{\alpha \in A}$ a family of left/right $R$-modules. 
    Then
    $$\bigoplus_{\alpha\in A} M_\alpha$$
    is a submodule of $\prod_{\alpha\in A} M_\alpha$.
\end{proposition}
\begin{proof}
    We already know it to be an additive subgroup.
    Let $r \in R$ and $(m_\alpha)\in \bigoplus_{\alpha\in A} M_\alpha$. Then for some finite subset $B\subset A$, $m_\alpha = 0$ for every $\alpha \in A\setminus B$. Hence $rm_\alpha = 0$ for every $\alpha \in A\setminus B$. It thus follows that $r(m_\alpha)=(rm_\alpha)\in \bigoplus_{\alpha \in A} M_\alpha$.
\end{proof}
\begin{lemma}\label{QuotientModule}
    Let $R$ be a ring $M$ be a left/right $R$-module. Let $N\subset M$ be a left/right $R$-submodule. Then $M/N$ is a left/right submodule under the left/right scalar multiplication $r(m+N) := rm+N$ resp. $(m+N)r := mr+N$. If $M$ is an $(R,S)$-bimodule for some ring $S$ and $N$ is a left $R$-submodule and a right $S$-submodule. Then $M/N$ is an $(R,S)$-bimodule.
\end{lemma}
\begin{proof}
    Let $r_1,r_2\in R$, $m+N,m'+N\in M/N$. Then 
    \begin{align*}
        &1. \quad (r_1+r_2)(m+N) = (r_1+r_2)m+N = (r_1m+r_2m)+N =(r_1m+N)+(r_2m+N)\\ &= r_1(m+N) + r_2(m+N).\\
        &2. \quad r_1((m+m')+N)=r_1(m+m')+N = (r_1m+r_1m')+N = (r_1m+N)+(r_1m'+N)\\ &= r_1(m+N)+r_1(m'+N).\\ 
        &3. \quad 1(m+N) = 1m+N = m +N\\
        &4. \quad (rr')(m+N)=(rr')m+N = r(r'm)+N= r(r'm+N)=r(r'(m+N))
    \end{align*}
    Suppose $M$ is an $(R,S)$-bimodule. Let $r\in R$, $s\in S$. Then 
    $$r((m+N)s)=r(ms+N)=r(ms)+N = (rm)s+N = (rm+N)s=(r(m+N))s.$$
\end{proof}
\begin{corollary}
    The canonical surjective group map $\pi : M\rightarrow M/N$ is a left/right module map
\end{corollary}
\begin{proof}
    Let $r\in R$, $m\in M$. Then $\pi(rm)=rm+N=r(m+N)=r\pi(m).$
\end{proof}
\begin{lemma}\label{SubmodulesOfQuotientModule}
    Let $R$ be a ring and $M$ a left/right $R$-module and $N\subset M$ a submodule. Then there is one-to-one correspondence between the sets 
    $$U=\{L\subset M : L \text{ is a submodule of M containing }N \}$$
    and 
    $$U' = \{ K \subset M/N : K \text{ is a submodule of } M/N\}.$$
\end{lemma}
\begin{proof}
     This is a corollary of Proposition~\ref{ClassificationOfSubgroupsOfQuotientGroup}. Note that by Lemma~\ref{QuotientModule}, $u:U\rightarrow U'. L\mapsto L/N$ is well-defined- Note that $U\subset S$ and $U'\subset S'$. Let $K\in U'$, we check that $L(K) := \{m\in M : m+N\in K\}$ is a submodule. Let $r\in R$, $m\in L(K)$. Then $(rm+N)=r(m+N)\in M/N$, hence $rm\in L(K)$. Then $u' : U'\rightarrow U, K\mapsto L(K)$ is well-defined. One easily verifies that $u$ and $u'$ are mutual inverses.   
\end{proof}
\begin{lemma}\label{SumOfSubmodulesIsSubmodule}
    Let $R$ be a ring, $M$ a left/right $R$-module, $A$ a set and $\{M_\alpha\}_{\alpha\in A}$ a family of left/right $R$-submodules of $M$. Then $s : \bigoplus_{\alpha\in A} M_\alpha \rightarrow M$ (cf. Proposition~\ref{CanonicalGroupHomomorphismBetweenDirectSumAndSum}), hence
    $$\sum_{\alpha\in A} M_\alpha$$
    is a left/right $R$-submodule.
\end{lemma}
\begin{proof}
    Indeed for $r\in R$, $(m_\alpha)\in \bigoplus_{\alpha\in A} M_\alpha$,
    \begin{align*}
        s(r(m_\alpha))=s((rm_\alpha))= \sum_{\alpha\in A} rm_\alpha = \sum_1^n rm_{\alpha_i}= r\sum_1^n m_{\alpha_i} = r\sum_{\alpha\in A} m_\alpha = rs((m_\alpha)),
    \end{align*}
    where $\alpha_1,\dots,\alpha_n\in A$ are chosen suitably.
\end{proof}
\begin{remark}
    We define $\sum_1^n M_1 = \sum_{i \in \{1,\dots,n\}} M_i$ for left/right $R$-submodules $M_1,\dots, M_n$ of $M$ and $M_1+M_2 := \sum_1^2 M_i$. 
\end{remark}
\begin{lemma}
    Let $R$ be a ring, $I,J\subset R$ be a left resp. right ideal, $M$ a left/right $R$-module and $m\in M$. Then 
    $$Im := \{rm : r\in I\} \ \& \ mJ := \{mr : r \in J\}$$
    is a left resp. right $R$-submodule of $M$. Let $X\subset M$. Then 
    $$IX := \sum_{x\in X} Rx \ \& \ XJ := \sum_{x\in X} xR$$
    is a left resp. right $R$-submodule of $M$.
\end{lemma}
\begin{proof}
    Indeed for the first statement let $a,b\in I$ and $r\in R$. Then $ra\in I$, hence 
    $$r(am)=(ra)m\in Im.$$
    Furthermore, since $a+b\in I$, hence
    $$am+bm=(a+b)m\in Im.$$
    The right case follows from $J$ being a left $R^{(\mathrm{op})}$-module hence $mJ$ is a left $R^{(\mathrm{op})}$-module, hence $mJ$ is a right $R$-module.
    $IX, XJ$ being left/right modules follows from the first statement and Lemma~\ref{SumOfSubmodulesIsSubmodule}.
\end{proof}
\begin{definition}
    Let $R$ be a ring and $M$ a left/right $R$-module. Then $M$ is said to be \textit{finitely generated over $R$} if there is a finite sequence $m_1,\dots,m_n\in M$ such that $M= \sum_1^n Rm_i$.
\end{definition}
\begin{definition}
    Let $A$ be a set, $R$ a ring and $\{M_\alpha\}_{\alpha\in A}$ a family of left/right $R$-modules. We say that $\sum_{\alpha\in A} M_\alpha$ is \textit{direct}, if 
    for every $\beta \in A$,
    $$M_\beta \cap \sum_{\alpha\in A \setminus\{\beta\}} M_\alpha = 0.$$
\end{definition}
\begin{lemma}
    Let $A$ be a set, $R$ a ring and $\{M_\alpha\}_{\alpha\in A}$ a family of left/right $R$-modules such that $\sum_{\alpha\in A} M_\alpha$ is direct. Then 
    $$\sum_{\alpha\in A} M_\alpha \simeq \bigoplus_{\alpha \in A} M_\alpha.$$
\end{lemma}
\begin{proof}
    We define the map 
    \begin{gather*}
        \rho : \bigoplus_{\alpha \in A} M_\alpha \rightarrow \sum_{\alpha\in A} M_\alpha\\
        (m_\alpha) \mapsto \sum_{\alpha \in A} m_\alpha,
    \end{gather*}
    where $\sum_{\alpha\in A} m_\alpha$ is defined to be the sum of non-zero entries of $(m_\alpha)$. Let $\sum_1^n m_{\alpha_i}$, where $n\geq 1$ and $\alpha_1,\dots,\alpha_n\in A$. One easily finds that this is a module homomorphism. For $\alpha \in A$, we then define $m_\alpha = m_{\alpha_i}$ if $\alpha = \alpha_i$ for some $i$ and $m_\alpha = 0$ if not. Then Clearly 
    $$\sum_1^n m_{\alpha_i}=\sum_{\alpha\in A} m_\alpha =\rho((m_\alpha)),$$
    which means $\rho$ is surjective. Suppose $(m_\alpha)\in \ker \ \rho$. Then 
    $$0=\rho((m_\alpha))= \sum_{\alpha \in A} m_\alpha = \sum_1^n m_{\alpha_1},$$
    for some distinct $\alpha_1,\dots,\alpha_n\in A$. Let $j\in \{1,\dots, n\}$. Then
    $$-m_{\alpha_j} = \sum_{i\in\{1,\dots,n\}\setminus \{j\}} m_{\alpha_i} \in \sum_{\alpha \in A\setminus\{\alpha_j\}} M_\alpha. $$
    This implies $m_{\alpha_j}\in M_{\alpha_j}\cap \sum_{\alpha \in A\setminus\{\alpha_j\}} M_\alpha =0$, hence $m_{\alpha_j} = 0$, which means $m_\alpha = 0$ for each $\alpha\in A$ and so $(m_\alpha)=0$. By the 1st Isomorphism Theorem for modules it follows that $\sum_{\alpha\in A} M_\alpha \simeq \bigoplus_{\alpha \in A} M_\alpha$.  
\end{proof}
\begin{definition}
    Let $R$ be a ring and $M$ a left/right $R$-module. A subset $X\subset M$ is said to be \textit{left/right linearly independent over $R$} (or if $R$ is commutative just \textit{linearly independent}), if for every finite sequence $m_1,\dots,m_n\in M$ and every finite sequence $r_1,\dots,r_n\in R$, 
    \begin{gather*}
        \sum_1^n r_im_i = 0 \iff r_i = 0\ \forall i\in \{1,\dots,m\} \text{ resp. } \sum_1^n m_ir_i =0 \iff r_i = 0\  \forall i\in \{1,\dots,m\}
    \end{gather*}
\end{definition}
\begin{remark}
    One should note that $0\notin X$, since $1\cdot 0 = 0$ and $1\neq 0$.
\end{remark}
\begin{proposition}
    Let $R$ be a ring, $M$ a left/right $R$-module and $X\subset M$ a subset. Then if $X$ is left/right linearly independent over $R$, $\sum_{x\in X} Rx$ is direct. 
\end{proposition}
\begin{proof}
    When $X$ is empty the statement is trivial, hence suppose $X\neq \emptyset$.\\ 
    Let $y\in X$ and let $m\in Ry \cap \sum_{x\in X\setminus \{y\}} Rx$. Then 
    $$r_{n+1} y = m = \sum_1^n r_i x_i,$$
    for suitable $x_1,\dots,x_n\in X\setminus\{y\}$ and $r_1,\dots,r_{n+1}\in R$. Thus, we have that 
    $$r_{n+1}y + \sum_1^n r_ix_i = 0 \implies 0=r_1=r_2=\dots=r_{n+1} \implies m = 0.$$
    We then conclude that $Ry\cap \sum_{x\in X\setminus\{y\}} Rx = 0$.
\end{proof}
\begin{definition}
    Let $R$ be a ring, $M$ a left/right $R$-module. A subset $X\subset M$ is called a \textit{basis of $M$ over $R$} if $X$ is linearly independent over $R$ and $M= RX$ respectively $M=XR$. If $X$ is finite and a basis of $M$ over $R$ it is called a \textit{finite basis}.
\end{definition}
\begin{proposition}
    Let $S$ be a ring and $R\subset S$ a subring. Consider $(M,\cdot,+)$, a left/right $S$-module, then $rm:= r\cdot m$, for $r\in R$, defines a structure of left/right $R$-modules. If in addition $Q$ is a subring of a ring $T$ and $M$ is an $(S,T)$-bimodule, then $M$ is an $(R,Q)$-bimodule.  
\end{proposition}
\begin{proof}
    Let $m_1,m_2\in M$, $r_1,r_2\in R$. Then
    \begin{align*}
        &1.\quad r_1(m_1+m_2)=r_1\cdot(m_1+m_2)=r_1\cdot m_1+r_1\cdot m_2=r_1m_1+r_1m_2,\\
        &2.\quad (r_1+r_2)m_1= (r_1+r_2)\cdot m_1=r_1\cdot m_1+r_2\cdot m_1=r_1m_1+r_2m_1,\\
        &3.\quad (r_1r_2)m_1=(r_1r_2)\cdot m_1 = r_1(r_2\cdot m_1)=r_1(r_2m_1),\\
        &4.\quad 1m_1=1\cdot m_1=m_1.
    \end{align*}
    Let $r\in R$, $q\in Q$, $m\in M$. Then
    $$r(mq)=r\cdot(m\cdot q)=(r\cdot m)\cdot q = (rm)q.$$
\end{proof}
\subsubsection{Ideals}
\begin{definition}
    Recall that a \textit{left/right ideal} in a ring $R$, is a left/right $R$-submodule of $R$. If it is an $(R,R)$-module it is called a \textit{both-sided ideal}. If $R$ is a commutative a left/right ideal is simply refered to as an ideal.   
\end{definition}
\begin{definition}
    Let $\sigma : R \rightarrow S$ be a ring homomorphism. When we refer to the kernel of $\sigma$, we refer to the kernel of $\sigma$ when seen as a group homomorphism between $(R,+)$ and $(S,+)$, i.e. $\ker \sigma := \sigma^{-1}(0).$
\end{definition}
\begin{lemma}
    Let $\sigma : R \rightarrow S$ be a ring homomorphism and $I \subset S$ be a left/right/both-sided ideal. Then $\sigma^{-1}(I)\subset R$ is a left/right/both-sided ideal. 
\end{lemma}
\begin{proof}
    By Lemma~\ref{KernelAndImageOfGroupHomomorphismIsSubgroup}  it follows that $\sigma^{-1}(I)\subset R$ is an additive subgroup. Let $r\in R$ and $a\in \sigma^{-1}(I)$. Then
    $$\sigma(ra)=\sigma(r)\sigma(a)\in I,$$
    hence $ra \in \sigma^{-1}(I)$.
\end{proof}
\begin{corollary}\label{TheKernelOfARingHomomorphismIsAnIdeal}
    The kernel of a ring homomorphism $\sigma : R \rightarrow S$ is an ideal in $R$
\end{corollary}
\begin{proof}
    This follows immediately from the above lemma. 
\end{proof}
\begin{lemma}\label{ImageOfSurjectiveRingHomOfIdealIsIdeal}
    Let $\sigma : R \rightarrow S$ be a surjective ring homomorphism and $I\subset R$ be a left/right/both-sided ideal. Then $\sigma(I)\subset S$ is a left/right/both-sided ideal. 
\end{lemma}
\begin{proof}
     By Lemma~\ref{KernelAndImageOfGroupHomomorphismIsSubgroup} $\sigma(I)$ is an additive subgroup of $S$. Let $s\in S$ and $\sigma(a)\in \sigma(I)$. Then for some $r\in R$, $s = \sigma(r)$. It follows that 
     $$s\sigma(a) = \sigma(r)\sigma(a)=\sigma(ra)\in \sigma(I).$$
\end{proof}
\begin{remark}
    We call $RX$ and $XR$ \textit{the left/right ideal generated by $X$}. \textit{The ideal generated by $X$} is the ideal $\langle X\rangle := R(XR) = (RX)R$.\\ 
    Suppose $R$ is commutative. For $M\subset R$, one can easily check that $RM =MR$, and hence the left/right ideal generated by $M$ over $R$ is a two-sided ideal. Thus $\langle M\rangle = RM = MR$. 
\end{remark}
\begin{example}
    One may note that quite clearly $R = R1=1R$ and hence that $R= \langle 1\rangle$.
\end{example}
\begin{lemma}\label{IdealIsNonProperIff1IsInIdeal}
    Let $R$ be a ring and $I\subset R$ a left/right/both-sided ideal. Then 
    $$1\in I \iff I = R.$$
\end{lemma}
\begin{proof}
    We need only work with the assumption that $I$ is a left ideal.\\ 
    "$\implies$": Let $r \in R$. Then $r=r1\in I$, hence $R\subset I\implies R=I$.\\
    "$\impliedby$": This is trivial, since $1\in R = I$.
\end{proof}

\begin{definition}
    An ideal in a ring generated by only one element is called a \textit{principal ideal}. A ring in which every ideal is principal is called a \textit{principal ideal domain} or a \textit{PID}
\end{definition}
An example of a PID is $\Z$. Indeed $(\Z,+)$ is a cyclic group generated by $1$, thus every subgroup of the form $\langle n\rangle = n\Z$ for some $n\geq 0$, in particular every ideal is of this form. 
\begin{proposition}\label{EliminationIdeal}
    Let $S$ be a ring and $R\subset S$ a subring. Consider a left/right ideal $I\subset S$. Then $I\cap R\subset R$ is an ideal.
\end{proposition}
\begin{proof}
    $R$ and $I$ are both left/right $R$-submodules of $S$, hence so is $R\cap I$.
\end{proof}
\begin{lemma}\label{IntersectionAndProductOfIdeals}
    Let $R$ be a commutative ring and $I,J\subset R$ ideals. Then 
    $$IJ\subset I\cap J.$$
\end{lemma}
\begin{proof}
    Let $ij\in IJ$. Then since $j\in J$, $ij\in J$ and since $i\in I$, $ij\in I$, hence $ij\in I\cap J$. 
\end{proof}
\subsubsection{Quotient Rings}
    \begin{proposition}\label{QuotientRingIsRing}
    Let $R$ be a ring and $I\subset R$ an ideal. View $I$ as a subgroup of the additive group $(R,+)$. Then $(R/I,+)$ is an additive group by Corollary~\ref{QuotientGroupOfAdditiveGroupIsAdditive}. Define 
    $$\cdot : R/I\times R/I \rightarrow R/I$$
    by $(r+I)(r'+I):= (rr'+I)$. This is a well-defined operation and $(R/I, +,\cdot)$ is a ring. It is also commutative if $R$ is commutative.
\end{proposition}
\begin{proof}
    Let $(r_1+I,r_2+I) = (r_1'+I,r_2'+I)\in R/I\times R/I$. Then 
    \begin{align*}
        r_1r_2 - r_1'r_2' &= r_1r_2 - r_1r_2'+r_1r_2' - r_1'r_2' = r_1(r_2-r_2') + (r_1-r_1')r_2' \in I, 
    \end{align*}
    hence $r_1r_2 +I = r_1'r_2' + I $ and it follows that $\cdot$ is well-defined. 
    Let $r_1+I,r_2+I,r_3+I\in R/I$. Then 
    \begin{align*}
        (r_1+I)((r_2+I)(r_3+I)) &=(r_1+I)(r_2r_3+I) = (r_1(r_2r_3))+ I = (r_1r_2)r_3 + I\\ &= (r_1r_2+I)(r_3+I)= ((r_1+I)(r_2+I))(r_3+I).
    \end{align*}
    We also have that 
    $$(1+I)(r+I) = (1r+I) = (r1+I) = r+I.$$
    Furthermore 
    \begin{align*}
        (r_1+I)((r_2+I)+(r_3+I)) &= (r_1+I)((r_2+r_3)+I)=(r_1(r_2+r_3))+I = (r_1r_2+r_1r_3)+I\\ &= (r_1r_2+I) + (r_1r_3+I) = (r_1+I)(r_2+I)+(r_1+I)(r_3+I).
    \end{align*}
    Suppose $R$ is commutative. Then 
    $$(r_1+I)(r_2+I) = (r_1r_2)+I  = (r_2r_1)+I = (r_2+I)(r_1+I).$$
\end{proof}
\begin{corollary}
    The canonical surjective group map $\pi: R\mapsto R/I$ is a ring map.
\end{corollary}
\begin{proof}
    Let $r_1,r_2\in R$. Then $\pi(r_1r_2)=r_1r_2+I=(r_1+I)(r_2+I)=\pi(r_1)\pi(r_2)$. Lastly $\pi(1)=1+I$.
\end{proof}
\subsubsection{Noetherian Modules and Noetherian Rings}
\begin{definition}
    Let $X$ be a set. Let $\leq \subset X\times X$, where we write $x\leq y$ if $(x,y)\in \leq$. $\leq$ is a \textit{partial order}, if it is 1. reflexive, 
 2. antisymmetric and 3. transitive, i.e. for $x,y,z \in X$ 
 \begin{align*}
     &1. \quad x\leq x,\\
     &2. \quad x \leq y \ \& \ y \leq x \implies x = y,\\
     &3. \quad x \leq y \ \& \ y \leq z \implies x \leq z.
 \end{align*}
 We write $x\geq y$ if $y\leq x$.
\end{definition}
\begin{remark}
    Given a partial order $\leq$, we have that $\geq$ is a partial order as well.
\end{remark}
\begin{example}
    Let $X$ be a set and $\mathcal{X}\subset 2^X$. Then $\subset := \left\{(A,B)\in \mathcal{X}\times \mathcal{X} : \forall x(x\in A \implies x\in B) \right\}$ defines a partial order on $\mathcal{X}$.\\
    Another example is that of $\leq$ on $\N$,$\Z$ or $\Q$. 
\end{example}
\begin{definition}
    Let $X$ be a set with a partial order and $\{x_i\}_{i\in \N} \subset X$ be a sequence. We say that $\{x_i\}_{i\in\N}$ is \textit{descending with respect to $\leq$} if $x_i \geq x_{i+1}$ for every $i\geq 0$ and \textit{ascending with respect to $\leq$} if $x_i \leq x_{i+1}$ for every $i\geq 0$. A sequence $\{x_i\}_{i\in \N}$ is said to \textit{stabilize} if there is a non-negative integer $n$ such that $x_n = x_{n+d}$ for every $d\geq 0$. 
\end{definition}
\begin{definition}
    Let $X$ be a set with a partial order $\leq$. A subset $Y$ of $X$ is called a \textit{chain} if for every $c,d\in Y$, $c\leq d$ or $d\leq c$.  
\end{definition}
\begin{remark}
    Any ascending/descending sequence $\{x_i\}_{i\in \N}$ is a chain and is denoted 
    $$x_1\leq x_2\leq \dots \text{ respectively } x_1\geq x_2 \geq \dots,$$
    these are called \textit{ascending/descending chains}
\end{remark}
\begin{definition}\label{NoetherianArtinianDef}
    Let $M$ be a left/right $R$-module and 
    $$\mathcal{M} := \left\{ N\in 2^M : N \text{ is a left/right submodule of } M \right\}$$
    be a chain. We say that $M$ is \textit{left/right noetherian} if every ascending chain stabilizes and \textit{left/right artinian} if every descending chain stabilizes. A ring $S$ is left/right noetherian/artinian, if it is noetherian/artinian as a left/right $S$-module or simply artinian/noetherian if is both left and right artinian/noetherian.  
\end{definition}
\begin{definition}
    A simple left/right $R$-module is one whose only submodules are $0$ and $M$.
\end{definition}
\begin{example}
    Any simple left/right $R$-module $M$ is noetherian/artinian. A family of submodules of $M$ is of the form $\{0,M\},\{M\}$ or $\{0\}$. These are all finite sets, hence any chain $C$ is finite and thus has an upper/lower bound in $C$.\\
    Simple rings are simple modules. This means division rings and fields are both noetherian/artinian.
\end{example}
\begin{lemma}\label{UnionOverChainOfSubmodulesIsSubmodule}
    Let $M$ be a left/right $R$-module. Consider a chain $C$ of submodules of $M$. Then 
    $$\bigcup_{N\in C} N$$
    is a submodule.
\end{lemma}
\begin{proof}
    Let $m_1,m_2\in \bigcup_{N\in C} N$. $m_1\in N_1$ and $m_2\in N_2$ for some $N_1,N_2\in C$. WLOG $N_1\subset N_2$, hence $m_1\in N_2$, which means $m_1+m_2\in N_2 \subset \bigcup_{N\in C} N$. Clearly $0\in \bigcup_{N\in C} N$. Let $r\in R$, clearly $rm_1\in \bigcup_{N\in C} N$.
\end{proof}
We give the following axiom which one check is equivalent to the axiom of choice 
\begin{axioms}\label{ZornsLemma}(Zorn's Lemma)
    Let $X\neq \emptyset$ be a set with a partial order $\leq$ such that for every chain $C\subset X$ there exists an $x\in C$ such that $c\leq x$ for every $c\in C$, (i.e. there is an upper bound $x$ for $C$ in $C$). Then there is a maximal element in $m \in X$, i.e. for every $y \in X$ if $m\leq y$, then $m=y$.
\end{axioms}
\begin{example}
    In certain situations we do not need to assume Zorn's Lemma. 
    \begin{enumerate}
        \item Suppose $X$ is a non-empty finite set with $n$ elements and a partial order $\leq$. Then $X$ has a maximal element. Indeed, this is easily proven by induction in $n$. If $X$ has one element this is trivially maximal. Consider for $n\geq1$ $X= \{x_1,\dots,x_{n+1}\}$. Then by induction $\{x_1,\dots,x_n\}$ has a maximal element $x_i$. Then $\max_\leq(x_i,x_{n+1})$ is a maximal element of $X$. 
        \item A topology $\tau$ on some set $X$ has $X$ as a maximal element
    \end{enumerate}
     
\end{example}
We give a reformulation of every chain having a maximal/minimal element
\begin{lemma}
    Let $X\neq \emptyset$ be a set with a partial ordering $\leq$. Every ascending/descending sequence in $X$ stabilizes if and only if every chain $C$ in $X$ has a upper/lower bound in $C$.
\end{lemma}
\begin{proof}
    We only check the ascending case since a descending sequence is just an ascending sequence with respect to $>$ and a minimal element is just a maximal element with respect to $>$.\\
    "$\implies$":We prove the contrapositive. Suppose $C\subset X$ is a chain that does not have an upper bound in $C$. Let $c_1\in C$. Then there exists $c_2 \in C$ such that $c_1 < c_2$. Continuing this process recursively we get a sequence $\{c_i\}_{i\in \N}$ such that $c_i<c_{i+1}$ for every $i\geq 0$, hence this is a sequence in $X$ that does not stabilize.\\
    "$\impliedby$": Let $\{x_i\}_{i\in \N}$ be an ascending sequence in $X$. Then $\{x_i\}_{i\in \N}$ is a chain. Then there exists a $x_n\in \{x_i\}_{i\in \N}$ such that $x_j\leq x_n$ for every $j\geq 1$. Now since $x_n \leq x_{n+d}$ for every $d\geq 0$ it follows that $x_n=x_{n+d}$ for every $d\geq0$, hence $\{x_i\}_{i\in \N}$ stabilizes. 
\end{proof}
The consequence of the above lemma is the following 
\begin{proposition}\label{NoetherianRespArtinianImpliesMaximalMinimalElement}
    If $M$ is a left/right $R$-module and $X$ is a non-empty set of submodules of $M$, and $M$ is noetherian/artinian then $X$ has maximal/minimal element
\end{proposition}
\begin{proof}
    If $M$ is noetherian/artinian, then every chain $C$ in $X$ has an upper/lower bound in $C$. Using Zorn's Lemma, this implies that $X$ has a maximal/minimal element.  
\end{proof}
\begin{corollary}
    Every left/right noetherian ring $R$ has a maximal left/right ideal. In particular every noetherian ring has a maximal ideal. 
\end{corollary}
\begin{proof}
    Let 
    $$X = \{ I \subsetneq R : I \text{ a left ideal in } R\}.$$
    Then it follows by the above proposition that this set has a maximal element, which by definition will be a maximal left/right ideal of $R$. 
    Defining 
    $$Y = \{I \subsetneq R : I \text{ an ideal in } R\},$$
    the above proposition implies the existence of a maximal ideal.
\end{proof}
\begin{theorem}\label{NoetherianIffEverySubmoduleFinitelyGenerated}
    Let $M$ be a left/right $R$ module. Then $M$ is left/right noetherian if ad only if every submodule of $M$ is finitely generated over $R$.
\end{theorem}
\begin{proof}
    "$\implies$": Suppose $M$ is not finitely generated. Let $m_1\in M$. Then $M_1:= Rm_1 \subsetneq M$ since $M$ is not finitely generated. We then recursively define $M_{n+1} = Rm_{n+1} +M_n$ where $m_{n+1}\in M\setminus M_n$ which we can do since every $M_n$ is finitely generated and thus by assumption a proper submodule of $M$. We thus obtain an strictly ascending chain of submodules
    $$M_1\subsetneq M_2\subsetneq M_3\subsetneq \dots$$
    Hence this is an ascending chain that does not stabilize.\\
    "$\impliedby$": Suppose every submodule of $M$ is finitely generated over $R$. Let an ascending chain of submodules, say
    $$M_1\subset M_2\subset \dots$$
    be given. Then by Lemma~\ref{UnionOverChainOfSubmodulesIsSubmodule} $N:=\bigcup_{1}^\infty M_n$ is a submodule. 
    By assumption $N = \sum_1^m Rn_k$ for some $n_1,\dots,n_m \in N$. For each $k\in \{1,\dots,m\}$ there is a $j(k)\geq 0$ such that $n_k\in M_{j(k)}$. Let 
    $p = \max\{j(1),\dots,j(k)\}$. Thus we have that $n_1,\dots, n_m\in M_p$. Hence, $M_p = \sum_1^m Rn_k = N$. Hence for $d\geq 0$, $N=M_p \subset M_{p+d} $, implying $M_p = M_{p+d}$. In other words, every ascending chain of submodules of $M$ stabilizes.
\end{proof}
\begin{corollary}
    A PID is a Noetherian ring.
\end{corollary}
\begin{lemma}\label{ModuleNoetherianIffQuotientModuleAndSubModuleNoetherian}
    Let a left/right $R$-module $M$ and a submodule $N\subset M$ be given. Then $M$ is left/right noetherian/artinian if and only if $N$ and $M/N$ is left/right noetherian/artinian. 
\end{lemma}
\begin{proof}
    We prove the left noetherian version.\\
    "$\implies$": An ascending chain of submodules of $N$, is in particular an ascending chain of submodules of $M$, which by assumption stabilizes. A chain of submodules in $M/N$ is of the form 
    $$L_1/N\subset L_2/N\subset L_3/N \subset\dots$$
    where $L_i\subset M$ is a submodule of $M$ containing $N$ by Lemma~\ref{SubmodulesOfQuotientModule}. For some $n\geq 0$, $L_n = L_{n+d}$ for every $d\geq 0$, hence $L_n/N = L_{n+d}/N$ for every $d\geq 0$.\\
    "$\impliedby$": Let $M_1\subset M_2\subset M_3\subset \dots $. Then 
    $$M_1+N\subset M_2 + N\subset \dots $$
    is an ascending chain of submodules of $M$ containing $N$. Hence 
    $$(M_1+N)/N \subset (M_2+N)/N \subset \dots$$
    is an ascending chain of submodules of $M/N$. Hence for some $n\geq 1$, $M_{n}+N=M_{n+d}+N$ for every $d\geq 0$. Then by Lemma~\ref{SubmodulesOfQuotientModule} $M_n+N =M_{n+d} +N$ for every $d\geq 0$. We also have that 
    $$M_1\cap N \subset M_2\cap N\subset \dots$$
    is an ascending chain of submodules of $N$. Thus for some $m\geq 1$, $M_m \cap N = M_{m+d} \cap N$ for every $d\geq 0$. Put $k = n+m$ and let $d\geq 0$ be given. Let $x\in M_{k+d}$. Then $x \in M_{k+d}+N = M_k+N$. Hence $x = y+z$ for some $y\in M_k$ and $z \in N$. It thus follows that $z = x-y \in N\cap M_{k+d} = N \cap M_k$. In particular, $z \in M_k$, hence $x = z+y \in M_k$, hence $M_k = M_{k+d}$. Thus $M_1\subset M_2\subset \dots$ stabilizes\\
    We proceed to prove the left artinian case. "$\implies$" is dual to the noetherian case.\\
    "$\impliedby$": Consider a descending chain of submodules of $M$,
    $$M_1\supset M_2\supset\dots $$
    Similarly as above $M_1+N\supset M_2+N \supset \dots$ and $M_1\cap N \supset M_2\cap N\supset \dots$ give descending chain stabilizing some positive $n$ respectively $m$. Put $k=n+m$ and let $d\geq 0$. Consider $x\in M_k$. Then in particular $x \in M_k + N = M_{k+d} + N$. Thus $x = y+z$ for some $y\in M_{k+d}$ and $z\in N$. Then $z = x-y \in M_k \cap N =M_{k+d}\cap N$, hence $x =z+y \in M_{k+d}$, hence $M_k = M_{k+d}$, meaning $M_1\subset M_2\subset \dots$ stabilizes. 
\end{proof}
\subsubsection{A First Look at Algebras over Rings}
\begin{definition}
    By a \textit{ring extension $S$ over $R$} we mean a ring $S$ containing a ring $R$ as a subring. Such a pair is denoted $S\supset R$. Such a pair is a \textit{field extension} if $S$ is a field and $R$ is a subfield.
\end{definition}
\begin{remark}
    This defines a partial order. Indeed any ring is a ring extension of itself. If $S\supset R$ and $R\supset S$ then $R=S$. Lastly, if $T\supset S$ and $S\supset R$ are ring extensions, then $T\supset R$ and $R$ is a subring of $T$.
\end{remark}
\begin{definition}
    Let $R$ be a ring. We define \textit{the center of $R$} to be the set 
    $$Z(R):= \{x \in R : xy =yx \textit{ for every } y \in R\}$$
\end{definition}
\begin{remark}
    The center $R$ is a subring of $R$. Indeed, let $x,x'\in Z(R)$. Then given $y\in R,$ 
    $$y(x+x') = yx+yx' = xy+x'y= (x+x')y \implies x+x' \in Z(R).$$
    We also have that $y0=0=0y$ for every $y\in R$, hence $0\in Z(R)$. In addition,
    $$y(-x) = -(yx) = -xy\implies -x \in Z(R).$$
    This means $Z(R)$ is a subgroup of $R$. Furthermore, 
    $$y(xx')= (yx)x'=(xy)x'=x(yx') = x(x'y)=(xx')y \implies xx' \in Z(R)$$
    and $y1=y=1y$, hence $1\in Z(R)$. We thus see that $Z(R)$ is the largest commutative subring of $R$
\end{remark}
\begin{definition}
    Let $R$ be a commutative ring. A ring $A$ is an $R$-algebra, if $(A,+)$ is an $R$-module such that 
    $$r(a_1a_2)=(ra_1)a_2$$ 
    for every $r\in R$, $a_1,a_2\in A$.
\end{definition}
\begin{remark}
    An equivalent definition is that an algebra is a ring $A$ together with ring homomorphism $R\rightarrow A$ whose image is contained in $Z(A)$.
\end{remark}
\begin{definition}
    Let $A$ be an $R$-algebra. A subset $B$ of $A$ is \textit{an $R$-subalgebra of $A$} if it is a subring of $A$ and an $R$-submodule of $A$.
\end{definition}
\begin{remark}
    One easily checks that indeed an $R$-subalgebra is itself an $R$-algebra.
\end{remark}
\begin{definition}
    Let $S$ be an algebra over commutative ring $R$ and $s_1,\dots,s_n\in Z(S)$. We define the \textit{algebra over $R$ generated by $s_1,\dots,s_n$} to be the set 
    \begin{align*}
        R[s_1,\dots,s_n] &:= \left\{ \sum_{v=(v_1,\dots,v_n)\in \N^n} a_vs_1^{v_1}\cdots s_n^{v_n} : a_v\in R \forall v\in \N^n, a_v = 0 \text{ for all but finitely many } v\in \N^n \right\}.
    \end{align*}
    If $S\supset R$, then the above set is the \textit{ring extension over $R$ generated by $s_1\dots,s_n$}
\end{definition}
\begin{lemma}
    Let $S$ be an algebra over a commutative ring $R$ and $s_1,\dots,s_n\in Z(S)$. $R[s_1,\dots,s_n]$ is an $R$-subalgebra of $S$. Furthermore, $R[s_1,\dots,s_n]$ is the smallest subalgebra over $R$ containing $s_1,\dots,s_n$.   
\end{lemma}
\begin{proof}
    Clearly $1,0\in R[s_1,\dots,s_n]$. Let $\sum_{v\in \N^n} a_vs_1^{v_1}\cdots s_n^{v_n}, \sum_{v\in \N^n} b_vs_1^{v_1}\cdots s_n^{v_n}\in R[s_1,\dots,s_n]$. Then 
    \begin{align*}
        \sum_{v\in \N^n} a_vs_1^{v_1}\cdots s_n^{v_n}+\sum_{v\in \N^n} b_vs_1^{v_1}\cdots s_n^{v_n} &= \sum_{v\in \N^n} \left(a_vs_1^{v_1}\cdots s_n^{v_n}+b_vs_1^{v_1}\cdots s_n^{v_n}\right)\\ &= \sum_{v\in\N^n} (a_v+b_v)s_1^{v_1}\cdots s_n^{v_n} \in R[s_1,\dots,s_n].
    \end{align*}
    and 
    \begin{align*}
        \left(\sum_{v\in \N^n} a_vs_1^{v_1}\cdots s_n^{v_n}\right)\left(\sum_{w\in \N^n} b_ws_1^{w_1}\cdots s_n^{w_n}\right) &= \sum_{v\in \N^n}\sum_{w \in \N^n} a_vb_w s_1^{v_1+w_1}\cdots s_n^{v_n+w_n} \\
        &= \sum_{u\in \N^n} \left(\sum_{v,w\in \N^n : v+w = u} a_vb_w \right)s_1^{u_1}\cdots s_n^{u_n}\in R[s_1,\dots,s_n]
    \end{align*}
    Let $r\in R$. Then 
    $$r\sum_{v\in\N^n} a_vs_1^{v_1}\cdots s_n^{v_n} = \sum_{v\in\N^n} (ra_v)s_1^{v_1}\cdots s_n^{v_n}\in R[s_1,\dots,s_n].$$
\end{proof}
\begin{lemma}
    Let $S\supset R$ be a ring extension of commutative rings and $s_1,\dots,s_n,t_1,\dots,t_m\in S$. Then $R[s_1,\dots,s_n,t_1,\dots,t_m]=R[s_1,\dots,s_n][t_1,\dots,t_m].$
\end{lemma}
\begin{proof}
    We already have that $R[s_1,\dots,s_n,t_1,\dots,t_m]\supset R[s_1,\dots,s_n]$, hence $R[s_1,\dots,s_n,t_1,\dots,t_m]\supset R[s_1,\dots,s_n][t_1,\dots,t_m]$. Let $\sum_{(v,w)\in\N^{n+m}} a_vs_1^{v_1}\cdots s_n^{v_n}t_1^{w_1}\cdots t_m^{w_m}\in R[s_1,\dots,s_n,t_1,\dots,t_m]$. We then see that 
    $$\sum_{(v,w)\in\N^{n+m}} a_{vw}s_1^{v_1}\cdots s_n^{v_n}t_1^{w_1}\cdots t_m^{w_m} = \sum_{w\in \N^m}\left[\sum_{v\in \N^n} a_{vw}s_1^{v_1}\cdots s_n^{v_n}\right] t_1^{w_1}\cdots t_m^{w_m}\in R[s_1,\dots,s_n][t_1,\dots,t_m].$$
\end{proof}
\begin{definition}
    Let $R$ be a ring and consider $R$-algebras $A,B$. A map $\sigma : A\rightarrow B$ is an \textit{$R$-algebra homomorphism} if it is both a ring homomorphism and an $R$-module homomorphism.
\end{definition}
\begin{lemma}
    Let $R$ be a ring, consider $R$-algebras $A,B$ and a map $\sigma : A\rightarrow B$. Then $\sigma$ is an $R$-algebra homomorphism if and only if $\sigma$ is a multiplicative map fixing $R$, i.e. for every $a_1,a_2\in A$, $\sigma(a_1a_2)=\sigma(a_1)\sigma(a_2)$ and for every $r\in R$, $\sigma(r) = r$.
\end{lemma}
\begin{proof}
    {\Large DO AT SOME POINT}.
\end{proof}
\subsection{Abelian Categories}
\subsubsection{Preadditive Categories}
\begin{definition}
    A category $\pazocal{C}$ is \textit{preadditive} if every $\Hom(A,B)$ is an additive group and 
    $$f(g+h)=fg+fh, \quad (f+g)h=fh+gh.$$
\end{definition}
\begin{example}
    \begin{enumerate}
        \item 
    \end{enumerate}
\end{example}
\subsubsection{Initial Objects, Terminal Objects \& Zero Objects}
\begin{definition}
    An \textit{initial object} $I$ in a category $\pazocal{C}$ is an object in $C$ such that for every object $X$, $\Hom(I,X)$ is singleton. A \textit{terminal object} in $\pazocal{C}$ is a an initial object in $\pazocal{C}\op$. An element in $\pazocal{C}$ is a \textit{zero object} if it is both initial and terminal. 
\end{definition}
\subsubsection{Additive, Pre-abelian \& Abelian Categories}
\begin{definition}
    A preadditive category $\pazocal{C}$ with zero objects is called \textit{additive} if it admits binary coproducts.{\LARGE by some theorem} 
\end{definition}
\begin{definition}
    In an additive category $\pazocal{C}$ of a pair of objects $A,B$ in $\pazocal{C}$ a \textit{biproduct} of $A$ and $B$ is an object $A\oplus B$ such that 
    $$\begin{tikzcd}
        A \arrow[r, shift left, "p_1"] & A\oplus B \arrow[l, shift left, "i_1"] \arrow[r,shift left, "p_2"] & B \arrow[l, shift left, "i_2"]
    \end{tikzcd}$$
    $i_1p_1 = \fone_A$, $p_2i_2 = \fone_B$ and $p_1i_1+ i_2p_2=\fone_{A\oplus B}$
\end{definition}
\begin{lemma}
    Any additive category admits a biproduct. In particular it admits a product and finite coproducts and products are isomorphic. 
\end{lemma}
\begin{proof}
    
\end{proof}
\begin{definition}
    In an additive category $\pazocal{C}$ a \textit{kernel} of a morphism $f: A \rightarrow B$ is an object $K$ and a morphism $k: K \rightarrow A $ such that 
    $$\begin{tikzcd}
        K \arrow[r, "0"] \arrow[d, "k"] & B\\
        A \arrow[ur, "f"]
    \end{tikzcd}$$
    commutes. 
    A \textit{cokernel} in $\pazocal{C}$ is a kernel in $\pazocal{C}\op$
\end{definition}
    
\subsection{Homological Algebra}
\subsubsection{Exact Sequences}
    \begin{definition}
        \textit{A sequence of left/right $R$-modules}, is a collection of pairs $$\{(M_i,\rho_i) : i\in \Z, M_i \text{ is a left/right } R\text{-module }, \rho_i\in \Hom(M_i,M_{i+1})\}.$$ A sequence is finite if $M_i=0$ for every but finitely many $i$. A sequence is in general denoted
        $$\begin{tikzcd}
            \cdots \arrow[r, "\rho_{i-1}"]  & M_i \arrow[r, "\rho_{i}"]  & M_{i+1} \arrow[r, "\rho_{i+1}"] & \cdots
        \end{tikzcd}$$
        When the maps are obvious we opt to not explicitly denote them. When a sequence is finite we of often opt to denote it
        $$\begin{tikzcd}
            0 \arrow[r] & M_s \arrow[r] & M_{s+1} \arrow[r] & \cdots \arrow[r] & M_{b-1}\arrow[r] & M_b \arrow[r] & 0
        \end{tikzcd}$$
        Where $s,b$ are respectively the largest and smallest index for which $M_s\neq 0$ and $M_b\neq 0$.
    \end{definition}
    \begin{definition}
        A finite sequence
        $$\begin{tikzcd}
            M \arrow[r,"\rho"] & M' \arrow[r,"\rho'"] & M''
        \end{tikzcd}$$
        is said to be \textit{exact (at $M'$)} if $\im \ \rho = \ker \rho'$.
        In general a sequence 
         $$\begin{tikzcd}
            \cdots \arrow[r, "\rho_{i-1}"]  & M_i \arrow[r, "\rho_{i}"]  & M_{i+1} \arrow[r, "\rho_{i+1}"] & \cdots
        \end{tikzcd}$$
        is said to be \textit{exact} if 
        $$\begin{tikzcd}
            M_{i-1} \arrow[r,"\rho_i"] & M_i \arrow[r,"\rho_{i}"] & M_{i+1} 
        \end{tikzcd}$$
        is exact for each $i\in \Z$
    \end{definition}
    \begin{remark}
        Equivalently a sequence $\begin{tikzcd} M\arrow[r,"\rho"] & M' \arrow[r,"\rho'"] & M'' \end{tikzcd}$ is exact if $\rho'\circ \rho =0$.
    \end{remark}
    \begin{lemma}\label{ExactnessIsEquivalentToInjFirstSESSurjSecondSES}
        Consider a sequence 
        $$\begin{tikzcd}
            0 \arrow[r] &M \arrow[r,"\rho"] & M' \arrow[r,"\rho'"] & M'' \arrow[r] & 0
        \end{tikzcd}$$
        The following are equivalent:
        \begin{enumerate}
            \item The sequence is exact
            \item $\rho $ is injective and $\rho'$ is surjective 
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        "1.$\implies$ 2.": by exactness $\ker \rho = \im\ 0 = 0$ and $\im\ \rho' =\ker \ 0 = M''$.
        "2.$\implies$ 1.": $\im \ 0 = 0 = \ker \ \rho $ and $\ker \ 0 = M'' = \im\ \rho'$ \end{proof}
    \begin{corollary}
        Given a left/right $R$-module map $\rho : M \rightarrow N$, the sequence
        $$\begin{tikzcd}
            0 \arrow[r] & \ker \ \rho \arrow[hookrightarrow,r] & M \arrow[twoheadrightarrow, r] & \im \ \rho \arrow[r] &0 
        \end{tikzcd}$$
        is exact.
    \end{corollary}
    \subsubsection{Isomorphism Theorems}
    We are going to construct ways of identifying certain algebraic structures given certain maps. We will develop these theorems for groups, rings, modules and algebra homomorphisms in a sense separately. However, in a certain categorical setting {\Large which I don't know} we would be able to develop them all at once.   
\begin{definition}
    An \textit{isomorphism of groups} is a bijective group homomorphism. A \textit{isomorphism of rings} is a bijective ring homomorphism. If there exists an isomorphism between groups/rings $G,H$/$R,S$ then $G,H$/$R,S$ are said to be \textit{isomorphic as groups/rings} and we write $G\simeq H$/$R\simeq S$, when this does not lead to confusion.   
\end{definition}
\begin{remark}
    One easily check that the inverse of of a bijective group/ring homomorphism is automatically a group/ring homomorphism itself. Indeed, if $\rho: G\rightarrow H$ is a bijective monoid map, let $h,h'\in H$, then for some $g,g'\in G$, $h=\rho(g)$ and $h' = \rho(g')$. Then 
    $$\rho^{-1}(hh')=\rho^{-1}(\rho(g)\rho(g'))=\rho^{-1}(\rho(gg'))=gg'=\rho^{-1}(h)\rho^{-1}(h').$$
    Lastly 
    $$\rho^{-1}(e_H) = \rho^{-1}(\rho(e_G))= e_G.$$
\end{remark}
\begin{example}
    Let $R$ be a commutative ring. Consider the identity map
    \begin{gather*}
        \text{id} : (R,\cdot) \rightarrow (R,\ast)= R^{(\text{op})},
        r \mapsto r
    \end{gather*}
    Then this clearly a bijective map of groups. Let $r,r'\in R$ then 
    $$\text{id}(rr') = rr' = r'r = \text{id}(r')\text{id}(r) = \text{id}(r)\ast \text{id}(r'),$$
    hence $R\simeq R^{(\text{op})}$.
\end{example}
\begin{lemma}\label{KernelIsZerosIffInjective}
    Let $\rho : G \rightarrow H$ be a group homomorphism. Then $\ker \ \rho = \{e\}$ if and only if $\rho$ is injective.
\end{lemma}
\begin{proof}
    "$\implies$": Let $g_1,g_2 \in G$ be given such that $\rho(g_1)=\rho(g_2)$. Then
    $$\rho\left(g_1g_2^{-1}\right)=\rho(g_1)\rho(g_2)^{-1} = e \implies g_1g_2^{-1} = e \implies g_1 = g_2.$$
    "$\impliedby$": Let $k\in \ker \ \rho$. Then $\rho(k) = e = \rho(e)$, implying $k=e$, hence $k\in \{e\}$.
\end{proof}
\begin{corollary}
    Let $\sigma : R \rightarrow S$ be a ring homomorphism. Then $\ker \ \sigma = 0$ if and only if $\sigma$ is injective.
\end{corollary}
\begin{lemma}\label{WellDefinednessOfQuotientGroupMaps}
    Let $G,H$ be groups, $\rho : G \rightarrow H$ a group homomorphism and $I\subset G$ $J\subset H$ be normal subgroups. Then $\varrho : G/I \rightarrow H/J$, $gI \mapsto \rho(g)J$ is a well-defined group homomorphism if and only if $\rho(I)\subset J$. 
\end{lemma}
\begin{proof}
    "$\implies$": Suppose $\varrho$ is a well-defined group homomorphism. Let $\rho(i)\in \rho(I)$. Then since $iI = eI$, 
    $$\rho(i)J =\varrho(iI) = \varrho(eI)=\rho(e)J = eJ,$$
    hence $\rho(i)\in J$.\\
    "$\impliedby$": Suppose $\rho(I)\subset J$. Let $g_1I=g_2I\in G/I$. Then $g_1g_2^{-1}\in I$, hence 
    $$\varrho(g_1I)\varrho(g_2I)^{-1} = \rho(g_1)\rho(g_2)^{-1}J = \rho\left(g_1g_2^{-1}\right)J = eJ \implies \varrho(g_1I)=\varrho(g_2I).$$
    We now check that $\varrho$ is a group homomorphism. Let $g_1I,g_2I\in G/I$. Then 
    $$\varrho((g_1I)(g_2I)) = \varrho(g_1g_2I)=\rho(g_1g_2)=\rho(g_1)\rho(g_2) = \varrho(g_1I)\varrho(g_2I).$$
\end{proof}
\begin{corollary}\label{WellDefinedQuotientRingHomFromRingHom}
    Let $R,S$ be rings, $\sigma : R \rightarrow S$ a ring homomorphism and $I\subset R$, $J\subset S$ be left/right ideals. Then the $\vartheta : R/I \rightarrow S/J,$ $r+I \mapsto \sigma(r)+J$ is a well-defined ring homomorphism if and only if $\sigma(I)\subset J$
\end{corollary}
\begin{proof}
    By the above proposition $\vartheta$ being a well-defined ring homomorphism implies $\sigma(I)\subset J$. Conversely if $\sigma(I)\subset J$ it remains to check that $\vartheta$ is a ring homomorphism. Let $r_1+I,r_2+I\in R/I$. Then 
    $$\vartheta((r_1+I)(r_2+I)) = \vartheta(r_1r_2+I)=\sigma(r_1r_2)+ J= \sigma(r_1)\sigma(r_2) +J= \vartheta(r_1+I)\vartheta(r_2+I).$$
    Furthermore, 
    $$\vartheta(1+I) = \sigma(1) + J = 1 + J.$$
\end{proof}
\begin{corollary}
    Let $R$ be a subring of a ring $S$. Let $I\subset R$ be a left/right ideal. Then $\sigma : R/I \rightarrow S/IS, r+I \mapsto r+SI$ is a well defined ring map in the left case and so is $r+ I \mapsto r+IS$ in the right case.
\end{corollary}
\begin{proof}
    Consider $\iota : R \hookrightarrow S, r\mapsto r$. Then since $\iota(I)\subset SI$, it follows that $\sigma : R/I \mapsto S/SI, r + I \mapsto \iota(r)+SI=r+SI$ is a well-defined ring map.
\end{proof}
\begin{corollary}\label{WellDefinedQuotientModuleHomFromModuleHom}
    Let $R$ be a ring, $M,N$ left/right $R$-modules, $\rho : M \rightarrow N$ a left/right module homomorphism and $L\subset M$, $K\subset N$ be submodules. Then the $\vartheta : M/L \rightarrow N/K,$ $m+L \mapsto \rho(m)+K$ is a well-defined module homomorphism if and only if $\rho(L)\subset K$
\end{corollary}
\begin{proof}
    The above proposition again tells us that if $\vartheta$ is a well defined module homomorphism, then $\rho(L)\subset K$. Conversely if $\rho(L)\subset K$, we just need to check the map is homogeneous of degree $1$. Indeed let $r\in R$, $m\in M$. Then 
    $$\vartheta(rm + L) =  \sigma(rm)+ K = r(\sigma(m)+K)=r\vartheta(m+L).$$
\end{proof}
\begin{proposition}\label{UniqueMapQuotientGroupMap}
    Let $G,H$ be groups, $\rho : G \rightarrow H$ a group homomorphism and $N \subset G$ a normal subgroup such that $N\subset \ker \ \rho$. Consider the canonical surjection $\pi : G \twoheadrightarrow G/N$, i.e. $g \mapsto gN$. Then $\varrho : G/N \rightarrow H$, $gN \mapsto \rho(g)$ is the unique group homomorphism with the property that 
    $\rho= \varrho \pi.$ In other words the diagram 
     $$
            \begin{tikzcd}
                G \arrow[rd, "\rho"] \arrow[r, "\pi", twoheadrightarrow] & G/N \arrow[d, "\exists! \varrho", dotted] \\
                & H
            \end{tikzcd}
    $$
    commutes.
\end{proposition}
\begin{proof}
    The assumption that $I\subset \ker \ \rho$ implies that $ \rho(I) = \{e\}$ hence the above lemma shows that $\varrho$ is a well-defined group homomorphism. Since $H/\{e\} = H$. Indeed, for $g\in G$,
    $$\varrho \pi (g) = \varrho(\pi(g))= \varrho(gI)=\rho(g) \implies \varrho\pi = \rho.$$
    \textbf{Uniqueness:} Consider another group homomorphism $\varrho': G/I \rightarrow H$ such that $\varrho \pi = \rho$. Let $gI \in G/I$. Then 
    $$\varrho'(gI) = \varrho'(\pi(g))=\rho(g) = \varrho(\pi(g))=\varrho(gI)\implies \varrho' = \varrho.$$
\end{proof}
\begin{corollary}\label{UniqueMapQuotientRingMap}
    Let $R,S$ be rings, $\sigma : R \rightarrow S$ and $I\subset R$ an ideal such that $I \subset \ker \ \sigma$. Define $\pi : R \rightarrow R/I $ to be the canonical surjection, i.e. $r\mapsto r + I$. Then there is a unique ring homomorphism $\varrho : R/I \rightarrow S$ such that  
    $$\sigma = \varrho \pi.$$
\end{corollary}
\begin{proof}
    This follows from the above proposition together with corollary~\ref{WellDefinedQuotientRingHomFromRingHom}. 
\end{proof}
\begin{corollary}\label{UniqueMapQuotientModuleMap}
    Let $R$ be a ring and $M,N$ be left/right $R$-modules. Consider $\rho : M \rightarrow N$ a left/right module map and $L\subset M$ a submodule such that $L \subset \ker \ \sigma$. Define $\pi : M \rightarrow M/L $ to be the canonical surjection, i.e. $r\mapsto r + L$. Then there is a unique ring homomorphism $\varrho : M/L \rightarrow N$ such that  
    $$\rho = \varrho \pi.$$
\end{corollary}
\begin{proof}
    This follows from the above proposition together with corollary~\ref{WellDefinedQuotientModuleHomFromModuleHom}. 
\end{proof}
\begin{theorem}\label{1stIsomorphismTheoremForGroups}(1st Isomorphism Theorem for Groups)\\
    Let $G,H$ be a group and $\rho : G \rightarrow H$ a group homomorphism. Then $G/\ker \ \rho \simeq \rho(G)$ via the group homomorphism $\varrho : G/\ker\ \rho \rightarrow H$, $g(\ker \ \rho)\mapsto \rho(g)$ 
\end{theorem}
\begin{proof}
    By Proposition~\ref{UniqueMapQuotientGroupMap}, $\varrho : G/\ker\ \rho \rightarrow H$, $g(\ker \ \rho)\mapsto \rho(g)$ is a well-defined group homomorphism. Then  $\overline{\varrho} : G/\ker\ \rho \rightarrow \overline{\varrho}(G/\ker\ \rho) $, $g(\ker \ \rho)\mapsto \varrho(g)$ is a surjective group homomorphism. We check that $\varrho(G/\ker\ \rho) = \rho(G)$. Indeed, let $\varrho(g(\ker \ \rho))\in \varrho(G/\ker\ \rho)$. Then $\varrho(g(\ker \ \rho))=\rho(g)\in \rho(G)$. Similarly, if $\rho(g)\in \rho(G)$, then $\rho(g)=\varrho(g(\ker\ \rho))\in \varrho(G/\ker \ \rho)$. It remains to check that $\overline{\varrho}$ is injective. Suppose $0=\overline{\varrho}(g(\ker \ \rho))$. Then $\rho(g) = 0$, which implies $g\in \ker \ \rho$, hence $g(\ker \ \rho) = e(\ker \ \rho)$, meaning $\ker \overline{\varrho} = 0$. By Lemma~\ref{KernelIsZerosIffInjective} $\overline{\varrho}$ is injective. We thus conclude that $\overline{\varrho}$ is a bijective group homomorphism, which means $$G/\ker \ \rho \simeq \overline{\varrho}(G/\ker\ \rho) =\varrho(G/\ker\ \rho) = \rho(G).$$
\end{proof}
\begin{corollary}\label{1stIsomorphismTheoremForRings}(1st Isomorphism Theorem for Rings)\\
    Let $R,S$ be rings and $\sigma: R \rightarrow S$ a ring homomorphism. Set $ I= \ker\ \sigma$. Then $R/I \simeq \sigma(R)$ via the ring homomorphism $\vartheta : R/I \rightarrow \sigma(R)$, $r+I \mapsto \sigma(r).$
\end{corollary}
\begin{proof}
    By Corollary~\ref{UniqueMapQuotientRingMap} and the above theorem $\vartheta$ is a bijective ring homomorphism, hence $R/I \simeq \sigma(R)$. 
\end{proof}
\begin{corollary}\label{1stIsomorphismTheoremForModules}(1st Isomorphism Theorem for Modules) Let $R$ be a ring, $M,N$ be a left/right $R$-modules and consider a left/right $R$-module homomorphism $\rho : M \rightarrow N$. Then $M/\ker \ \rho \simeq \rho(M)$ via the left/right $R$-module homomorphism $\varrho : M/\ker \ \rho \rightarrow \rho(M)$, $m + \ker \ \rho \mapsto \rho(m) $ 
\end{corollary}
\begin{proof}
    Theorem~\ref{1stIsomorphismTheoremForGroups} and Corollary~\ref{UniqueMapQuotientModuleMap} ensures that $\varrho : M/\ker\ \rho \rightarrow \rho(M)$ is a bijective module homomorphism.
\end{proof}
\begin{corollary}(1st Isomorphism Theorem for Algebras) Let $R$ be a commutative ring and $A,B$ be $R$-algebras. Consider an $R$-algebra homomorphism $\sigma: A\rightarrow B$. Then $A/\ker\ \sigma \simeq \sigma(A)$ via the $R$-algebra homomorphism $\vartheta : A/\ker\ \sigma \rightarrow \sigma(A), a+\ker\ \sigma \mapsto \sigma(a)$.
\end{corollary}
\begin{proposition}
    Let $G$ be a group, $N',N\subset G$  normal subgroups with $N'\subset N$. Then 
    $$\frac{G/N'}{N/N'}\simeq G/N.$$
\end{proposition}
\begin{proof}
    Consider the surjective group map 
    $$\pi : G \rightarrow G/N, g \mapsto gN'.$$
    Then by Lemma~\ref{WellDefinednessOfQuotientGroupMaps}
    $$\varpi : G/N' \rightarrow G/N, gN'\mapsto gN,$$
    is a surjective group map, since $N'\subset \ker \ \pi$. Clearly $N'/N\subset \ker \ \varpi$. Let $gN'\in \ker\ \varpi$. Then $gN = 0$, hence $g\in N$. Thus $gN' \in N/N'$. Then $\ker \ \varpi = N/N'$, hence by the 1st isomorphism theorem
    $$\frac{G/N'}{N/N'}=\frac{G/N'}{\ker\ \varpi}\overset{\varpi}{\simeq} G/N.$$
\end{proof}
\begin{corollary}
    Let $R$ be a ring $I,J\subset R$ left/right ideals with $J\subset I$. Then 
    $$\frac{R/J}{I/J}\simeq R/I.$$
\end{corollary}
\begin{proof}
    Follows from the 1st isomorphism for rings and the fact that $\pi$ is also a ring homomorphism. 
\end{proof}
\begin{corollary}
    Let $R$ be a ring $M$ a left/right $R$-module. Consider submodules $N,N'\subset M$ with $N'\subset N$. Then 
    $$\frac{M/N'}{N/N'}\simeq M/N.$$
\end{corollary}
\begin{proof}
    Follows from the 1st isomorphism theorem for modules together with the fact that $\pi$ is also a module homomorphism. 
\end{proof}
\begin{lemma}\label{NoetherSecondIsoForModules}
        Consider $_RL\leq {_RN} \leq {_RM}$. Then 
        $$\begin{tikzcd}
            0 \arrow[r] & N/L \arrow[r,hookrightarrow] & M/L \arrow[r, twoheadrightarrow] &M/N \arrow[r] &0
        \end{tikzcd}$$
        is exact
\end{lemma}
\begin{proof}
    This is an immediate consequence of Lemma~\ref{ExactnessIsEquivalentToInjFirstSESSurjSecondSES}
\end{proof}
\begin{theorem}\label{FirstNoetherIsoTheorem}
    Let $N,L\leq M$. Then $N/(N\cap L)\simeq (N+L)/N$.
\end{theorem}
\begin{proof}
    We get the chain of modules $N\cap L \leq N \leq N+L$, hence we have an exact sequence 
    $$\begin{tikzcd}
        0\arrow[r] & N/(N\cap L) \arrow[r,"\rho"] & (N+L)/(N\cap L) \arrow{r}{\rho'} & (N+L)/N \arrow[r] & 0
    \end{tikzcd}$$
    Substituting $(N+L)/(N\cap L)$ for $\im \ \rho$ we preserve exactness, i.e.
    $$\begin{tikzcd}
        0\arrow[r] & N/(N\cap L) \arrow[r,"\overline{\rho}"] & \im\ \rho \arrow{r}{\rho'\mid_{\im\ \rho}} & (N+L)/N \arrow[r] & 0
    \end{tikzcd}$$
    where $\overline{\rho}(n+N\cap L)= \rho(n+N\cap L).$ Note that for $n+ N\cap L \in \im\ \rho$, 
    $$\rho'\mid_{\im\ \rho}(n+N\cap L) = \rho' (n+N\cap L)= n +N=0 \implies \rho'\mid_{\im\ \rho} = 0.$$
    It follows that $\im \ \overline{\rho} = \ker\ \rho'\mid_{\im\ \rho} = \ker \ 0 = (N+L)/N$, hence $\rho$ is surjective. By exactness $\rho$ is also injective hence $N/(N\cap L) \overset{\rho}{\simeq} (N+L)/N$.
\end{proof}
\subsubsection{Free Modules}
\subsection{Vector Spaces}
\subsubsection{Finite Dimensional Vector Spaces}
\begin{definition}
    Let $V$ be an $n$-dimensional vector space over a field $K$ with basis $\pazocal{V}=\{v_1,\dots,v_n\}$. Let $\pazocal{W}=\{w_1,\dots,w_n\}\subset V$. Write $w_i = \sum_1^n a_{ij}v_j$ for suitable (unique!) $a_{ij}\in K$ and consider the matrix 
    $$_\pazocal{V}T_\pazocal{W} := (a_{ij})^T\in M_n(K).$$
    When $_\pazocal{V}T_\pazocal{W}$ is invertible we call it \textit{the basis transformation of $\pazocal{V}$ to $\pazocal{W}$} or \textit{a change-of-basis}  
\end{definition}
\begin{remark}
    We canonically identify $_\pazocal{V}T_\pazocal{W}$ with an endomorphism on $V$:
    \begin{gather*}
        _\pazocal{V}T_\pazocal{W} : V\rightarrow V,
        v = \sum_1^n \alpha_i v_i \mapsto \sum_1^n \left(\sum_1^n a_{ji}a_j \right)v_i
    \end{gather*}
    Note that 
    $$_\pazocal{V}T_\pazocal{W} v_i= \sum_1^n\left(\sum_1^n a_{kj}\delta_{ki}\right)v_j = \sum_1^n a_{ij}v_j = w_i $$
\end{remark}
\begin{theorem}\label{BasisTransformationTheorem}
    Let $V$ be an $n$-dimensional vector space over a field $K$ with basis $\pazocal{V}=\{v_1,\dots,v_n\}$. Consider $\pazocal{W}=\{w_1,\dots,w_n\}\subset V$. Then $\pazocal{W}$ is a basis of $V$ over $K$ if and only if $_\pazocal{V}T_\pazocal{W}$ is invertible.  
\end{theorem}
\begin{proof}
    "$\implies$": write $v_i = \sum_1^n b_{ij}w_j$. Note that 
    $w_i = _\pazocal{V}T_\pazocal{W} v_i = _\pazocal{V}T_\pazocal{W} {_\pazocal{W}}T_\pazocal{V} w_i$, and that $v_i = _\pazocal{W}T_\pazocal{V} w_i = _\pazocal{W}T_\pazocal{V} {_\pazocal{V}}T_\pazocal{W} v_i$. Since a module homomorphism is uniquely characterized by its behavior on the basis elements {\Large to be written} it follows that $_\pazocal{V}T_\pazocal{W} {_\pazocal{W}}T_\pazocal{V} = _\pazocal{W}T_\pazocal{V} {_\pazocal{V}}T_\pazocal{W} = I_n$, hence ${_\pazocal{V}}T_\pazocal{W}$ is invertible with inverse ${_\pazocal{W}}T_\pazocal{V}$.\\
    "$\impliedby$": by {\large An invertible linear map, maps a basis to a basis}, hence $\pazocal{W} = _\pazocal{V}T_\pazocal{W}(\pazocal{V})$ is a basis of $V$.
\end{proof}
\begin{lemma}\label{DimensionsOfExactSequence}
        Given an exact sequence of vector spaces
         $$\begin{tikzcd}
            0 \arrow[r] &V \arrow[r,"\rho"] & V' \arrow[r,"\rho'"] & V'' \arrow[r] & 0
        \end{tikzcd}$$
        we have that 
        $$\dim\ V= \dim \ V' + \dim \ V''$$
    \end{lemma}
    \begin{proof}
        This follows from the above Lemma~\ref{ExactnessIsEquivalentToInjFirstSESSurjSecondSES} and {\LARGE theorem not yet written about finite dimensional vector spaces}.
    \end{proof}
    \begin{proposition}
        For sequence of vector spaces
         $$\begin{tikzcd}
            0 \arrow[r] & V_1 \arrow[r,"\rho_1"] & V_2 \arrow[r,"\rho_2"] & V_3 \arrow[r,"\rho_3"] & V_4 \arrow[r] & 0
        \end{tikzcd}$$
        we have that 
        $\dim \ V_4 = \dim \ V_3 -\dim \ V_2 + \dim \ V_1$.
    \end{proposition}
    \begin{proof}
        Set $W := \im \ \rho_2 = \ker \ \rho_3$. Then 
        $$\begin{tikzcd}
            0 \arrow[r] & V_1 \arrow[r,"\rho_1"] & V_2 \arrow[r,"\rho_2"] & W \arrow[r] & 0 
        \end{tikzcd}$$
        and 
        $$\begin{tikzcd}
            0\arrow[r] &W \arrow[hookrightarrow,r] &V_3 \arrow[r, "\rho_3"] &V_4 \arrow[r] &0
        \end{tikzcd}$$
        are exact, hence by the above lemma $\dim \ V_2 = \dim \ V_1 +\dim \ W$ implying $\dim \ W = \dim \ V_2- \dim \ V_1$. Moreover, 
        $$\dim\ V_3 = \dim \ V_4 +\dim \ W = \dim \ V_4 +\dim \ V_2 - \dim \ V_1,$$
        hence $\dim \ V_4 = \dim \ V_3 -\dim \ V_2 +\dim \ V_1.$
    \end{proof}
    \begin{lemma}\label{SecondIsomorphismTheoremForVS}
        Let $U\subset W\subset V$ be vector spaces where $V/U$ finite dimensional. Then 
        $$\dim\ V/U = \dim \ V/W+ \dim \ W/U,$$
        In particular we get that $V/W$ and $W/U$ are finite dimensional. 
    \end{lemma}
    \begin{proof}
        This follows directly from the above proposition and Lemma~\ref{NoetherSecondIsoForModules}. 
    \end{proof}
    \begin{lemma}
        Let 
        $$\begin{tikzcd}
            0 \arrow[r,"\rho_0"] &V_1\arrow[r,"\rho_1"] & \cdots \arrow[r,"\rho_{n-1}"] & V_n\arrow[r,"\rho_n"] & 0
        \end{tikzcd}$$
        be an exact sequence of finite dimensional vector spaces. Then $\sum_1^n (-1)^i \dim\ V_i = 0$
    \end{lemma}
    \begin{proof}
        In the case $n=1$, it's easy to see that $V_1=0$. Denote the first $0$-map
        In general for a finite sequence of elements in an additive group, $a_0,\dots,a_n$, say $\sum_{1}^{1} (-1)^i(a_{i-1}+a_{i})= -a_0+(-1)^{n-1} a_n$. By the rank nullity theorem and exactness we have for each $i\in\{1,\dots,n\}$ that $\dim\ V_i = \dim\ \ker \ \rho_i +\dim \ \im \ \rho_i = \dim \ \im \ \rho_{i-1 } + \dim \ \im \ \rho_i$. Hence picking $a_i = \dim \ \im\ \rho_i$, it follows that 
        \begin{align*}
            \sum_1^n (-1)^i\dim\ V_i &= \sum_1^n (-1)^i(a_{i-1}+a_i)= -a_0 + (-1)^na_n = -\dim\ \im \ \rho_0 +(-1)^n \dim\ \im \ \rho_n\\ &= -\dim\ \im \ 0 +(-1)^n \dim\ \im \ 0 = 0.
        \end{align*}
    \end{proof}
    \begin{lemma}\label{CodimensionOfIntersectionIsBoundedBySumOfCodimensions}
        Let $V$ be a finite dimensional vector space and $V_1,\dots,V_n$ be subspaces. Then 
        $$\codim \ \bigcap_1^n V_i \leq \sum_1^n \codim \ V_i.$$
    \end{lemma}
    \begin{proof}
        Pick a basis of $V$, $B$ and bases of $V_1,\dots,V_n,\bigcap_1^n V_i$, denoted respectively $\pazocal{V}_1,\dots,\pazocal{V}_n,\pazocal{V}\subset B$. Then 
        $$\codim\ \bigcap_1^n V_i = \#\left( B\setminus \pazocal{V}\right)= \#\left(\bigcup_1^n B\setminus \pazocal{V}_i \right)\leq \sum_1^n \#\left(B\setminus \pazocal{V}_i\right)=\sum_1^n \codim\ V_i.$$
    \end{proof}
\subsubsection{Projective Space}
    \begin{definition}\label{ProjectiveEquivalence}
        Let $V$ be a vector space over some field $K$. For $v,w\in V\setminus 0$ we write $v\sim w$ if there exists a $\lambda\in K\setminus 0$ such that $w = \lambda v$
    \end{definition}
    \begin{remark}
        This an equivalence relation. Indeed $v = 1\cdot v$, hence $v\sim v$. If $v\sim w$, then $w =\lambda v$, hence $v= \lambda^{-1}w$, meaning $w\sim v$. Suppose $v\sim w$ and $w\sim u$. Then $w=\lambda v$ and $u = \mu w$, hence $u = \mu\lambda v$, implying $v\sim u$. 
    \end{remark}
    \begin{definition}
        We define \textit{the projective space of $V$ over $K$} to be the set 
        $$\Pp(V) := (V\setminus 0)/\sim.$$
        We furthermore define $\Pp^n:=\Pp^n(K):= \Pp(K^{n+1})$ which is called \textit{the projective $n$-space over $K$}. We denote an element $[v]=[(v_1,\dots,v_{n+1})]\in \Pp^n$ by $[v_1,\dots,v_n]$.   We call $\Pp^1$ \textit{the projective line over $K$} and $\Pp^2$ \textit{the projective plane over $K$}.   
    \end{definition}
    \begin{remark}
        Note that $[\lambda v]=[v]$ for every $\lambda\in K\setminus 0$ and $[v]\in \Pp(V)$, hence $[\lambda v_1,\dots,\lambda v_{n+1}]=[v_1,\dots,v_{n+1}]$ for every $[v_1,\dots,v_{n+1}]\in\Pp^n$.\\
        Consider the category of Vector Spaces with morphism being maps that are homogeneous of degree $1$. Consider also category of sets $P$ with a $(K^\ast,\cdot)$-action, satisfying $p=\lambda p$ for every $p\in P$, with morphisms being maps that are homogeneous of degree $1$ with respect to this $K^\ast$-action. Then $V\mapsto \Pp(V)$ and $f: V\rightarrow W\mapsto \widehat{f} : \Pp(V)\rightarrow \Pp(W), [v]\mapsto [f(v)]$, defines a functor. Restricting all sets to be topological spaces (with vector spaces being topological vector spaces) and all maps to be continuous, we get two subcategories of category of topological spaces such that the functor $(V,f)\mapsto (\Pp(f),\widehat{f})$ restricts to a functor between these categories. Indeed if $f: X\rightarrow Y$ is continuous and $\pi: X\rightarrow X/\sim_X$, $\tau: Y\rightarrow Y/\sim_Y$, denotes quotient maps to some quotient spaces and $\widehat{f}: X/\sim_X \rightarrow Y/\sim_Y, [x]\mapsto [f(x)]$ is well-defined, then $\widehat{f}\pi = \tau f $, hence $\widehat{f}$ is continuous, by the universal property of quotient space {\large perhaps write some point set topology?}.    
    \end{remark}
    \begin{definition}
        For each $i\in\{1,\dots,n+1\}$ we define \textit{the $i$'th copy of $K^n$ in $\Pp^n$} to be the set 
        $$U_i := \left\{[v_1,\dots,v_{n+1}]\in \Pp^n : v_i\neq 0 \right\}.$$
        We furthermore define \textit{the $i$'th hyperplane at infinity in $\Pp^n$} to be the set 
        $$H_{\infty,i}:=\left\{[v_1,\dots,v_{n+1}]\in \Pp^n: v_i=0\right\}.$$
        We define \text{the hyperplane at infinity in $\Pp^n$} to be $H_\infty := H_{\infty,{n+1}}$
    \end{definition}
    \begin{remark}
        \begin{enumerate}
            \item Suppose $V$ is a topological vector space. Consider the map $m_\lambda : \Pp(V)\rightarrow \Pp(V), [v]\mapsto [\lambda v]$ for $\lambda\in K\setminus0$. One clearly has that $m_\lambda =\mathrm{id}$, hence it is continuous.  
            \item One notes that $\Pp^n = \bigcup_1^{n+1} U_i$. Note that $\varphi : K^n \rightarrow U_i, v\mapsto [v_1,\dots,v_{i-1},1,v_{i+1},\dots,v_{n}]$ is a bijection. Indeed the map $$\varphi^{-1} : U_i\rightarrow K^n, [v_1,\dots,v_i,\dots,v_{n+1}]\mapsto (v_1/v_i,\dots,v_{i-1}/v_i,v_{i+1}/v_i,\dots,v_{n+1}/v_i)$$
            is well-defined, since 
            \begin{align*}((\lambda v_1)/(\lambda v_i),\dots,(\lambda v_{i-1})/(\lambda v_i),(\lambda v_{i+1})/(\lambda v_i),\dots,(\lambda v_{n+1})/(\lambda v_i)) &=\\ (v_1/v_i,\dots,v_{i-1}/v_i,v_{i+1}/v_i,\dots,v_{n+1}/v_i).\end{align*} 
            Clearly $\varphi$ and $\varphi^{-1}$ are mutual inverses. Suppose $K$ is a topological field. Then $K^m$ becomes a topological vector space and we can endow $\Pp^m$ with the quotient topology. Note that $\varphi$ is continuous, since it is given by pre-composition of $\iota: K^n\rightarrow \pi^{-1}(U_i)\setminus 0, v\mapsto (v_1,\dots,v_{i-1},1,v_{i+1},\dots,v_{n+1})$ with $\pi\mid_{\pi^{-1}(U_i)}: \pi^{-1}(U_i)\rightarrow U_i$, which are continuous maps. Let $U\subset K^n$ be open. Let $Q:=\{ v\in K^{n+1} : v_i\in K, (v_1,\dots,v_{i-1},v_{i+1},\dots,v_{n+1})\in KU\}\simeq_{\pazocal{S}_{n+1}} K\times KU$. Define $O:= Q\setminus 0$. One easily verifies that $O=Q\cap K^{n+1}\setminus 0$ and that $\pi^{-1}(\pi(O))=O$, implying that $\pi(O)$ is open. One checks that $\varphi(U)=U_i\cap \pi(O)$, hence $\varphi(U)$ is open in $U_i$, hence $\varphi^{-1}$ is continuous. We thus conclude that $K^{n}$ is homeomorphic to $U_i$ for each $i$. Hence $\Pp^n$ is locally homeomorphic to $K^n$. 
            \item Consider the map $\pi\mid_{S^n(\R)}: S^n(\R) \rightarrow \Pp^n(\R)$, $S^n(K)=\{v\in \R^{n+1} : \Vert v\Vert =1\}$. Then for $[v]\in \Pp^n(\R)$, $[v]=[1/\Vert v\Vert v]=\pi_{S^n(\R)}(1/\Vert v\Vert v)$, hence $\Pp^n(\R)$ is compact. Since $\C^n \simeq \R^{2n}$, it follows from functoriality that $\Pp^n(\C)\simeq \Pp^{2n+1}(\R)$. Moreover for every $[v]=[w]$ for $v,w\in S^n(\R)$, then $w=\lambda v$, hence  $1=\Vert\lambda v\Vert = \vert \lambda \vert \Vert v\Vert = \vert \lambda \vert$, hence $w = \pm v$. So $\Pp^n(\R)$ is homeomorphic to the northern hemisphere, i.e. $S^n(\R)/(x\sim -x)$.
            \item Another thing to note is that $\Pp^n = U_i\sqcup H_{\infty,i}$
        \end{enumerate}
    \end{remark}
    \subsubsection{The Projective Span}
        \begin{definition}
            For $[v_1],\dots,[v_m]\in\Pp^n$ we define 
            $$\Span([v_1],\dots,[v_m]):=\left\{\left[\sum_1^m \lambda_iv_i\right] : (\lambda_1,\dots,\lambda_m)\in K^m\setminus 0 \right\}$$
        \end{definition}
        \begin{remark}
            Of course one should ask if this is well-defined. Suppose we are given $\alpha_1,\dots,\alpha_m\in K\setminus 0$. Then for any $(\lambda_1,\dots,\lambda_m)\in K^m\setminus 0$,
            $$\sum_1^m \lambda_i(\alpha_i v_i) = \sum_1^m (\lambda_i\alpha_i)v_i\in \left\{\left[\sum_1^m \lambda_iv_i\right] : (\lambda_1,\dots,\lambda_m)\in K^m\setminus 0 \right\} $$
            and conversely 
            $$\sum_1^m \lambda v_i = \sum_1^m (\lambda_i\alpha_i^{-1})\alpha_iv_i \in \left\{\left[\sum_1^m \lambda_i(\alpha_iv_i)\right] : (\lambda_1,\dots,\lambda_m)\in K^m\setminus 0 \right\}.$$
            A further thing to note with this construction, is that if $v_1,\dots,v_m\in \A^{n+1}\setminus 0$ span $\A^{n+1}$ (this can only happen for $m\geq n+1$, then $\Span([v_1],\dots,[v_m])=\Pp^n.$
        \end{remark}
        \begin{lemma}\label{EquivalentDefinitionOfLinearlyIndependent}
            $v_1,\dots,v_m\in \Pp^n$ are linearly independent if and only if $[v_i]\notin \Span([v_1],\dots,\widehat{[v_i]},\dots,[v_m])$ for any $i$.
        \end{lemma}
        \begin{proof}
            "$\implies$": Suppose there is a $[v_i]\in \Span([v_1],\dots,\widehat{[v_i]},\dots,[v_m])$. Then for some $\lambda_1,\dots,\lambda_m\in K^m$ with $\lambda_i \neq 0$,
            $$-\lambda_i v_i = \sum_{j\neq i} \lambda_jv_j \implies \sum_1^m \lambda_jv_j = 0,$$
            hence $v_1,\dots,v_m$ are not linearly independent. 
            "$\impliedby$": Suppose there are $(\lambda_1,\dots,\lambda_m)\in K^m\setminus 0$ such that $\sum_1^m \lambda_i v_i = 0$. Then for some $j\in\{1,\dots,m\}$, $\lambda_j\neq 0$, hence 
            $$v_j = \lambda_j^{-1}\sum_{i\neq j} \lambda_i v_i \in \Span([v_1],\dots,\widehat{[v_j]},\dots, [v_m])$$
        \end{proof}
    \subsubsection{Normed Vector Spaces}
        \begin{definition}
            An \textit{ordered field} is a field $K$ together with a total ordering $\leq$ on $K$, satisfying, for every $a,b,c\in K$
            \begin{enumerate}
                \item $a\leq b \implies a+c \leq b+c$
                \item $0\leq a, 0\leq b\implies 0\leq ab$
            \end{enumerate}
        \end{definition}
        \begin{example}
            In this example we endow $\Q$ with the structure of ordered field. For $\frac{a}{b},\frac{c}{d}\in \Q$ we define $\frac{a}{b} \leq \frac{c}{d}$ if $ad\leq cb$ and $b,d>0$. Note that we can always find a representative of a rational number whose numerator is greater than $0$. It is easy to check that this is a partial order on $\Q$. Given any two $\frac{a}{b},\frac{c}{d}\in\Q$ with $b,d>0$ we get that $bd>0$ and $ad\leq bc$ or $ad\geq bc$, since $(\Z,\leq)$ is totally ordered. It follows that $\leq$ is a total ordering. Suppose $\frac{x}{y},\frac{z}{w},\frac{v}{u}\in \Q$ with $y,w,u>0$ are given. Suppose $\frac{x}{y}\leq \frac{z}{w}$. Then $yu,wu>0$ and
            \begin{align*}
                wu(xu+vy) \leq u(yzu+vwy)=yu(zu+vw) \implies \frac{x}{y}+\frac{v}{u}=\frac{xu+vy}{yu} \leq \frac{zu+vw}{wu}=\frac{z}{w}+\frac{v}{u}.
            \end{align*}
            Suppose instead now that $0\leq \frac{x}{y}$ and $0\leq \frac{z}{w}$.  Then $0\leq x$ and $0\leq z$, hence $0\leq xz$, meaning $0\leq \frac{xz}{yw}$.
        \end{example}
        \begin{definition}
            On an ordered field $K$, we define \textit{the absolute value} to be the function $\vert \cdot \vert : K \rightarrow K_{\geq 0} :=\{a\in K : a\geq 0\}$ to be given by 
            $$\vert a \vert = \begin{cases}
                a & \text{if } a\geq 0\\
                -a & \text{if } a <0
            \end{cases}$$
        \end{definition}
        \begin{definition}
            A \textit{normed vector space} is a vector space $V$ over an ordered field $K$ with a map $\Vert \cdot \Vert : V\rightarrow K$ satisfying 
            \begin{enumerate}
                \item For every $v\in V$, $\Vert v\Vert \geq 0$.
                \item For every $v\in V$, $\Vert v \Vert = 0\iff v = 0$.
                \item For every $v\in V$, $a\in K$, $\Vert av\Vert = \vert a\vert \Vert v\Vert$.
                \item For every $v,w\in V$, $\Vert v+ w\Vert \leq \Vert v\Vert +\Vert w\Vert.$
            \end{enumerate}
            We call such a function \textit{a norm on $V$ over $K$}.
        \end{definition}
        \begin{lemma}
            On an ordered field $K$, the absolute value defines a norm on $K$ over $K$, making $K$ a normed vector space over $K$.
        \end{lemma}
        \begin{proof}
            1. This is trivial, since if $a <0$, then $-a>0$.\\
            2. Suppose $\vert a \vert =0$. Then $a\geq 0$, hence $a=\vert a\vert =0$.\\
            3. Let $a,b\in K$. Then if $a,b\geq 0$ we have that $ab\geq 0$, hence $\vert ab\vert = ab=\vert a \vert \vert b\vert$. If $a,b <0$, then $-a,-b>0$, hence $ab=(-a)(-b)>0$. It follows that $\vert ab\vert=ab =(-a)(-b)=\vert a\vert \vert b\vert$. If $a\geq 0$ and $b<0$, then $ab\leq 0$, hence $\vert ab\vert = -ab=a(-b)=\vert a\vert \vert b\vert$. The case $a<0$ and $b\geq 0$ is symmetric.\\
            4. Let $a,b\in K$. Observe that in any case $-c,c\leq \vert c \vert $ for any $c\in K$. If $a+b\geq 0$, then $\vert a+b\vert = a+b\leq \vert a \vert +\vert b\vert$. In the other case $\vert a+b\vert = -a-b\leq \vert a \vert + \vert b\vert $.
        \end{proof}
\subsection{Ring theory}
\subsubsection{Matrix Rings}
\begin{definition}
    Let $R$ be a ring and $n,m$ be positive integers. We define \textit{the set of $n\times m$ ($n$ by $m$) matrices over $R$} to be the set 
    $$M_{n\times m}(R) = \prod_{i\in \{1,\dots,n\}, j\in \{1,\dots,m\}} R.$$
    For an element $(a_{i,j})\in M_{n\times m}(R)$ we define $(a_{ij}):= (a_{i,j})$, when no disambiguity arises from this notation. An element of $M_{n\times m}(R)$ is called an ($n\times m$) \textit{matrix}. We define $M_n(R):= M_{n\times n}(R)$. 
\end{definition}
\begin{remark}\label{SetOfMatricesIsAnAdditiveGroup}
    By Lemma~\ref{DirectProductRestrictedDirectProductOfGroupsAreGroups} $M_{n,m}(R)$ is an additive group.
\end{remark}
\begin{example}
    Let $R= \Z$, $n = 2$ and $m = 3$ and consider $a_{11} = 1, a_{12} = 2, a_{13}=3$ and $a_{21} = 2, a_{22} = 3, a_{23} = 2$. We opt to write the element $(a_{ij})$ as table with $2$ rows and $3$ columns, i.e.
    $$\begin{pmatrix}
        1 & 2 & 3\\
        2 & 3 & 2
    \end{pmatrix} := (a_{ij}).$$
    For arbitrary rings positive integers $n,m$, we in general can write an element $(a_{ij})\in M_{n\times m}$ as a table with $n$ rows and $m$ columns, i.e.
    $$\begin{pmatrix}
        a_{11} & \cdots & a_{1m}\\
        \vdots & \ddots & \vdots\\
        a_{n1} & \cdots & a_{nm}
    \end{pmatrix} := (a_{ij})$$
\end{example}
\begin{lemma}\label{MatrixMultiplicationIsAssociativeAndDistributesOverAddition}
    Let $R$ be a ring and $n,m,l$ be positive integers. We define \textit{matrix multiplication} to be the operation 
    \begin{gather*}
        \cdot : M_{n\times m}(R) \times M_{m\times l}(R) \rightarrow M_{n\times l}(R)
    \end{gather*}
    defined by 
    $$(a_{ij})(b_{ij}) := \left(\sum_{k=1}^m a_{ik}b_{kj}\right).$$
    Suppose we have an additional positive integer $p$ and let $(a_{ij})\in M_{n\times m}(R),(b_{ij}) \in M_{m\times l}(R), (c_{ij})\in M_{l\times p}(R).$
    Then 
    $$(a_{ij})\left((b_{ij})(c_{ij})\right) =  \left((a_{ij})(b_{ij})\right)(c_{ij}). $$
    Suppose $(a_{ij})\in M_{n\times m}(R),(b_{ij}), (c_{ij})\in M_{m\times l}(R)$. Then 
    $$(a_{ij})\left((b_{ij})+(c_{ij})\right) = (a_{ij})(b_{ij}) + (a_{ij})(c_{ij})$$
\end{lemma}
\begin{proof}
    Indeed for $(a_{ij})\in M_{n\times m}(R),(b_{ij}) \in M_{m\times l}(R), (c_{ij})\in M_{l\times p}(R)$,
    \begin{align*}(a_{ij})\left((b_{ij})(c_{ij})\right) &=  (a_{ij})\left(\sum_{k=1}^l b_{yk}c_{kz}\right) = \left(\sum_{h=1}^m a_{xh}\sum_{k=1}^l b_{hk}c_{kz}\right) = \left(\sum_{h=1}^m \sum_{k=1}^l a_{xh}(b_{hk}c_{kz})\right) \\ &= \left( \sum_{k=1}^l \sum_{h=1}^m (a_{xh}b_{hk})c_{kz}\right) = \left(\sum_{k=1}^l \left(\sum_{h=1}^m a_{xh}b_{hk}\right)c_{kz}\right)  = \\
    &= \left(\sum_{h=1}^m a_{xh}b_{hk}\right) (c_{ij}) = \left((a_{ij})(b_{ij})\right)(c_{ij})  =\left((a_{ij})(b_{ij})\right)(c_{ij}). \end{align*}
    Furthermore, for $(a_{ij})\in M_{n\times m}(R),(b_{ij}), (c_{ij})\in M_{m\times l}(R)$,
    \begin{align*}
        (a_{ij})\left((b_{ij})+(c_{ij})\right) &= (a_{ij})(b_{ij}+c_{ij}) = \left(\sum_{k=1}^m a_{ik}(b_{kj}+c_{kj}) \right) = \left(\sum_{k=1}^m a_{ik}b_{kj}+a_{ik}c_{kj}\right) \\
        &= \left(\sum_{k=1}^m a_{ik}b_{kj}+ \sum_{k=1}^ma_{ik}c_{kj}\right) = \left(\sum_{k=1}^m a_{ik}b_{kj}\right)+ \left(\sum_{k=1}^ma_{ik}c_{kj}\right)
        \\ 
        &=  (a_{ij})(b_{ij}) + (a_{ij})(c_{ij})
    \end{align*}
\end{proof}
\begin{lemma}\label{MatrixRingIsRing}
    Let $R$ be a ring and $n$ a positive integer. Then $(M_n(R),+,\cdot)$, where $\cdot$ is matrix multiplication of matrices in $M_n(R)$ and $M_n(R)$, is a ring called \textit{the ($n\times n$) matrix ring (over $R$)}. 
\end{lemma}
\begin{proof}
    $M_n(R)$ is an additive group by Remark~\ref{SetOfMatricesIsAnAdditiveGroup}. Let $I:=1:=  (\delta_{ij})$ (where $\delta_{ii}=1$ for $i\in \{1,\dots,n\}$ and $\delta_{ij} = 0$ for $i,j\in\{1,\dots,n\}$ with $i \neq j$). Then for $(r_{ij})\in M_n(R)$ 
    $$1(r_{ij}) = \left(\sum_{k=1}^n \delta_{ik}r_{kj}\right) = \left(\delta_{ii}a_{ij} +\sum_{k\in\{1,\dots,n\}\setminus\{i\}}\delta_{ik}a_{kj} \right) = (1a_{ij}+0)=(a_{ij}).$$
    In a dual way one can prove that 
    $$(a_{ij})1= (a_{ij}).$$
    By Lemma~\ref{MatrixMultiplicationIsAssociativeAndDistributesOverAddition} it follows that $(M_n(R),+,\cdot)$ is a ring.
\end{proof}
\begin{example}
    A matrix ring is never a commutative ring: Take $(a_{ij})\in M_n(R)$ where $a_{11}= 1$ and $a_{ij} = 0$ when $a_{ij} =0$ for $i\neq 1$ or $j\neq 1$. Take $(b_{ij})\in M_n(R)$ where $b_{1m} =1$ and $b_{ij}=0$ when $i \neq 1$ or $j \neq m$. Then it is easy to check that $(a_{ij})(b_{ij}) = (b_{ij})$ while $(b_{ij})(a_{ij}) = 0$.
\end{example}
\subsubsection{Fields, Integral Domains \& some Important Ideals}
\begin{definition}
    Let $R$ be a ring. An element $r\in R$ is called a \textit{unit} if there is an element $r'\in R$ such that $rr'=r'r = 1$. We denote the set of units of $R$ by $R^\ast$.
\end{definition}
\begin{remark}
    \begin{enumerate}[I.]
        \item Suppose there are two elements $r_1,r_2$ such that $rr_i = r_ir = 1$. Then 
        $$r(r_1-r_2) = rr_1-rr_2=1-1=0 \implies r_1-r_2=r_1r(r_1-r_2)=0 \implies r_1 =r_2.$$
        Hence an element satisfying $rr'=r'r=1$ is unique. We denote it by $r^{-1}$ and refer to it as the multiplicative inverse of $r$. 
        \item $(R^\ast, \cdot)$ is a group. We check that $R^\ast$ is a submonoid of $R$ and hence a monoid. Clearly $1 \in R^\ast$. Let $r,r' \in R^\ast$. Then $$\left(rr'\right)\left(r'^{-1}r^{-1}\right)=r\left(r'\left(r'^{-1}r^{-1}\right)\right)=r\left(\left(r'r'^{-1}\right)r^{-1}\right)=r\left(er^{-1}\right) = rr^{-1} = 1,$$
        and similarly one can check that $\left(r'^{-1}r^{-1}\right)(rr') = 1$, hence $rr'\in R^\ast$ a. Let $r\in R^\ast$. Then $r^{-1}r=rr^{-1}=1$, hence $r^{-1}$ is a unit, hence $R^\ast$ is a group.
        \item If $R$ is commutative it is sufficient to check that $rr'=1$ to verify that $r$ is a unit.
    \end{enumerate}   
\end{remark}
\begin{definition}
    A commutative ring $K$ where $K^\ast = K\setminus \{0\}$ is called a \textit{field}. Removing the restriction of $K$ being commutative, $K$ is called a \textit{division ring} or \textit{skew field}.
\end{definition}
\begin{definition}
    An left/right ideal $M\subsetneq R$ is called a \textit{left/right maximal} ideal, if for every left/right ideal $I\subset R$ with $M\subset I$, either $I=M$ or $I=R$. If $M$ is an ideal with aforementioned property it is called a \textit{maximal ideal}.  
\end{definition}
\begin{definition}
    A ring $R$ is called \textit{simple} if the only ideals in $R$ are the trivial ones, i.e. $0$ and $R$.
\end{definition}
\begin{lemma}\label{DivisionRingsAreSimple}
    Any division ring is simple.
\end{lemma}
\begin{proof}
    Let $D$ be a division ring and consider a non-zero ideal $I\subset R$. Then there is an $x\in I\setminus 0$. Since $D$ is a division ring, there exists $x^{-1}$ s.t. $1 = x^{-1}x\in I$, meaning $I = D$.
\end{proof}
\begin{lemma}\label{MaximalIdealIffQuotientRingSimple}
    Let $R$ be a ring. Let $I\subset R$ be an ideal. Then $I$ is a maximal ideal if and only if $R/I$ is a simple.
\end{lemma}
\begin{proof}
    "$\implies$": Let $J/I\subset R/I$ be a non-zero ideal, i.e. assume $I\subsetneq J\subset R$ for some ideal $J$ in $R$. Then $J = R$, hence $J/I= R/I$, implying $R/I$ is simple.\\
    "$\impliedby$": Conversely, consider and ideal $J\subset R$ such that $I\subsetneq J$. Then $0\neq J/I\subset R/I$, implying that $J/I = R/I$ and hence that $J=R$. 
\end{proof}
\begin{proposition}\label{InCommutativeRingMaximalIdealIffQuotientRingField}
    Let $R$ be a commutative ring. Let $I\subset R$ be an ideal. Then $I$ is a maximal ideal if and only if $R/I$ is a field.
\end{proposition}
\begin{proof}
    "$\implies$": We need to prove that every non-zero element of $R/I$ is a unit. Consider $a + I \in R/I\setminus \{0+I\}$, i.e. an element in $R/I$, where $a \notin I$. Since $I$ is maximal we have that $I+Ra = R = R1$. This implies that we can find $b\in I$ and $r \in R$ such that $1 = b + ra$, hence
    $$(r+I)(a+I) = (ra + I) + (b+I) = (ra+b)+I = 1 + I,$$
    and since $R/I$ is commutative, this implies $a+I$ is a unit.\\
    "$\impliedby$": Since $R/I$ is a field, it is in particular a division ring. Then by Lemma~\ref{DivisionRingsAreSimple} $R/I$ is simple. By Lemma~\ref{InCommutativeRingMaximalIdealIffQuotientRingField} $I$ is maximal.  
\end{proof}
\begin{definition}
    Let $R$ be a ring, $r\in R$. $d\in R$ is called a \textit{left divisor of $r$} if $r\in Rd$ and a \textit{right divisor of $r$} if $r\in dR$. If $d$ is both a left and a right divisor of $r$, we write $d\mid r$. Hence if $R$ is commutative, $d\mid r \iff r\in \langle d\rangle$.
\end{definition}
\begin{definition}
    Let $R$ be a ring. An element $a\in R$ is called a \textit{left/right zero divisor} if there is an element $r\in R\setminus 0$ such that $ar = 0$ respectively $ra = 0$. In a commutative ring an element is a left zero divisor if and only if it is a right zero divisor, hence we just call a left/right zero divisor in a commutative ring a \textit{zero divisor}.  
\end{definition}
\begin{definition}
    Let $R$ be a ring. If the only left/right zero divisor of $R$ is $0$, then $R$ is called a \textit{left/right domain}. If $R$ is a commutative ring and a domain it is called an \textit{integral domain}. 
\end{definition}
\begin{lemma}\label{SubringOfIDIsID}
    Suppose $S\supset R$ is a ring extension where $S$ is a left/right domain. Then so is $R$.
\end{lemma}
\begin{proof}
    Let $a\in R\setminus 0$, hence in particular in $S\setminus 0$ then for every $R\setminus 0$, $ar\neq0$. 
\end{proof}
\begin{proposition}
    A division ring $D$ is a domain. Hence a field is an integral domain.
\end{proposition}
\begin{proof}
    Let $a\in D\setminus 0$. Then $\langle a \rangle = D$ since $D$ is simple, hence $1\in \langle a\rangle$, meaning there is some $b\in a$ such that $ba = ab = 1$. 
\end{proof}
\begin{lemma}\label{WeCanDivideByElementsWhichAreNotZeroDivisors}
    Let $x,y, a\in R$ where $a$ is not a zero divisor. If $ax = ay$, then $x=y$. The same result can be proven if $xa = ya$. In particular, if $R$ is a domain, then for $x,y\in R$, $a\in R\setminus 0$, $ax=ay$ xor $xa=ya$ implies $x=y$. 
\end{lemma}
\begin{proof}
    We have that $a(x-y)=ax-ay = 0$ implies $x-y=0$. The other result is dual. 
\end{proof}
\begin{definition}
    Let $R$ be a commutative ring. An ideal $I\subsetneq R$ is said to be \textit{prime} if for any $a,b \in R$ such that $ab\in I$, then $a \in I$ or $b\in I$   
\end{definition}
\begin{lemma}\label{PrimeIdealIffQuotientRingID}
    Let $R$ be a commutative ring and $I\subset R$ and ideal. Then $I$ is prime if and only if $R/I$ is an integral domain
\end{lemma}
\begin{proof}
    "$\implies$": Let $a+I,b+I\in R/I$ be given such that $ab + I = 0 +I$. Then $ab\in I$, hence by assumption $a \in I$ or $b\in I$, meaning $a + I = 0$ or $b+ I =0$.\\
    "$\limplies$": Let $a,b\in R$ such that $ab \in I$. Then $ab + I = 0 + I$, hence by assumption $a+I = 0$ or $b + I = 0$, hence $a\in I$ or $b\in I$. 
\end{proof}
\begin{corollary}
    A maximal ideal $M$ in commutative ring $R$ is prime.
\end{corollary}
\begin{proof}
    Indeed $R/M$ is a field and a field is an integral domain, hence $M$ is prime.
\end{proof}
\begin{proposition}\label{EliminationIdealOfPrimeIdealIsPrime}
    Let $S\supset R$ be a commutative ring extension. Let $I\subset S$ be prime. Then $I\cap R\subset R$ is prime.  
\end{proposition}
\begin{proof}
    By Proposition~\ref{PrimeIdealIffQuotientRingID} $S/I$ is an integral domain. Note that $S/I\supset (R+I)/I$ (cf. {\LARGE res saying $R+I$ is subring}). Hence from Lemma~\ref{SubringOfIDIsID} $(R+I)/I$ is an integral. It follows from {\LARGE Isomorphism theorem not yet written} that $(R+I)/I\simeq R/(I\cap R)$, hence $I\cap R$ is prime.
\end{proof}
\begin{definition}
    Let $R$ be a commutative ring. A non-unit and non-zero element $p \in R$ is called a \textit{prime element} if for every $a,b\in R$, $p\mid ab$ implies $p\mid a$ or $p\mid b$. 
\end{definition}
\begin{lemma}\label{PrimeElementsGeneratePrimeIdeals}
    Let $R$ be a commutative ring. Then for a non-zero $p\in R\setminus R^\ast$, $p$ is prime if and only if $\langle p \rangle$ is prime.
\end{lemma}
\begin{proof}
    This is seen by the fact that $x \in \langle p\rangle $ is by definition equivalent to $p\mid x$. 
\end{proof}
\begin{definition}
    Consider a commutative ring $R$ and an ideal $I \subset R$. We define the \textit{radical} of $I$ to be the set
    $$\rad(I)= \left\{ r\in R : r^n\in I \text{ for some } n>0 \right\}.$$
    An ideal with the property that $I = \rad(I)$ is called a \textit{radical ideal}. 
\end{definition}
\begin{remark}\label{IdealContainedInItsRadical}
    Trivially we have that if $a\in I$, then $a=a^1\in I$, hence $a\in \rad(I)$. In other words, $I\subset \rad(I)$.
\end{remark}
\begin{lemma}
    The radical of an ideal $I\subset R$ is an ideal in $R$.
\end{lemma}
\begin{proof}
    Let $a,b \in \rad(I)$ and $r\in R$. Clearly $0^1=0\in I$, hence $0\in \rad(I)$. For some $n,m>0$, $a^n,b^m\in I$. Thus, we also have that 
    \begin{align}\label{UsageOfBinomialFormula}
        (a+b)^{n+m} = \sum_{0}^{n+m} {n+m\choose k} a^{n+m-k}b^k,
    \end{align}
    For $k\in \{0,\dots,m\}$, $n+m-k\geq n$, implying $a^{n+m-k}\in I$. For $k\in \{m+1,\dots,n\}$, $b^k \in I$. Then using (\ref{UsageOfBinomialFormula}), it follows that $(a+b)^{n+m}\in I$ and hence that $a+b \in \rad(I)$. Finally we also have that 
    $$(ra)^n = r^na^n\in I \implies ra\in \rad(I).$$
\end{proof}
\begin{lemma}\label{PrimeIdealIsRadical}
    Let $I\subsetneq R$ be a prime ideal. Then $I$ is a radical ideal.
\end{lemma}
\begin{proof}
    Let $a\in I$ and $n>0$. We prove by induction in $n$ that if $a^n \in I$ then $a \in I$. For $n=1$, $a = a^1\in I$. Suppose $a^{n+1}\in I$. Then $aa^n\in I$. Using that $I$ is prime we get that $a \in I$ or $a^n \in I$. If we land in the first case, we are done. In the second case it follows by induction that $a \in I$.   
    From the above it follows that if $a \in \rad(I)$, then $a \in I$. Hence it follows from Remark~\ref{IdealContainedInItsRadical} that $I = \rad(I)$.
\end{proof}
The following definition will be important way later on. 
\begin{definition}
    Let $R$ be a commutative ring. We define the \textit{spectrum of $R$} to be the set 
    $$\text{Spec } R := \{ I \subset R : I \text{ is a prime ideal}\}$$
\end{definition}
\begin{proposition}\label{RadicalPrimeMaximalIdealsMatchThoseOfQuotientRings}
    Let $R$ be a commutative ring and $I\subset R$ an ideal. Then there is a one-to-one correspondence between radical/prime/maximal ideals in $R$ containing $I$ and radical/prime/maximal ideals in $R/I$  
\end{proposition}
\begin{proof}
    \textbf{Radical:} Let $J\subset R$ be a radical ideal containing $I$. Let $x+ I\in \rad\left(J/I\right)$, then for some $n\geq 1$, $x^n + I \in J/I$, hence $x^n \in J$, implying $x\in \rad(J) = J$. This means $x+I \in J/I$.\\
    Let $K \subset R/I$ be a radical ideal. Then $K=J/I$ for some ideal $J\subset R$ containing $I$. Let $x\in \rad(J)$. Then for some $n\geq 1$, $x^n+I\in J/I$, hence $x+I \in \rad\left(J/I\right) = J/I$, implying $x\in J$.\\  
    \textbf{Prime:}
        $J/I$ is prime if and only if $R/J\simeq \frac{R/I}{J/I}$ is an integral domain which is equivalent to $J$ being prime.\\    
    \textbf{Maximal:}
        $J/I$ is maximal if an only if $R/J\simeq \frac{R/I}{J/I}$ is maximal which is equivalent to $J$ being maximal.
\end{proof}
\begin{lemma}\label{RadicalLemma}
    Let $I,J$ be ideals in a commutative ring $R$. Suppose $I=\langle a_1\dots,a_m\rangle $ for suitable $a_1,\dots,a_m\in I$ and $I\subset \rad(J)$. Then $I^n \subset J$ for some $n\geq 0$.
\end{lemma}
\begin{proof}
     Let $n_i\geq 0$ be given such that $a_i^{n_i}\in J$. Let $n=\sum_1^m n_i$. We prove the statement by induction in $m$. For $m=1$ the statement is trivial. 
     $$\lambda_{i,j}\in R\quad (i\in\{1,\dots, m\}, j\in\{1,\dots,2n\})$$
     Then 
     $$\prod_1^{n} \left(\sum_1^{m} \lambda_{i,j}a_i\right) = \sum_{v\in \N^m} \mu_v a_1^{v_1}\cdots a_m^{v_m}.$$
     A simple induction argument shows that if $v_i< n_i$ for some $i$ then $v_j>n_j$ for some $j$, hence $a_i^{v_1}\cdots a_m^{v_m}\in J$ for each $v\in \N^m$ with $\sum_1^m v_i = n$. It follows that $\prod_1^{n} \left(\sum_1^{m} \lambda_{i,j}a_i\right)\in J$, hence $I^n\subset J$.
\end{proof}
\subsubsection{Comaximal ideals}
In this subsection every ring will be assumed commutative. 
\begin{definition}
    Let $R$ be a ring. A pair of ideals $I,J$ in $R$ are said to be \textit{comaximal} if $I+J = R$.
\end{definition}
\begin{lemma}\label{IntersectionAndProductOfComaximalIdeals}
    Let $I,J$ be comaximal ideals in a ring $R$. Then 
    $$IJ = I \cap J$$
\end{lemma}
\begin{proof}
    The first inclusion is implied by Lemma~\ref{IntersectionAndProductOfIdeals}. Let $a\in I\cap J$. Since $I$ and $J$ are comaximal we can write $1 = i+j$ for suitable $i\in I$ and $j\in J$. Then
    $$a=a(i+j) = ai+aj=ia+aj\in IJ,$$
    since $ia,aj\in IJ$.
\end{proof}
\begin{lemma}
    Let $I,J$ be comaximal ideals in a ring $R$. Then $I^n$ and $J^m$ are comaximal for every $n,m\geq 1$.
\end{lemma}
\begin{proof}
    \textbf{Claim 1:} We first show that $I,J^m$ are comaximal for every $m\geq 1$ by way of induction in $m$. The base case is true by assumption. Let $m\geq 1$ and $x\in R$. By induction $R= I+J^m$, hence $x=a+b$ for suitable $a\in I$ and $b\in J^m$. Moreover, $1=i+j$ for suitable $i\in I$ and $j\in J$. Then 
    $$x=a+b= a+b(i+j)=(a+bi)+bj\in I+J^{m+1}.$$
    We now fix $m\geq 1$ it follows by a similar induction argument in $n$ that $I^n$ and $J^m$ are comaximal. 
\end{proof}
\begin{lemma}\label{ForComaximalIdealsFiniteIntersectionCommutesWithExponentiation}
    Let $R$ be a ring. Consider ideals $I_1,\dots,I_N$ in $R$ and set $J_i := \bigcap_{j\neq i} I_j$. Suppose $I_i$ and $J_i$ are comaximal for each $i$. Then $$\bigcap_1^N I_i^n=\left(\prod_1^NI_i\right)^n= \left(\bigcap_1^N I_i\right)^n $$
\end{lemma}
\begin{proof}
    Let $n\geq 1$. Note for each $N\geq 1$, $I_1J_1=I_1\cap J_1$, by lemma~\ref{IntersectionAndProductOfComaximalIdeals}, hence by induction we have that $\prod_1^N I_i = \bigcap_1^N I_i$, hence for each $n\geq 1$, $\left(\prod_1^NI_i\right)^n= \left(\bigcap_1^N I_i\right)^n$. By assumption and induction $I_1^{n}$ and $\bigcap_2^{N+1} I_i^n = \left(\bigcap_2^{N+1} I_i\right)^n$ are comaximal, hence 
    $$\bigcap_1^{N+1} I_i = I_1^n\cap \bigcap_2^{N+1} I_i^n= \prod_1^{N+1} I_i^n=\left(\prod_1^{N+1} I_i\right)^n.$$
\end{proof}
\subsubsection{Greatest Common Divisor and Least Common Multiples}
\begin{definition}
    A \textit{greatest common divisor} of two elements $a,b$ in a commutative ring is an element $d$ where for every $c\in R$ such that $c\mid a$ and $c\mid b$ we have that $c\mid d$. 
\end{definition}
\begin{remark}
    Note that $\gcd(a,0)=a$ since if $a\mid a$ and any element divides $0$.
\end{remark}
\begin{definition}
    A \textit{Least common multiple} of two elements $a,b$ in a commutative ring is an element $m\in R$ where for every $c\in R$ such that $a\mid c$ and $b\mid c$ we have that $m\mid c$.
\end{definition}
\begin{remark}
    $\lcm(a,0)=$ since $0$ is the only element that is a multiple of $0$.
\end{remark}
\subsubsection{Unique Factorization Domains and Euclidean Domains}
In our exploration of unique factorization domains and Euclidean Domains we will mean fix an integral domain $R$.
\begin{definition}
    Let a non-unit and non-zero element $a\in R$ be given. $a$ is said to be an \textit{irreducible element} if for every $b,c\in R$
    $$a = bc \implies b\in R^\ast \text{ or } c \in R^\ast.$$
\end{definition}
\begin{lemma}
    A prime element is irreducible.
\end{lemma}
\begin{proof}
    Let $p\in R$ be prime. Consider $a,b\in R$ such that $p=ab$, then $p\mid ab$, hence either $p\mid a$ or $p\mid b$. Suppose $p\mid a$. Then $a = pr$ for some $r\in R$. This means $p = prb$, which by Lemma~\ref{WeCanDivideByElementsWhichAreNotZeroDivisors} means $rb = 1$, hence that $b$ is a unit. In the case $p\mid b$, we can similarly show that $a$ is a unit. It thus follows that $p$ is irreducible. 
\end{proof}
\begin{definition}
    $R$ is called a \textit{Unique factorization domain (UFD)} if for every $r\in R\setminus\{ R^\ast\cup \{0\}\}$ has unique factorization into irreducible elements, i.e. there are distinct irreducible elements $p_1,\dots,p_n\in R$ unique and $v_1,\dots,v_n\geq 1$ such that 
    $$r = \prod_1^n p_i^{v_i}.$$
\end{definition}
\begin{remark}
    By uniqueness we more precisely mean that given $q_1,\dots, q_m\in R$ another sequence of distinct irreducible elements and $w_1,\dots,w_m\geq 1$ such that 
    $$r = \prod_1^m q_i^{w_i},$$
    then $m=n$ and there is some bijection $\omega : \{1,\dots, n\} \rightarrow \{1,\dots,n\}$ (i.e. a permutation $\omega \in\mathcal{S}_n$) and units $a_1,\dots,a_n\in R$ such that 
    $$p_i = a_i q_{\tau(i)} \text{ and } v_i = w_{\tau(i)},$$
    for each $i\in\{1,\dots,n\}.$
\end{remark}
\begin{proposition}\label{EquivalentFormulationOfUFD}
    Let $R$ be a ring in which every element that is not zero or a unit can be written as a product of irreducible elements. $R$ is a UFD if and only if every irreducible element is a prime.
\end{proposition}
\begin{proof}
    "$\implies$": Let $p\in R$ be irreducible and suppose there are $a,b\in R$ such that $p\mid ab$. We aim to prove that $p\mid a$ or $p\mid b$. Since $p\mid 0$, we are done if $a = 0$ or $b=0$. So assume $a,b\neq 0$. In general for $x,y,z\in R$ if $x\mid y$, then $x\mid yz$. Hence if $a$ is a unit, then $p\mid b = a^{-1}(ab)$, and similary if $b$ is a unit then $p\mid a = b^{-1}(ab)$. So assume that $a$ and $b$ are not units. For some $q\in R$, $pq=ab$. $q$ is not a unit, for otherwiser $p = abq^{-1}$ contradicting the irreduciblity of $p$. We can then find irreducible $q_1,\dots,q_n,p_1,\dots,p_m,p_{m+1},\dots,p_l\in R$ and $v_1,\dots,v_m,w_1,\dots,w_m,w_{m+1},\dots,v_l\geq 1$ such that 
    $$q = \prod_1^n q_i^{v_i}\text{ and } a = \prod_1^m p_i^{w_i} \text{ and } b = \prod_{m+1}^{l} p_i^{w_i}.$$
    From this it follows that 
    $$p\prod_1^n q_i^{v_i} = \prod_1^{l}p_i^{w_i}.$$
    Since the above is a factorization into irreducible it follows from the assumption that $R$ is a UFD that there exists an $i\in\{1,\dots,l\}$ and a unit $s\in R$ such that $w_i = 1$ and  $p = sp_i$. If $i\in\{1,\dots,m\}$ then 
    $$a = \prod_1^m p_j^{w_j} = s^{-1}p\prod_{j\in\{1,\dots,m\}\setminus\{i\}} p_j^{w_j} \implies p \mid a.$$
    By a similar argument, if $i\in\{m+1,\dots,l\}$, then $p\mid b$.
    "$\impliedby$": Suppose there are irreducible elements $p_1,\dots,p_n,q_1,\dots,q_m\in R$ and positive integers $v_1,\dots,v_n,w_1,\dots,w_m\geq 1$ such that 
    $$\prod_1^n p_k^{v_k} =\prod_1^m q_k^{w_k}.$$
    Let $i\in\{1,\dots,n\}$. Then $p_i\mid \prod_1^m q_k^{w_k}$. By assumption $p_i$ is prime, hence $p_i \mid q_{\tau(i)}$ for some $\tau(i)\in\{1,\dots,m\}$, hence for some $s_{i}\in R$, $q_{\tau(i)}=s_ip_i$, by irreducibility, we get that $s_i$ is a unit. Similarly for $j\in\{1,\dots,m\}$, we can find $\omega(j)\in\{1,\dots,n\}$ and $t_{j}\in R^\ast$ such that $p_{\omega(j)} = t_jq_j$. Thus $p_i = s_iq_{\tau(i)} = s_it_jp_{\omega(\tau(i))}$, hence $\omega(\tau(i))=i$. Conversely one can show that $\tau(\omega(j))=j$, hence $n=m$ and $\tau$ is a bijection. Now we show that $v_i = w_{\tau(i)}$ for each $i$. WLOG $v_i\geq w_{\tau(i)}$, Then 
    $$p_i^{v_i-w_{\tau(i)}}p^{w_{\tau(i)}}\prod_{k\in\{1,\dots,n\}\setminus\{i\}} p_k^{v_k} =p_i^{v_i}\prod_{k\in\{1,\dots,n\}\setminus\{i\}} p_k^{v_k} = p_i^{w_{\tau(i)}}\prod_{k\in\{1,\dots,n\}\setminus\{\tau(i)\}} p_k^{w_k},$$
    which implies that 
    $$p_i^{v_i-w_{\tau(i)}}\prod_{k\in\{1,\dots,n\}\setminus\{i\}} p_k^{v_k} = \prod_{k\in\{1,\dots,n\}\setminus\{\tau(i)\}} p_k^{w_k}\implies p^{v_i-w_{\tau(i)}}\mid \prod_{k\in\{1,\dots,n\}\setminus\{\tau(i)\}} p_k^{w_k},$$
    and if $v_i-w_{\tau(i)}\neq 0 $ then since $p_i$ is prime $p_i\mid p_k$ for some $k\in\{1,\dots,n\}\setminus\{\tau(i)\}$ which is not possible since $p_i$ and $p_k$ are distinct. So we conclude that $v_i = w_{\tau(i)}$.
 \end{proof}
 \begin{definition}
     Let $R$ be a UFD $\mathcal{P}$ be the set of prime/irreducible elements in $R$. We say two element $p,q\in \mathcal{P}$ are \textit{associated} if there is a unit $a\in R$ such that $p = aq$.  
 \end{definition}
 \begin{remark} 
    Write $p \sim q$ if $p$ and $q$ are associated. Being   associated is an equivalence relation on $\mathcal{P}$. Indeed for $p,q,r\in \mathcal{P}$. If $p = 1p$ implying $p\sim p$. $p\sim q$, then for some $a\in R^\ast$, $p = aq$, hence $q = a^{-1}p$, hence $q\sim p$. Suppose $p\sim q$, $q\sim p$, then for $a,b\in R^\ast$, $p = aq$ and $q= br$. Then $p = (ab)r$, hence $p\sim r$. We may then write any element as a product over $\mathcal{P}$
    $$\prod_{[p]_\sim \in \mathcal{P}/\sim} p^{v_p},$$
    where $v_p$ is equal to $0$ for all but finitely many $p$.
 \end{remark}
 \begin{lemma}
     Let $R$ be a UFD. Then $\prod_{[p]_\sim \in \mathcal{P}/\sim} p^{v_p}\mid \prod_{[p]_\sim \in \mathcal{P}/\sim} p^{w_p}$ if and only if $v_p \leq w_p$ for every $p\in\mathcal{P}$.
 \end{lemma}
 \begin{proof}
     We that 
     $$\prod_{[p]_\sim \in \mathcal{P}/\sim} p^{w_p}=\left(\prod_{[p]_\sim \in \mathcal{P}/\sim} p^{v_p}\right)\left(\prod_{[p]_\sim \in \mathcal{P}/\sim} p^{u_p}\right) = \prod_{[p]_\sim \in \mathcal{P}/\sim} p^{v_p+u_p} $$
     for suitable $u_p\geq 0$. By uniqueness $v_p +u_p = w_p$ implying that $v_p\leq w_p$ for every $p\in\mathcal{P}$. Conversely if $v_p\leq w_p$ then there is some $u_p\geq 0$ such that $v_p + u_p = w_p$ and hence 
     $$\left(\prod_{[p]_\sim \in \mathcal{P}/\sim} p^{v_p}\right)\left(\prod_{[p]_\sim \in \mathcal{P}/\sim} p^{u_p}\right)=\left(\prod_{[p]_\sim \in \mathcal{P}/\sim} p^{v_p+u_p}\right) = \left(\prod_{[p]_\sim \in \mathcal{P}/\sim} p^{w_p}\right)$$
 \end{proof}
\begin{lemma}
    Let $a,b\in R\setminus 0$. Then $a = \prod_{[p]_\sim \in \mathcal{P}/\sim} p^{v_p(a)}$ and $b = \prod_{[p]_\sim \in \mathcal{P}/\sim} p^{v_p(b)}$. One finds that 
    $$\gcd(a,b) = \prod_{[p]_\sim \in \mathcal{P}/\sim} p^{\min\left(v_p(a),v_p(b)\right)} \text{ and } \lcm(a,b) = \prod_{[p]_\sim \in \mathcal{P}/\sim} p^{\max\left(v_p(a),v_p(b)\right)},$$
    and these are unique up to multiplication by units.
\end{lemma}
\begin{proof}
    Let $c\in R$ such that $c \mid a$ and $c\mid b$, then 
    $$c=\prod_{[p]_\sim \in \mathcal{P}/\sim} p^{v_p(c)} $$
    with $v_p(c)\leq v_p(a)$ and $v_p(c)\leq v_p(b)$ by the above lemma, hence $v_p(c)\leq \min(v_p(a),v_p(b))$. Suppose $d\in R$ is a greatest common divisor of $a$ and $b$, Then $d\mid \gcd(a,b)$ and $\gcd\mid d$, hence $v_p(d)=\max(v_p(a),v_p(b))$. Let $c\in R$ such that $a \mid c$ and $b \mid c$. Then $v_p(a)\leq v_p(c)$ and $v_p(b)\leq v_p(c)$ by the above lemma, hence $\max(v_p(a),v_p(b))\leq v_p(c)$. Showing that $\lcm(a,b)$ is that unique up to multiplication by a unit is similar to the $\gcd$-case.
\end{proof}
\begin{lemma}\label{RadicalOfPrincipalIdealInUFD}
    Let $R$ be an integral $r = \prod_1^m p_i^{v_i} \in R$ where $p_1,\dots,p_m\in R$ are distinct primes and $v_1,\dots,v_m\geq 1$. Then $\rad(\langle r\rangle) = \left\langle \prod_1^m p_i \right\rangle.$
\end{lemma}
\begin{proof}
    Clearly $\prod_1^m p_i\in \rad(\langle r\rangle)$, since for $n = \max \ 
 v_i$, $r=\prod_1^m p_i^{v_i}\mid \prod_1^m p_i^n$. Conversely, if $a\in \rad(\langle r\rangle)$, then for some $n\geq 0$, $r\mid a^n$, implying $\prod_1^mp_i\mid a^n$, hence $\prod_1^m p_i\mid a$.
\end{proof}
\begin{lemma}\label{PrimeIdealsInAUFD}
    Let $R$ be a UFD. The prime ideals of $R$ are $R$, $0$ and principal ideals generated by $\langle p\rangle$ for an irreducible element $p\in R$. This means that a proper non-zero prime ideal in a UFD contains no non-trivial prime ideal. 
\end{lemma}
\begin{proof}
    This follows from Lemma~\ref{PrimeElementsGeneratePrimeIdeals} and Lemma~\ref{EquivalentFormulationOfUFD}. A non-trivial prime ideal contained in $\langle p \rangle$ is on the form $\langle q\rangle$. Then $q = ap$ for some $a\in K$. Since $q$ is irreducible $a$ is a unit hence $\langle p \rangle = \langle q\rangle$. 
\end{proof}
\subsubsection{Principal Ideal Domains}
    \begin{definition}
        An ideal $I\subset R$ is \textit{principal}. A domain in which every ideal is principal is called a \textit{principal ideal domain} or a \textit{PID}. 
    \end{definition}
    \begin{lemma}\label{InPIDPrimeIdealsAreMaximal}
        Let $R$ be a PID. The non-trivial maximal ideals of $R$ are those generated by primes.
    \end{lemma}
    \begin{proof}
        A maximal ideal is generated by some $p$ that is non-zero and a non-unit. Since a maximal ideal is prime we have that $p$ is prime.\\
        Let a prime $p$ be given. Suppose $\langle p\rangle \subset \langle x\rangle$. Then $p=qx$. Then $q\in R^\ast$ or $x\in R^\ast$, since $p$ is in particular irreducible. Then $\langle p\rangle = \langle q\rangle$ or $\langle p\rangle = R$, hence $\langle p\rangle$ is maximal. 
    \end{proof}
    \begin{lemma}\label{InPIDIrreduciblesArePrimes}
        Let $R$ be a PID. Irreducible elements in $R$ are prime. 
    \end{lemma}
    \begin{proof}
         $p$ be irreducible. Suppose $\langle p\rangle \subset \langle p'\rangle$. Then $p = qp'$ for some $q$, then $q\in R^\ast$ or $p'\in R^\ast$. In the first case $\langle p \rangle = \langle p'\rangle$ and in the second case $\langle p'\rangle = R$. 
        Then $\langle p \rangle$ is maximal, hence $p$ is prime. 
    \end{proof}
    \begin{lemma}
        A PID is a UFD.
    \end{lemma}
    \begin{proof}
        Let $R$ be a PID. If we can prove that any non-unit non-zero element in $R$ decomposes into a product of irreducible elements, we are done by  Proposition~\ref{EquivalentFormulationOfUFD}, having the prior lemma in mind. Let $a\in R\setminus 0$ be a non-unit. Since $R$ is Noetherian we can find a maximal ideal $\langle p_1\rangle \supset \langle a\rangle$. Note that then $\langle p_1\rangle$ is prime, hence $p_1$ is prime and that $a=a_1p_1$ for some $a_1$. Define $a_{n+1}$ to be an element such that $a_n=a_{n+1}p_n$ for some $p_n$. We get an ascending chain 
        $$\langle a\rangle \subset \langle a_1\rangle \subset \dots$$
        Since $R$ is Noetherian, for some $m$, $\langle a_m\rangle=\langle a_n\rangle $ for $n\geq m$. Pick $m$ to be the smallest such. If $a_m$ was reducible, then $\langle a_m\rangle \subsetneq \langle a_{m+1}\rangle$. So it follows by induction that $a = a_m\prod_1^m p_i$; a product of irreducible elements.\\
    \end{proof}
    The following is an immediate result of the prior two lemmas
    \begin{corollary}\label{MaximalIdealsAreThoseGeneratedByIrreducibleElements}
        The non-zero maximal ideals of a PID are those generated by irreducible elements 
    \end{corollary}
\subsubsection{Local Rings, Localizations \& Field of Fractions}
\begin{definition}
    A ring $R$ is called \textit{local} if it has unique maximal left ideal. 
\end{definition}
\begin{proposition}\label{EquivalentFormulationOfRingBeingLocal}
    Let $R$ be ring. $R$ is local if and only if $\mathfrak{m}:=R\setminus R^\ast$ is a left ideal.
\end{proposition}
\begin{proof}
    "$\implies$": Let $I$ be the unique maximal left ideal of $R$. Then since $I$ is proper, $I\subset \mathfrak{m}$. Note that for every $x\in \mathfrak{m}$, $\langle x\rangle$ is a proper ideal in $R$, hence $x\in I$. Therefor $\mathfrak{m} = I$, hence $\mathfrak{m}$ is an ideal.\\
    "$\impliedby$": Let $I\subsetneq R$ be an ideal. Then every element of $I$ is a non-unit, hence $I\subset \mathfrak{m}$, thus $\mathfrak{m}$ is the unique maximal left ideal in $R$.
\end{proof}
\begin{definition}
    Let $R$ be a commutative ring and $X\subset R$ a subset that is a submonoid of $(R,\cdot)$. For $(r,x),(r',x')\in R\times X$ we define a relation that $(r,x)\sim (r',x')$ if $rx' = r'x$. We define $X^{-1}R := R/\sim$ and denote an $(r,x)\in X^{-1}R$ by $\frac{r}{x}$. Hence 
    $$X^{-1}R = \left\{\frac{r}{x} : r \in R, x\in X \right\}.$$
    This is called the \textit{localization of $R$ with respect to $X$}. For an $x\in X$, if 
    $$X:= \left\{x^n : n\geq 0\right\},$$
    we define $R_x := X^{-1}R$. When $X = R\setminus \{0\}$, we define $Q(R) := X^{-1}R$. In this case $Q(R)$ is called the \textit{field of fractions} of $R$.
\end{definition}
\begin{remark}
    We give some properties of this construction. Note that every $x,y \in X$
    $$0y = 0x \implies \frac{0}{x} = \frac{0}{y}$$
    and that for every $r,r'\in R$
    $$r = r' \implies \frac{r}{1} = \frac{r'}{1},$$
    thus we may regard $R$ as a subset of $X^{-1}R$ via the map $r \mapsto \frac{r}{1}$. We also have that  
    $$xy = yx \implies \frac{x}{x} = \frac{y}{y}.$$
    Furthermore,
    $$(rx)x= rx^2  \implies \frac{rx}{x^2} = \frac{r}{x}$$
\end{remark}
\begin{lemma}
    Let $R$ be a commutative ring, $X\subset R$ a submonoid of $R$
\end{lemma}
\begin{lemma}
    Let $R$ be an integral domain and $X\subset R\setminus \{0\}$ a subset that is a submonoid of $(R,\cdot)$. For $\frac{r_1}{x_1},\frac{r_2}{x_2} \in X^{-1}R$ we define 
    $$\frac{r_1}{x_1}+\frac{r_2}{x_2} := \frac{r_1x_2+r_2x_1}{x_1x_2}$$
    and 
    $$\frac{r_1}{x_1}\frac{r_2}{x_2} := \frac{r_1r_2}{x_1x_2}.$$
    This makes $(X^{-1}R,+,\cdot)$ a commutative ring containing $R$ as a subring, i.e. the image of the embedding of $R$ in $X^{-1}R$ is a subring isomorphic to $R$. 
\end{lemma}
\begin{proof}
    We first need to check that the two operations are well-defined. Let $\frac{r_1}{x_1}=\frac{r_1'}{x_1'}\in X^{-1}R$ and $\frac{r_2}{x_2}=\frac{r_2'}{x_2'}\in X^{-1}R$. Then $r_1x_1' = r_1'x_1$ and $r_2x_2'=r_2'x_2$, which means
    \begin{align*}
        (r_1x_2+r_2x_1)x_1'x_2' &= r_1x_1'x_2x_2'+r_2x_2'x_1x_1' = r_1'x_1x_2x_2'+r_2'x_2x_1x_1'=\left(r_1'x_2'+r_2'x_1'\right)x_1x_2,
    \end{align*}
    implying 
    $$\frac{r_1}{x_1}+\frac{r_2}{x_2} = \frac{r_1x_2+r_2x_1}{x_1x_2} = \frac{r_1'x_2'+r_2'x_1'}{x_1'x_2'} =\frac{r_1'}{x_1'}+\frac{r_2'}{x_2'},$$ hence addition is well-defined. In the same vein 
    $$r_1r_2x_1'x_2' = r_1x_2'r_2x_1' = r_1'x_2r_2'x_1 = r_1'r_2'x_1x_2,$$
    implies 
    $$\frac{r_1}{x_1}\frac{r_2}{x_2} = \frac{r_1r_2}{x_1x_2} = \frac{r_1'r_2'}{x_1'x_2'} = \frac{r_1'}{x_1'}\frac{r_2'}{x_2'}.$$
    We proceed to check the ring axioms. Let, in addition, $\frac{r_3}{x_3}\in X^{-1}R$ be given. Then 
    \begin{align*}
        \frac{r_1}{x_1}+\left(\frac{r_2}{x_2}+\frac{r_3}{x_3}\right) &= \frac{r_1}{x_1} + \frac{r_2x_3+r_3x_2}{x_2x_3} = \frac{r_1x_2x_3+r_2x_3x_1+r_3x_2x_1}{x_1x_2x_3} = \frac{(r_1x_2+r_2x_1)x_3+r_3x_2x_1}{x_1x_2x_3} \\
        &= \frac{r_1x_2+r_2x_1}{x_1x_2}+\frac{r_3}{x_3} = \left(\frac{r_1}{x_1}+\frac{r_2}{x_2}\right) + \frac{r_3}{x_3}.
    \end{align*}
    We define $0 := \frac{0}{1}$, with which we get 
    $$0+\frac{r_1}{x_1} = \frac{0x_1+r_1\cdot 1}{1x_1} = \frac{r_1}{x_1}.$$
    One should note that for any $x\in X$ $x\cdot 0 = 1\cdot 0$, hence
    $$\frac{0}{1} = \frac{0}{x}.$$
    We define $-\frac{r_1}{x_1}:= \frac{-r_1}{x_1}$ with which we get 
    $$\frac{r_1}{x_1}-\frac{r_1}{x_1}=\frac{r_1x_1-r_1x_1}{x_1x_1} = \frac{0}{x_1x_1}= 0.$$
    Lastly 
    $$\frac{r_1}{x_1}+\frac{r_2}{x_2} = \frac{r_1x_2+r_2x_1}{x_1x_2} = \frac{r_2x_1+r_1x_2}{x_2x_1} = \frac{r_2}{x_2}+\frac{r_1}{x_1},$$
    hence $(X^{-1}R,+)$ is an additive group. We also have that 
    \begin{align*}
        \frac{r_1}{x_1}\left(\frac{r_2}{x_2}\frac{r_3}{x_3}\right) &= \frac{r_1}{x_1}\frac{r_2r_3}{x_2x_3} = \frac{r_1(r_2r_3)}{x_1(x_2x_3)} = \frac{(r_1r_2)r_3}{(x_1x_2)x_3} = \left(\frac{r_1r_2}{x_1x_2}\right)\frac{r_3}{x_3} = \left(\frac{r_1}{x_1}\frac{r_2}{x_2} \right)\frac{r_3}{x_3}.
    \end{align*}
    We define $1 := \frac{1}{1}$. Then 
    $$1 \frac{r_1}{x_1} = \frac{1r_1}{1x_1} = \frac{r_1\cdot 1}{x_1\cdot 1} = \frac{r_1}{x_1}.$$
    Furthermore,
    \begin{align*}
        \frac{r_1}{x_1}\left(\frac{r_2}{x_2}+\frac{r_3}{x_3}\right) &= \frac{r_1}{x_1}\frac{r_2x_3+r_3x_2}{x_2x_3} = \frac{r_1r_2x_3+r_1r_3x_2}{x_1x_2x_3} = \frac{r_1r_2x_3x_1+r_1r_3x_2x_1}{x_1x_2x_1x_3} = \frac{r_1r_2}{x_1x_2} + \frac{r_1r_3}{x_1x_3}\\
        &= \frac{r_1}{x_1}\frac{r_2}{x_2}+\frac{r_1}{x_1}\frac{r_3}{x_3}.
    \end{align*}
    Thus $(X^{-1}R,+,\cdot)$ is a ring. We check that is commutative. Indeed
    $$\frac{r_1}{x_1}\frac{r_2}{x_2} = \frac{r_1r_2}{x_1x_2}= \frac{r_2r_1}{x_2x_1}=\frac{r_2}{x_2}\frac{r_1}{x_1}.$$
    Let $r,r'\in R$. Then 
    \begin{align*} 
        &r + r' = \frac{r}{1}+ \frac{r'}{1} = \frac{r+r'}{1}\in R,\\
        &-r = \frac{-r}{1}\in R\\
        &0 = \frac{0}{1}\in R\\
        &rr' = \frac{r}{1}\frac{r'}{1}= \frac{rr'}{1}\in R\\
        & 1 = \frac{1}{1}\in R.
    \end{align*}
    These computations prove that $\text{im}\ R\hookleftarrow X^{-1}R$ is a subring of $X^{-1}R$ (or that $R\hookleftarrow X^{-1}R$ is a ring homomorphism), hence $\text{im}\ R\hookrightarrow X^{-1}R\simeq R$. 
\end{proof}
\begin{proposition}
     Let $R$ be a commutative ring, $\mathfrak{p}\subset R$ a prime ideal and $X := R\setminus \mathfrak{p}$. Then $X$ is a submonoid of $(R,\cdot)$ and $X^{-1}R$ is a local ring. 
\end{proposition}
\begin{proof}
    Let 
\end{proof}
\begin{definition}
    Let $R$ be an integral domain. Let $X$ be a submonoid of $(R\setminus \{0\}, \cdot)$. We define the \textit{saturation} of $X$ to be the set
    \begin{align*}
        \widehat{X} := \left\{ r\in R : \exists r'\in R, r'r \in X \right\}.
    \end{align*}
    A submonoid $X \subset R\setminus \{0\}$ is \textit{saturated}, if 
    $$\widehat{X} = X.$$
\end{definition}
\begin{remark}
    Let $x\in X$, then $1x\in X$, hence $x\in \widehat{X}$. We thus have $X\subset \widehat{X}$. Let $r,s\in \widehat{X}$, then for some $r',s'\in R$, $r'r\in X$ and $s's\in X$. Then $r's'rs\in X$, hence $rs \in \widehat{X}$. Clearly $1\in \widehat{X}$. Thus $\widehat{X}$ is a submonoid of $R\setminus\{0\}$. The saturation of $X$ is clearly saturated. Let $Y$ be a saturated submonoid of $R\setminus \{0\}$ containing $X$. Let $r \in \widehat{X}$. Then for some $r'\in R$, $r'r\in X\subset Y$. Thus $\widehat{X}\subset Y$, hence $\widehat{X}$ is the smallest saturated submonoid of $R\setminus\{0\}$ containing $X$.
\end{remark}
\begin{lemma}
    Let $R$ be an integral domain and $X\subset R\setminus \{0\}$ a subset that is a submonoid of $(R\setminus \{0\},\cdot)$. Then $X$ is saturated if and only if for every $x,y\in R$ s.t. $xy \in X$, $x,y\in X$
\end{lemma}
\begin{proof}
    "$\implies$": Suppose $X$ is saturated. Let $x,y\in R$ s.t. $xy\in X$. Then $y\in \widehat{X}=X$ and since $yx = xy \in X$, $x\in \widehat{X} = X$.\\
    "$\impliedby$": Let $r\in \widehat{X}$, then for some $r'\in R$, $r'r\in X$, which by assumption means $r\in X$. 
\end{proof}
\begin{lemma}\label{UnitsOfLocalization}
     Let $R$ be an integral domain and $X\subset R\setminus \{0\}$ a subset that is a submonoid of $(R\setminus \{0\},\cdot)$. Consider the map 
     \begin{gather*}
         \iota : R \hookrightarrow X^{-1}R\\
         r \mapsto \frac{r}{1}
     \end{gather*}
     Let $\frac{r}{x}\in X^{-1}R$. Then 
     $$\frac{r}{x}\in \left(X^{-1}R\right)^\ast \iff r \in Y:= \iota^{-1}\left(\left(X^{-1}R\right)^\ast\right).$$
     Furthermore, $\widehat{X} = Y$. Thus 
     $$\left( X^{-1}R \right)^\ast = \left\{ \frac{r}{x}\in X^{-1}R : r \in \widehat{X}\right\}.$$
\end{lemma}
\begin{proof}
    "$\implies$": Suppose $\frac{r}{x}\in \left(X^{-1}R\right)^\ast$. Then for some $\frac{s}{y}\in X^{-1}R$, $\frac{r}{x}\frac{s}{y}=\frac{s}{y}\frac{r}{x}=1$.  From this we get that 
    $$r \frac{s}{xy} = \frac{rx}{x}\frac{s}{xy}= \frac{r}{x}x\frac{1}{x}\frac{s}{y} = 1,$$
    hence $r\in Y$.\\
    "$\impliedby$": If $r \in Y$, then $r \frac{s}{y}=1$ for some $\frac{s}{y}\in X^{-1}R$, hence $\frac{r}{x}\frac{sx}{y} = 1$, implying $\frac{r}{x} \in \left( X^{-1}R\right)^\ast$.\\
    If $r \in \widehat{X}$. Then for some $r'\in R$, $r'r\in X$. Then 
    $$r\frac{r'}{r'r} = \frac{r'r}{r'r} = 1 \implies r\in Y.$$
    Let $r \in Y$. Then for some $\frac{s}{y}\in X^{-1}S$, $r\frac{s}{y}=1$, meaning
    $$sr = sr \frac{1}{y}y = r\frac{s}{y}y= y \in X \implies r \in \widehat{X}.$$
\end{proof}
\begin{proposition}
    Let $R$ be an integral domain. Then $Q(R)$ is the smallest field containing $R$ as a subring. 
\end{proposition}
\begin{proof}
    The monoid $(R\setminus\{0\}, \cdot)$ is obviously saturated. Hence, by the above lemma,
    $$Q(R)^\ast = \left\{ \frac{r}{s}\in Q(R) : r\in R\setminus\{0\} \right\} = Q(R)\setminus\{0\}.$$
    This means $Q(R)$ is a field. Let $K$ be a field containing $R$ as a subring. Let $\frac{r}{s}\in Q(R)$. Then $r \in K$ and $\frac{1}{s}=s^{-1} \in K$, hence $\frac{r}{s} = r \frac{1}{s} \in K$. This means $Q(R) \subset K$, hence $Q(R)$ is the smallest field containing $R$ as a subring. 
\end{proof}
\begin{remark}
    From the above we conclude that if $K$ is a field then $K = Q(K)$, and in general $Q(R) =  Q(Q(R))$.
\end{remark}
\begin{definition}
    We define the \textit{rational numbers} to be the field $\Q := Q(\Z)$.
\end{definition}

\begin{lemma}\label{ExtendingRingHomomorphismToLocalization}
    Let $R$ and $S$ be integral domains. Let $X\subset R\setminus 0$ be a submonoid of $(R,\cdot)$. Let $\sigma: R\rightarrow S$ such that $\sigma(X)\subset S\setminus 0$. Then 
    \begin{gather*}
        \overline{\sigma}: X^{-1}R \rightarrow \sigma(X)^{-1}S\\
        \frac{a}{b}\mapsto \frac{\sigma(a)}{\sigma(b)}
    \end{gather*}
    is unique well-defined ring homomorphism such that $\overline{\sigma}\mid_R = \sigma$
\end{lemma}
\begin{proof}
    By assumption $\sigma(X)$ is a submonoid of $(S,\cdot)$ not containing $0$. Let $\frac{a}{b}=\frac{c}{d}\in X^{-1}R$. Then $ad = bc$, hence $$\sigma(a)\sigma(d) = \sigma(ad)=\sigma(bc)=\sigma(b)\sigma(c)\implies \overline{\sigma}\left(\frac{a}{b}\right)=\frac{\sigma(a)}{\sigma(b)}=\frac{\sigma(c)}{\sigma(d)} = \overline{\sigma}\left(\frac{c}{d}\right).$$
    Let $\frac{a}{b},\frac{c}{d}\in X^{-1}R$ be arbitrary. Then 
    $$\overline{\sigma}\left(\frac{a}{b}+\frac{c}{d}\right) = \frac{\sigma(ad+bc)}{\sigma(bd)}=\frac{\sigma(a)\sigma(d)+\sigma(b)\sigma(c)}{\sigma(b)\sigma(d)}= \frac{\sigma(a)}{\sigma(b)}+\frac{\sigma(c)}{\sigma(d)}=\overline{\sigma}\left(\frac{a}{b}\right)+\overline{\sigma}\left(\frac{c}{d}\right),$$
    and 
    $$\overline{\sigma}\left(\frac{a}{b}\frac{c}{d}\right) = \frac{\sigma(ac)}{\sigma(bd)} = \frac{\sigma(a)\sigma(c)}{\sigma(b)\sigma(d)}= \frac{\sigma(a)}{\sigma(b)}\frac{\sigma(c)}{\sigma(d)}=\overline{\sigma}\left(\frac{a}{b}\right)\overline{\sigma}\left(\frac{c}{d}\right).$$
    Lastly, let $r\in R$. Then 
    $$\overline{\sigma}(r)=\overline{\sigma}\left(\frac{r}{1}\right) = \frac{\sigma(r)}{\sigma(1)}=\frac{\sigma(r)}{1}=\sigma(r),$$
    hence in particular $\overline{\sigma}(1)=\sigma(1)=1$.
    Let $\sigma': X^{-1}R\mapsto \sigma(X)^{-1}S$ be another homomorphism with the property that $\sigma'\mid_R = \sigma.$ Let $a,b\in R$ with $b\neq0$. Then 
    $$\sigma'\left(\frac{1}{b}\right) =\sigma'\left(\frac{b}{1}\right)^{-1} = \sigma'(b)^{-1}= \frac{1}{\sigma(b)}.$$
    One then sees that 
    $$\sigma'\left(\frac{a}{b}\right) = \sigma'(a)\sigma'\left(\frac{1}{b}\right)= \sigma(a)\frac{1}{\sigma(b)}=\frac{\sigma(a)}{\sigma(b)}=\overline{\sigma}\left(\frac{a}{b}\right)\implies \sigma'=\sigma.$$
    
\end{proof}
\begin{lemma}
    The collection of pairs $(R,X)$ where $R$ is an integral domain $X\subset R\setminus 0$ a multiplicative submonoid of $R$ with morphisms being ring homomorphisms $\sigma:(R,X)\rightarrow (S,Y)$ with $\sigma(X)\subset Y\subset R\setminus 0$, $X$ defines a category. 
\end{lemma}
\begin{proof}
    If $\sigma \in \Hom((R,X),(S,Y))$ and $\tau \in \Hom((S,Y),(T,Z))$, then $\sigma(X)\subset Y$ hence $$\tau\circ \sigma(X)=\tau(\sigma(X))\subset \tau(Y)\subset Z\subset T\setminus 0.$$
    Clearly $\mathbbm{1}_{(R,X)}:= \mathrm{id}_R \in \Hom((R,X),(R,X)).$
\end{proof}
\begin{proposition}
    Call the category described in the above lemma $\mathcal{C}$. The assignment of a pair $((R,X),\sigma)$ in $(\text{Ob}(\mathcal{C}),\Hom(\mathcal{C}))$ to $(X^{-1}R,\overline{\sigma})$ in the category of integral domains defines a covariant functor. 
\end{proposition}
\begin{proof}
    Let $\tau\in \Hom((S,Y),(T,Z))$, $\sigma\in \Hom((R,X),(S,Y))$ and $\frac{a}{b}\in X^{-1}R$. Then
    $$\overline{\tau\circ\sigma}\left(\frac{a}{b}\right)=\frac{(\tau\circ\sigma)(a)}{(\tau\circ\sigma)(b)}=\frac{\tau(\sigma(a))}{\tau(\sigma(b))} = \overline{\tau}\left(\frac{\sigma(a)}{\sigma(b)}\right)=\overline{\tau}\left(\overline{\sigma}\left(\frac{a}{b}\right)\right)=(\overline{\tau}\circ\overline{\sigma})\left(\frac{a}{b}\right),$$
    hence $\overline{\tau\circ\sigma} = \overline{\tau}\circ\overline{\sigma}.$ Lastly 
    $$\overline{\mathbbm{1}_{(R,X)}}\left(\frac{a}{b}\right)=\frac{a}{b}=\mathrm{id}_{X^{-1}R}\left(\frac{a}{b}\right)=\mathbbm{1}_{X^{-1}R}\left(\frac{a}{b}\right).$$
\end{proof}
\begin{corollary}\label{LocalizationFunctorGivesIso}
    Let $R$ and $S$ be integral domains and $X\subset R\setminus 0$ a submonoid. Then $R\overset{\sigma}{\simeq} S \implies X^{-1}R\simeq \sigma(X)^{-1}R$.
\end{corollary}
\begin{definition}
    Let $R$ be an integral domain and $X\subset R\setminus 0$ a multiplicative submonoid of $R$. Consider an ideal $I\subset R$. We define the \textit{localization ideal of $I$ in $X^{-1}R$}. To be the set 
    $$X^{-1}I:=\left\{\frac{a}{x}\in X^{-1}R: a\in I, x\in X\right\}$$
\end{definition}
\begin{lemma}\label{LocalizationIdealsDescribeIdealsGeneratedFromIdealsInSubring}
    Let $R$ be an integral domain and $X\subset R\setminus 0$ a multiplicative submonoid of $R$. Let $I\subset R$ be an ideal. Then $(X^{-1}R)I = X^{-1}I$. Consequentially, $X^{-1}I$ is an ideal.
\end{lemma}
\begin{proof}
    Let $\sum_1^n \frac{r_i}{x_i}a_i\in (X^{-1}R)I$ where $\frac{r_i}{x_i}\in X^{-1}R$ and $a_i\in I$. Then 
    $$\sum_1^n \frac{r_i}{x_i}a_i= \frac{\sum_{i=1}^n\left(\prod_{j\in \{1,\dots,n\}, j\neq i} x_j\right) r_ia_i}{\prod_{j=1}^n x_i}\in X^{-1}I.$$
    The converse inclusion is trivial. 
\end{proof}
\begin{lemma}\label{LocalizationOfEliminationIdealGenerateOriginalIdeal}
    Let $R$ be an integral domain and $X\subset R\setminus 0$ a multiplicative submonoid of $R$. Let $I\subset X^{-1}R$ be an ideal. Then $X^{-1}(I\cap R) = I$.
\end{lemma}
\begin{proof}
    Let $\frac{a}{x}\in X^{-1}(I\cap R)$, where $a\in I$, $x\in X$.Then 
    $$\frac{a}{x}=\frac{1}{x}a\in I.$$
    Conversely, let $\frac{a}{x}\in I$. Then $a=x\frac{a}{x}\in I$, hence $\frac{a}{x}\in X^{-1}(I\cap R)$.
\end{proof}
    \begin{lemma}
        A local ring $R$ with principal maximal ideal is Noetherian 
    \end{lemma}
\subsubsection{Discrete Valuation Rings}
    \begin{definition}
        Let $R$ be a non-field integral domain. $R$ is a \textit{discrete valuation ring (DVR)} if it is noetherian and local with the maximal ideal is principal. 
    \end{definition}
    \begin{proposition}\label{EquivalentDefinitionOfDVR}
        Let $R$ be a non-field integral domain. Then $R$ is a DVR if and only if there is an irreducible element $t\in R$ such that for every $z\in R\setminus 0$ there are unique $u\in R^\ast$ and $n\geq 1$ satisfying $z=ut^n$
    \end{proposition}
    \begin{proof}
        "$\implies $": Let $\mathfrak{m}=\langle t\rangle$ be the maximal ideal of $R$. Then $t$ is prime hence irreducible by the maximality. Let $z\in R\setminus0$. Then either $z$ is a unit in which case $z\notin \mathfrak{m}$ or $z$ is not a unit hence $z\in\mathfrak{m}$. There is a $u\in R\setminus 0$ with $t\nmid u$ and a maximal $n$ such that $z=ut^n$. Since $u\notin \langle t\rangle$ it is a unit by maximality. Suppose $u'\in R^\ast $ and $n'\geq0$ are given such that $ut^n = u't^{n'}$. Then $n=n'$, since otherwise $t\mid u'$, hence $u=u'$.\\
        "$\impliedby$": Let $\mathfrak{m}=\langle t\rangle$. Every non-unit is of the form $ut^n$, where $n\geq 1$, hence $R\setminus R^\ast = \mathfrak{m}$, hence $R$ is local by Proposition~\ref{EquivalentFormulationOfRingBeingLocal}. Let $I\subsetneq R$ be an ideal, then $I\subset \mathfrak{m}$. Then $I=\langle t^r\rangle$, where $r=\min \{n\geq 0: t^n\in I\rangle.$ Indeed if $a\in I$, then $a=ut^n$ for some $n\geq r$, hence $a=ut^{n-r}t^{r}\in I$. Then $R$ is a PID, and hence Noetherian.   
    \end{proof}
    \begin{remark}
        We refer to an element such as $t$ as an \textit{uniformizing parameter}.The uniformizing parameters of a DVR are of the form $ut$ where $u$ is a unit and $t$ is a uniformizing parameter. Set $K=Q(R)$. Then every $z\in K\setminus0$ can be written uniquely on the form $z=ut^{n}$ for a unit $u\in R$ and an integer $n$. The integer $n$ is called \textit{the order of $z$} denoted $\ord(z)$. We set $\ord(0)=\infty$. One sees that $R=\ord^{-1}(\Z_{\geq 0}\cup\{\infty\})$ and $\mathfrak{m}=\ord^{-1}(\Z_{\geq 1}\cup\{\infty\})$. The order is independent of uniformizing parameter. Since if $t$ is a uniformizing parameter and $z = ut^n$ for unique $u\in R^\ast$, $n\in \Z$, then for a unit $s\in R$, $z= \frac{a}{b} = \frac{vst^l}{yst^k} = \frac{v}{y}t^{l-k}$ for suitable units $v,y$, $l,k\geq 0$, hence by uniqueness $\frac{v}{y}=u$ and $l-k=n$.   
    \end{remark}
    \begin{proposition}
        The localization of $\Z$ with respect to a maximal ideal $\langle p\rangle$ ($p\in \Z$ is prime), $\Z_{\langle p\rangle}$ is a DVR whose quotient field is $\Q$.
    \end{proposition}
    \begin{proof}
        One notes that $\Z_{\langle p\rangle} = \{\frac{a}{n}\in \Q : p\nmid n\}$. Suppose $p = \frac{a}{n}\frac{b}{m}$. Then $mn\mid ab$, WLOG $mn=1$, hence $m=1$ and $n=1$. Then $p=ab$ hence $a$ or $b$ is a unit, meaning $p$ is irreducible in $\Z_{\langle p\rangle}$. For $\frac{a}{n}\in \Z_{\langle p\rangle}$ let $\nu_p\left(\frac{a}{n}\right)=\max(\{n\geq 0 : p^n\mid \frac{a}{n}\}).$ One eaily checks that $p\mid \frac{a}{n}$ if and only if $p\mid a$, hence $\nu_p(\frac{a}{n})=\nu_p(a).$ We thus get that 
        $$\frac{a}{n} = \frac{q}{n}p^{\nu_p(a)},$$
        where $p\nmid q$. Note that $\frac{n}{q}$ is the inverse of $\frac{q}{n}$. The uniqueness of this decomposition follows from $p$ not being being a unit. Proposition~\ref{EquivalentDefinitionOfDVR} shows that $\Z_{\langle p\rangle}$ is a DVR. Every element in $\Q$ can be written as $\frac{a}{b}=\frac{s}{t}p^{\nu_p(a)-\nu_p(b)} = \frac{s}{t}p^{\ord(\frac{a}{b})}$, where $p\nmid s,t$, hence $\Q \subset Q(\Z_{\langle p\rangle })\subset \Q$.
    \end{proof}
    \begin{lemma}\label{InversesOfNonRingElementsInQuotientFieldOfDVR}
        Let $R$ be a DVR. Set $K=Q(R)$. Let $\mathfrak{m}=\langle t\rangle$ be the maximal ideal in $R$. If $z = \frac{a}{b}t^{\ord(z)}\in K\setminus R$, then $z^{-1}\in \mathfrak{m}$.
    \end{lemma}
    \begin{proof}
        Note that 
        $$K\setminus R = \left\{u\frac{1}{t^{n}} : u\in R^\ast, n\geq 0\right\},$$
        hence $z=ut^{-n}$ for suitable $u\in R^\ast$, $n\geq0$. Then $z^{-1}=u^{-1}t^n\in\mathfrak{m}$.
    \end{proof}
    \begin{proposition}\label{UniquenessPropertyOfDVRs}
        Let $S$ be a DVR containing a subring $R$ which is also a DVR. Set $K=Q(R)$ and suppose $S\subset K$. Let $\mathfrak{m}=\langle t\rangle$ be the maximal ideal in $R$. If the maximal ideal of $S$, $\mathfrak{n}=\langle s\rangle$, contains $\mathfrak{m}$, then $S=R$.
    \end{proposition}
    \begin{proof}
        Since $t\in \mathfrak{n}$, $t = us^n$ for some $n\geq 1$. By irreducibility of $t$, $t=us$. Let $v\in S^\ast$. Then $v^{-1}\notin \mathfrak{n}\supset \mathfrak{m}$, hence $v\in R$ by the prior lemma. This means $R^\ast=S^\ast$. Thus if $x\in S$, $x=vs^n=ut^n$ for some $n\geq0$, $v\in S^\ast=R^\ast$, hence $x\in R$.
    \end{proof}
    \begin{definition}
        An \textit{order function} on a field $K$ is a function $\nu : K\rightarrow \Z \cup \{\infty\}$, satisfying:
        \begin{enumerate}
            \item for every $a\in K$, $\nu(a) = \infty \iff a =0$,
            \item for every $a,b\in K$, $\nu(ab)=\nu(a)+\nu(b)$,
            \item for every $a,b\in K$, $\nu(a+b)\geq \min(\nu(a),\nu(b))$.
        \end{enumerate}
    \end{definition}
    \begin{definition}
        Let $K$ be a field with an order function $\nu$. We define the ring induced by $\nu$ to be the set 
        $$R_\nu(K):= R_\nu := \nu^{-1}(\Z_{\geq 0}\cup\{\infty\})=\{r\in K: \nu(r)\geq0\}.$$
        Define the ideal induced by $\nu$ to be the set 
        $$\mathfrak{m}_\nu := \nu^{-1}(\Z_{\geq 1}\cup\{\infty\})=\{r\in R_\nu : \nu(r)>0\}\subset R_\nu$$
    \end{definition}
    \begin{lemma}
        Let $K$ be a field with an order function $\nu$. We collect the following facts about $R_\nu$ and $\mathfrak{m}_\nu$: 
        \begin{enumerate}[(i)]
            \item $R_\nu$ is subring of $K$ and hence is an integral domain. 
            \item For every $u\in R_\nu$, 
            $$u\in R_\nu^\ast \iff \nu(u)=0.$$
            \item $\mathfrak{m}_\nu$ is the unique maximal ideal of $R_\nu$, hence $R_\nu$ is local.
            \item $R_\nu = K$ if and only if $\nu$ is trivial. If $\nu$ is non-trivial, $R_\nu$ is not a field. 
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        (i) Property 2. ensures that $R_\nu$ is closed under multiplication while property 3. ensures that $R_\nu$ is closed under addition. $0\in R_\nu$ by property 1. Note that $\nu(1)=\nu(1\cdot1)=\nu(1)+\nu(1),$ hence $\nu(1)=0$, hence $1\in R_\nu$. Let $u\in R_\nu^\ast$. Then $0=\nu(1)=\nu(uu^{-1}) = \nu(u)+\nu(u^{-1})$, hence $\nu(u^{-1})=-\nu(u)$ and since $\nu(u)\geq0$, it follows that $\nu(u^{-1})=0$. We thus in particular find that $\nu(-1)=0$, hence for any $r\in R_\nu$ we have that $\nu(-r)=\nu(-1\cdot r)=\nu(-1)+\nu(r)=\nu(r)$. It thus follows that $-r\in R_\nu$. We thus get that $R_\nu$ is a subring of $K$.\\
        (ii) "$\implies$": This was already proven in the proof of (i).\\
        "$\impliedby$": Let $u\in R_\nu$ such that $\nu(u)=0$. For some $v\in K\setminus 0$, $uv=vu=1$. Then $0=\nu(1)=\nu(uv)=\nu(u)+\nu(v) = \nu(v)$, hence $v\in R_\nu$, which means $u$ is a unit in $R_\nu$.\\
        (iii) $\mathfrak{m}_\nu=R_\nu\setminus R_\nu^\ast$, hence it is sufficient to prove that $\mathfrak{m}_\nu$ is an ideal by Proposition~\ref{EquivalentFormulationOfRingBeingLocal}. $\mathfrak{m}_\nu$ is closed under addition by property 3. Let $r\in R_\nu$, $x\in \mathfrak{m}_\nu$. Then $\nu(rx)=\nu(r)+\nu(x)\geq 0+1=1,$ hence $rx\in \mathfrak{m}_\nu$.
        (iv) "$\implies$": Suppose $\nu$ is not trivial. Then there is some $x\in K\setminus 0$ such that $\nu(x)\geq 1$, then $\nu(x^{-1})\leq 1$, hence $x^{-1}\notin R_\nu$.\\
        "$\impliedby$": Suppose $\nu$ is trivial. Then for $x\in K$
        $$\nu(x)=\begin{cases}
            0 & \text{if } x\neq 0\\
            \infty &\text{otherwise}
        \end{cases},$$
        in any case $\nu(x)\geq 0,$ hence $x\in R_\nu$.\\
    \end{proof}
    \begin{theorem}
        Let $K$ be a field and $\nu$ a non-trivial order function on $K$. Then $R=\{z\in K : \nu(z)\geq 0\}$ is a DVR with maximal ideal $\mathfrak{m}:=\{z\in R : \nu(z)>0\}$ such that $Q(R_\nu)=K$. Conversely, if $R$ is a DVR with quotient field $K$, then $\ord : K \rightarrow \Z\cup \{\infty\}$ is an order function. We thus obtain a one-to-one correspondence between DVR's and order functions. 
    \end{theorem}
    \begin{proof}
         By the above lemma it is sufficient to check that $R_\nu$ is a PID. Let $I\subset R_\nu$ be a non-zero ideal. Let $s\in I$ be given such that $\nu(s)=\min(\nu(I))$. Let $x\in I$. For some $q\in K$, $x=qs$. Since 
         $$\nu(x)=\nu(q)+\nu(s)\implies \nu(q)=\nu(x)-\nu(s)\geq 0,$$
         it follows that $s\mid x$ in $R_\nu$, hence $I=\langle s\rangle$. Let $k\in K$. Then either $k\in R_\nu$ or $k^{-1}\in R_\nu $. In the first trivially $k\in Q(R_\nu)$. In the second case $k=(k^{-1})^{-1} = \frac{1}{k^{-1}}\in Q(R_\nu)$.\\
         For the second statement, we have for $a\in R$ that $\ord(a)=\infty$ if and only if $a=0$. Let $a,b\in K$. Then $a=ut^m$ and $b=vt^l$ for unique $u,v\in R^\ast$ and $l:=\ord(b)$, $m:=\ord(a)$.First, we get that
         $$ab=uvt^{m+l}\implies \ord(ab)=m+l=\ord(a)+\ord(b),$$
         hence $\ord$ satisfies property 2. Secondly,
         $$a+b = \underbrace{\left(ut^{m-\min(m,l)}+vt^{l-\min(m,l)}\right)}_q t^{\min(m,l)}.$$
         Note that $q\in R$ since $m-\min(m,l),l-\min(m,l)\geq 0$. We then have that $q = \alpha t^d$ for a unique unit $\alpha\in R^\ast$ and $d:=\ord(q)\geq0$. This implies that $a+b = \alpha t^{d+\min(l,m)},$ hence $$\ord(a+b)=d+\min(m,l)\geq \min(m,l)=\min(\ord(a),\ord(b)),$$
         proving property 3.
    \end{proof}
    \begin{remark}
        One notes from the above proof that the uniformizing parameter for $R_\nu$ is $t\in R_\nu$ where $\nu(t)=\min(\nu(\mathfrak{m}_\nu)).$
    \end{remark}
    \begin{lemma}\label{TechnicalDVRLemma}
        Let $R$ be a DVR with $K:=Q(R)$. If $a_1,\dots,a_n\in K$ where for some $i$ $\ord(a_i)<\ord(a_j)$ for every $j\neq i$, then $\ord(\sum_1^n a_j)=a_i$ and $\sum_1^n a_j\neq 0$
    \end{lemma}
    \begin{proof}
        We prove the first statement using induction in $n\geq 2$. Consider first the case $n=2$. WLOG $m_1:=\ord(a_1)<\ord(a_2):=m_2$. Write $a_1=u_1t^{m_1}$ and $a_2=u_2t^{m_2}$ for suitable $u_1,u_2\in R^\ast$. Then
        $a_1+a_2=(u_1+u_2t^{m_2-m_1})t^{m_1}$ and since $t\nmid u_1$ and $t\mid u_2t^{m_2-m_1}$, it follows that $t\nmid u:=u_1+u_2t^{m_2-m_1}$, hence $u$ is a unit in $R$, meaning $\ord(a_1)=\ord(a_1+a_2)$.\\
        WLOG $i=n+1$. Note that $\ord(a_n+a_{n+1})=\ord(a_{n+1})$, hence setting $a_n'=a_n+a_{n+1}$, we get $\ord(a_n')=\ord(a_{n+1})<a_j$ for every $j\in\{1,\dots,n-1\}$. By induction it follows that $$\ord(\sum_1^{n+1} a_j)=\ord(\sum_1^{n-1} a_j+(a_n+a_{n+1}))=\ord(\sum_1^{n-1}a_i+a_n')=\ord(a_n')=\ord(a_{n+1}).$$
        Since $\ord(\sum_1^{n}a_j)<\ord(a_j)\leq \infty$, it follows that $\sum_1^na_{j}\neq 0$ by property 1. of order functions.  
    \end{proof}
    \begin{lemma}\label{UniqueDecompOfElementsInDVRsThatAreKAlgebras}
        Let $R$ be a DVR with maximal ideal $\mathfrak{m}$, $L:=Q(R)$ and a subring $K$ that is a field. Suppose that the composition $\sigma: K \hookrightarrow R \twoheadrightarrow L$ is an isomorphism. Then for any $z\in R$ there is a unique $\lambda \in K$ such that $z-\lambda\in \mathfrak{m}$. 
    \end{lemma}
    \begin{proof}
        \textbf{The case $z\in \mathfrak{m}$:} Pick $\lambda =0$. By the prior lemma for any $\mu\in K\setminus0$ $\ord(z-\mu)=\ord(\mu)=0$, hence $z-\mu\notin \mathfrak{m}$.\\
        \textbf{The case $z\notin \mathfrak{m}$:} Then $z$ is a unit in $R$. By {\LARGE a result in the "A First Look a Algebras"-subsubsection} $\sigma$ is a $K$-algebra isomorphism. For some $\lambda \setminus 0$, $\lambda =\sigma(\lambda)=z$, hence $z-\lambda =0\in \mathfrak{m}$. Since $\sigma$ is injective it follows that $\lambda$ is unique. 
    \end{proof}
    \begin{proposition}\label{PropositionTowardsPowerSeriesDescriptionOfCerationDVRs}
        We assume the same setup as the prior lemma. Let $t$ be a uniformizing parameter of $R$ and $z\in R$. For any $n\geq 0$ there are unique $\lambda_0,\dots,\lambda_n\in K$, $z_{n}\in R$ such that 
        $$z = \sum_0^n \lambda_it^i + z_nt^{n+1}.$$
    \end{proposition}
    \begin{proof}
        \textbf{Existence:} For the case $n=0$ the statement follows from Lemma~\ref{UniqueDecompOfElementsInDVRsThatAreKAlgebras}. Assuming the statement is true for some $n\geq0$, we can write 
        $$z = \sum_0^n \lambda_it^i+z_nt^{n+1}.$$
        If $z_n \in R^\ast = K\setminus 0$, pick $\lambda_{n+1} := z_n$ and $z_{n+1}:=0$. Otherwise write $z_n = ut^l$, $l\geq 1$ and pick $\lambda_{n+1}:=0$, $z_{n+1} := ut^{l-1}$.\\
        \textbf{Uniqueness:} Suppose there are $\lambda_0,\dots,\lambda_n,\mu_1,\dots,\mu_n\in K$ and $z_n,w_n\in R$ such that 
        $$\sum_0^n \lambda_it^i +z_nt^{n+1} = \sum_0^n \mu_it^i +w_nt^{n+1},$$
        Then $\sum_0^n (\lambda_i-\mu_i)t^i + (z_n-w_n)t^{n+1}=0,$
        hence $\ord((\lambda_i-\mu_i)t^i)=\ord((\lambda_j-\mu_j)t^j)$ for every $i,j$ hence $(\lambda_i-\mu_i)t^i=0$ for every $i$, implying $\lambda_i=\mu_i$. Similarly one can conclude that $z_n=w_n$.  
    \end{proof}
    \begin{lemma}
        We keep the same setup as the above two results. Then $\dim_K\ \mathfrak{m}^n/\mathfrak{m}^{n+1} = 1$ for every $n\geq 0$. 
    \end{lemma}
    \begin{proof}
        We use induction in $n\geq 0$. We first prove that 
        $$\mathfrak{m}^n/\mathfrak{m}^{n+1} = \{ \lambda t^n + \mathfrak{m}^{n+1} : \lambda\in K\}.$$
        This is clear since any element in $\mathfrak{m}^n$, is equal to $\lambda t^n + zt^{n+1}$ for some $\lambda\in K$, $z\in R$ as by Lemma~\ref{TechnicalDVRLemma} $\ord(\sum_0^n\lambda_it^i) = l$ where $l$ is the smallest index such that $\lambda_l\neq 0$. Hence in the image of $\mathfrak{m}^n/\mathfrak{m}^{n+1}$ such an element is given by $\lambda t^n + zt^{n+1} + \mathfrak{m}^{n+1} = \lambda t^n + \mathfrak{m}^{n+1}$. It follows that 
        \begin{gather*}
            \sigma : K \rightarrow \mathfrak{m}^n/\mathfrak{m}^{n+1}\\
            \lambda \mapsto \lambda t^n+\mathfrak{m}^{n+1}
        \end{gather*}
        is a surjective $K$-algebra. Suppose $\lambda t^n \in \mathfrak{m}^{n+1}$. Then $\ord(\lambda)>0$, hence $\lambda = 0$. We thus conclude that $\mathfrak{m}^n/\mathfrak{m}^{n+1} \simeq K$, meaning $\dim \ \mathfrak{m}^n/\mathfrak{m}^{n+1} = \dim\ K = 1.$
    \end{proof}
    \begin{lemma}\label{OrderIsDimensionOfQuotientKAlg}
        We keep the setup from the prior results. For each $n\geq 0$, $\dim\ R/\mathfrak{m}^n = n$. It follows that $\ord(z) = \dim\ R/\langle z\rangle =  \dim\ R/\mathfrak{m}^{\ord(z)}$ for each $z\in R$.
    \end{lemma}
    \begin{proof}
        We prove the result by induction in $n$. The base case is trivial. By Lemma~\ref{NoetherSecondIsoForModules} and Lemma~\ref{DimensionsOfExactSequence} and the induction hypothesis it follows that 
        $$\dim\ R/\mathfrak{m}^{n+1} = \dim\ \mathfrak{m}^n/\mathfrak{m}^{n+1}+\dim\ R/\mathfrak{m}^n = n+1,$$
        where we also use the prior lemma. We now that $z = \lambda t^{\ord(z)}$, hence $\langle z \rangle = \langle t\rangle^{\ord(z)} = \mathfrak{m}^{\ord(z)},$ hence $\ord(z) = \dim \ R/\mathfrak{m}^{\ord(z)}.$
    \end{proof}    
\subsection{Polynomial Rings \& Formal Power Series}
In this subsection every ring will be commutative, unless we explicitly declare it to not (necessarily) be the case. Really the base ring for a polynomial ring need not be commutative, but for our purposes we do not need to explore the non-commutative case. By a polynomial in $n$ variables over a ring $R$, we mean some expression of the form 
$$\sum_{v=(v_1,\dots,v_n)\in \N^n} a_vx_1^{v_1}\cdots x_n^{v_n},$$
where $x_1,\dots,x_n$ are \textit{variables} and $a_v = 0$ for all but finitely many $a_v\in R$. Thus we want to consider elements of the algebra over $R$ generated by $x_1,\dots,x_n$, i.e. $R[x_1,\dots,x_n]$. The term variable is informal, and our goal will be to make the term variable precise. There are some properties that we want these variables to have. For instance we do not want $x_i = x_j$ when $i\neq j$. In general, we want $x_1^{v_1}\cdots x_n^{v_n} \neq x_1^{w_1}\cdots x_n^{w_n}$ whenever $(v_1,\dots,v_n)\neq (w_1,\dots,w_n)$. To do this, we first introduce the notion of algebraic (in)dependence
\begin{definition}
    Let $S$ be an $R$-algebra. We say a finite sequence of elements 
    $s_1,\dots,s_n\in Z(S)$ are \textit{algebraically independent} over $R$ if for every 
    finite sequence $(a_v)\in \prod_{v\in \N^n} R$, which is not the sequence $(0)\in \prod_{v\in \N^n} R$, we get that 
    $$\sum_{v\in \N^n} a_v s_1^{v_1}\cdots s_n^{v_n} \neq 0.$$
    If a finite sequence of elements in $S$ are not algebraically independent over $R$, we say that they are \textit{algebraically dependent}. 
\end{definition}
One quickly sees that the concept over algebraic independence is really just a special case of linear independence.
\begin{lemma}\label{AlgebraicallyIndependentIffMonomialsLinearlyIndependent}
    Let $S$ be an $R$-algebra. Then a finite sequence in $s_1,\dots,s_n \in Z(S)$ is algebraically independent over $R$ if and only if $\left\{s_1^{v_1}\cdots s_n^{v_n}\right\}_{(v_1,\dots,v_n)\in \N^n}$ is linearly independent over $R$.
\end{lemma}
We also want that elements of $R$ can be seen as polynomials. Before proceeding with actually constructing a polynomial ring that does the job we will present the approach that at first might seem fruitful, but will not quite capture the behaviour we desire. That is to define $R[x_1,\dots,x_n]$ as the set of functions 
$$\text{Pol}(R^n,R):= \left\{ f : R^n\rightarrow R : \substack{f(x_1,\dots,x_n) = \sum_{v\in \N^n} a_vx_1^{v_1}\cdots x_n^{v_n} \text{ for some}\\ \text{ finite sequence } \{a_v\}_{v\in\N^n}\subset R  \text{ for all } (x_1,\dots,x_n)\in R^n} \right\},$$
i.e. the set of polynomial functions, which is a subring of $\text{Fun}(R^n,R)$, the set of functions from $R^n$ to $R$. The main issue is that we can't always distinguish terms of form $x_1^{v_1}\cdots x_n^{v_n}$. For instance, if $\#R < \infty$, then clearly $\#\text{Pol}(R^n,R) < \infty$, but we want the number of distinct terms $x_1^{v_1}\cdots x_n^{v_n}$ to be countably infinite. To be concrete, taking $R := \Z/2\Z$ then $x\mapsto x$ and $x\mapsto x^2$ is the same function, hence $x = x^2 \iff x^2-x = 0$, thus $\text{Pol}(R^n,R)$ fails to produce the right notion of variable in a lot of cases. Instead we will present an alternative approach.  
\subsubsection{Defining the Polynomial Ring} 
In this subsection we give a rigorous construction of the polynomial ring. 
\begin{definition}
    Consider the function 
    \begin{gather*}
        \vert \bullet \vert : \N^n \rightarrow \N\\
        v=(v_1,\dots,v_n) \mapsto \sum_1^n v_i,
    \end{gather*}
    For an $n$-tuple $v\in \N^n$ we will refer to the quantity $\vert v\vert$ as the \textit{modulus} of $v$.
\end{definition}
\begin{remark}
    One easily sees that a sequence $(a_v) \in \bigoplus_{v\in \N^n} R$ for some ring $R$ is finite if and only if $a_v = 0$ whenever $\vert v\vert > N$ for some $N\geq 0$
\end{remark}
One recalls that $(\N^n,+)$ is a commutative monoid for every $n\geq 1$, where for $v=(v_1,\dots,v_n)$ and $w = (w_1,\dots,w_n)$ in $\N^n$,
$$v+w := (v_1+w_1,\dots,w_n+v_n).$$ 
We have the following result. 
\begin{lemma}\label{ModulusFunctionIsAdditive}
    For every $n\geq 1$, the modulus function is additive.
\end{lemma}
\begin{proof}
    Let $v,w\in \N^n$ with $v=(v_1,\dots,v_n)$ and $w=(w_1,\dots,w_n)$. Then
    $$\vert v+w\vert = \vert (v_1+w_1,\dots,v_n+w_n)\vert = \sum_1^n (v_i+w_i) = \sum_1^n v_i + \sum_1^n w_i = \vert v\vert +\vert w\vert.$$
\end{proof}
\begin{definition}
    Let $R$ be a ring and $n$ a positive integer. By a \textit{polynomial in $n$ variables over $R$}, we mean an element $(a_v)$ in the left $R$-module $\bigoplus_{v\in \N^n} R$. We denote the set of polynomials in $n$ variables over $R$ by $R[\N^n]$, i.e. $R[\N^n] := \bigoplus_{v\in \N^n} R$.
\end{definition}
With this definition we already have that $R[\N^n]$ is a left $R$-module, since it is an $R$-submodule of $\prod_{v\in \N^n} R$. The set $\{e_v : v\in \N^n\}$ where $e_v = (\delta_{vw})\in \prod_{w\in \N^n} R$ is a basis of $R[\N^n]$. We now aim to equip $R[\N^n]$ with a suitable multiplication. We do this by adding structure of ring on $\prod_{v\in \N^n} R$ and showing that $R[\N^n]$ is a subring. The set $\prod_{v\in \N^n} R$ with this structure of ring is called \textit{the ring of formal power series in $n$ variables over $R$}. 
\begin{lemma}
    We define multiplication on $\prod_{v\in \N^n} R$ by
    $$(a_v)(b_v)=\left( \sum_{v,w\in \N^n : v+w =u} a_vb_v \right)\in \prod_{u\in \N^n} R.$$
    This multiplication makes $\prod_{v\in \N^n} R$ a commutative ring. $R\left[\N^n\right]$ is a subring of $\prod_{v\in \N^n} R$.  
\end{lemma}
\begin{proof}
    Let $(a_v),(b_v),(c_v)\in \prod_{v\in \N^n} R$. Then 
    \begin{align*}
        ((a_v)(b_v))(c_v)&=\left(\sum_{v,w\in \N^n : v+w= u} a_vb_w\right)(c_v) =\left( \sum_{r,u\in \N^n : r+u=s} \left(\sum_{v,w\in \N^n : v+w = u} a_vb_w \right)c_r\right)\\
        &= \left( \sum_{r,v,w\in \N^n : r+v+w = s} (a_vb_w)c_r\right) = \left( \sum_{r,v,w\in \N^n : r+v+w = s} a_v(b_wc_r)\right)\\
        &= \left(\sum_{u,v\in \N^n u+v = s} a_v\left(\sum_{r,w\in \N^n : r+w =  u} b_wc_r \right)\right) = (a_v)\left(\sum_{r,w\in \N^n : r+w = u} b_wc_r \right)\\
        &= (a_v)((b_v)(c_v)). 
    \end{align*}
    Put $\mathbf{0}:= (0,\dots,0)\in \N^n$. We then define $1 := e_\mathbf{0} = (\delta_{\mathbf{0}v})$. Then 
    $$1(a_v) = \left(\sum_{v,w\in \N^n : v+w = u} \delta_{\mathbf{0}v} a_w \right) = \left(\sum_{w\in \N^n : w = u}  a_w \right) = (a_v).$$
    Similarly it is easy to check that $(a_v)1 = (a_v)$. Finally we have that 
    \begin{align*}
        (a_v)\left(((b_v)+(c_v)\right) &= \left(\sum_{v,w\in \N^n : v+w = u} a_v(b_w + c_w) \right) = \left(\sum_{v,w\in \N^n : v+w = u} a_vb_w + a_vc_w \right)\\ 
        &= \left(\sum_{v,w\in \N^n : v+w = u} a_vb_w + \sum_{v,w\in \N^n : v+w = u}a_vc_w \right)\\
        &= \left(\sum_{v,w\in \N^n : v+w = u} a_vb_w\right) + \left(\sum_{v,w\in \N^n : v+w = u}a_vc_w \right) = (a_v)(b_v)+(a_v)(c_v).
    \end{align*}
    This means $\prod_{v\in \N^n} R$ is a ring with this multiplication. Note also that
    $$(a_v)(b_v) = \left( \sum_{v,w\in \N^n : v+w =u} a_vb_w \right) =\left( \sum_{v,w\in \N^n : v+w =u} b_va_w \right)=(b_v)(a_v).$$
    Hence $\prod_{v\in \N^n} R$ is a commutative ring with this multiplication.\\
    To check that $R\left[\N^n\right]$ is a subring of $\prod_{v\in \N^n} R$, we need to check that $1\in R\left[\N^n\right]$ and that $R\left[\N^n\right]$ is closed under multiplication. Since $\delta_{\mathbf{0},v}=0$ for every $v\in \N^n \setminus \{\mathbf{0}\}$, it follows that $1\in R\left[\N^n\right]$. Let $(a_v),(b_v)\in R[\N^n]$. We note that for some $N,M\geq 0$, $a_v = 0$ for $v\in \N^n$ with $\vert v\vert \geq N$ and $b_w = 0$ for $w\in \N^n$ with $\vert w \vert \geq M$. Let $u\in \N^n$ with $\vert u\vert \geq N+M$. Consider then $v,w \in \N^n$ such that $v+w$. Then using {\LARGE LEMMA?}
    $$\vert v\vert + \vert w\vert = \vert v +w \vert = \vert u\vert \geq N+M \implies \vert v\vert \geq N \text{ or } \vert w \vert \geq M \implies a_vb_w = 0. $$
    Thus $\sum_{v,w\in \N^n : v+w = u} a_vb_w =  0$, meaning $(a_v)(b_v) \in R\left[\N^n\right]$.
\end{proof}
\begin{remark}
    As a notational trick one often denotes an $(a_v) \in \prod_{v\in \N^n}$ by $\sum_{v\in \N^n} a_v\mathbf{x}^v$, where $\mathbf{x}^v$ is a short-hand notation for $x_1^{v_1}\cdots x_n^{v_n}$. With this choice of notation the elements of $\prod_{v\in \N^n} R$ are seen to act like some sort of power series in $n$ variables with coefficients in $R$, where we of course "forget" the notion of convergence. The ring of formal power series in $n$ variables is denoted $R\llbracket x_1,\dots,x_n\rrbracket$, but for now we will make no more remarks about this ring and focus on the polynomial ring. We will see that the notation $\sum_{v\in \N^n} a_v\mathbf{x}^{v}$ is actually on the nose in the sense of Remark~\ref{SumsOfInfinitelyOftenZeroGroupElements} for the ring of polynomials. 
\end{remark}
\begin{definition}\label{MonomialDefinitionAndVariableDefinition}
    Let $R$ be a ring and $n$ a positive integer. Consider a $w\in\N^n$. We define the \textit{monomial} associated with $w$ as the polynomial 
    $$ e_v=(\delta_{vw})\in R[\N^n].$$
    For $i\in\{1,\dots,n\}$ we define \textit{the $i$'th variable} in $R[\N^n]$ to be monomial associated with the $n$-tuple of non-negative integers for which the $i$'th entry is $1$ and for which the remaining entries are $0$.
\end{definition}
\begin{remark}
    A note on notation: We choose to denote the $i$'th variable by some letter, say $x$, subscripted by $x_i$, i.e. we denote the $n$ variables by $x_1,\dots,x_n$. With this choice of letter we choose denote a monomial associated with a $v\in \N^n$ by $\mathbf{x}^v$. Later on this notation will be motivated. Had we chosen $y$ as our letter we would get variables $y_1,\dots,y_n$ and monomials $\mathbf{y}^v$. This remark is not of a mathematical nature and serves only as an excuse to not explicitly state what is meant by a notation a'la $\mathbf{x}^v$ every time we make use of it.    
\end{remark}
\begin{lemma}
    Any element $f=(a_v)\in R\left[\N^n\right]$ can be written uniquely as 
    $$\sum_{v\in \N^n} a_v\mathbf{x}^v,$$
\end{lemma}
\begin{proof}
    This is just a matter of book keeping. Since $\{\mathbf{x}^v : v \in \N^n \}=\{e_v : v\in \N^n\}$ is a basis of $R\left[ \N^n\right]=\bigoplus_{v\in \N^n} R$, it follows that for some finite set $X\subset \N^n$, $a_v \neq 0$ for every $v\in \N^n$. Hence 
    $$f = \sum_{v\in \N^n} a_v\mathbf{x}^v.$$
    The uniqueness of this representation follows from $\{\mathbf{x}^v : v \in \N^n \}$ being a basis.
\end{proof}
\begin{remark}
    A further consequence is that for $f=\sum_{v\in \N^n} a_v\mathbf{x}^v,g = \sum_{v\in \N^n} b_v\mathbf{x}^v\in R[\N^n]$
    $$fg = \sum_{u\in \N^n} c_u\mathbf{x}^u,$$
    where $(c_u) = \left(\sum_{v,w\in \N^n : v+w = u} a_vb_w\right).$
    Suppose $f = \mathbf{x}^\nu$ and $g = \mathbf{x}^\mu$. Then 
    $$fg = (c_u)= \left(\sum_{v,w\in \N^n : v+w = u} \delta_{\nu v}\delta_{\mu w} \right).$$
    Note that 
    $$\delta_{\nu v}\delta_{\mu w} = 0 \iff \delta_{\nu v} =0 \text{ or } \delta_{\mu w} = 0 \iff \nu = v \text{ or } \mu = w,$$
    hence $\delta_{\nu v}\delta_{\mu w} = 1$ if  $\nu = v \text{ and } \mu = w$ and $0$ else. Thus $c_u = 1$ when $u = \nu + \mu$ and else it is $0$. This means $f = (\delta_{v, \nu + \mu}) = \mathbf{x}^{\nu + \mu}$. It follows that $\mathbf{x}^\mu = x_1^{\mu_1}\cdots x_n^{\mu_n}$. Any polynomial can thus uniquely be represented as sum
    $$\sum_{v\in \N^n} a_v\mathbf{x}^v =\sum_{v=(v_1,\dots,v_n)\in \N^n} a_vx_1^{v_1}\cdots x_n^{v_n}.$$
    One useful fact that one should note is that this means that for some $v_1,\dots,v_m\in \N^n$, we have that the above is equal to
    $$\sum_1^n a_{v_i}\mathbf{x}^{v_i}.$$
    Another way of representing the above, which also may be useful is that for some $N$, the above is equal to
    $$\sum_{v\in \N^n : \vert v \vert \leq N} a_v\mathbf{x}^v $$
\end{remark}
We record the fact that $R$ is embedded as a subring in a polynomials in the most natural way
\begin{lemma}
   $R1$ is a subring of $R\llbracket  x_1,\dots,x_n\rrbracket$ contained in $R[x_1,\dots,x_n]$, ring isomorphic to $R$. Furthermore, $R1[x_1,\dots,x_n] = R[x_1,\dots,x_n]$. Lastly $x_1,\dots,x_n$ are algebraically independent over $R$. 
\end{lemma}
\begin{proof}
    We consider the map
    \begin{gather*}
        \sigma : R \rightarrow R1\\
        r\mapsto r1 = r
    \end{gather*}
    This is clearly a surjective ring homomorphism hence $R1$ is subring of $R[x_1,\dots,x_n]$ whose inverse is 
    \begin{gather*}
        \sigma^{-1}: R1 \rightarrow R\\
        r1 \mapsto r
    \end{gather*}
    We already know that $R1[x_1,\dots,x_n]$ is a subring of $R\left[\N^n\right]$. Furthermore we have already seen in the above remark that any element in $R\left[x_1,\dots,x_n\right]$ can be written as finite linear combination over $R$ of elements in $\left\{x_1^{v_1}\cdots x_n^{v_n} : (v_1,\dots,v_n)\in \N^n \right\}$. The fact that this set constitutes a basis of $R\left[x_1,\dots,x_n\right]$ over $R$, means that $x_1,\dots,x_n$ are algebraically independent over $R$ (cf. Lemma~\ref{AlgebraicallyIndependentIffMonomialsLinearlyIndependent}).
\end{proof}
\begin{remark}
    We have now fully justified the existence of a polynomial ring with the properties described in the introduction to this subsection. In summary, we found that the set of finite sequences in $R$ indexed by elements in $\N^n$ could be endowed with the structure we sought after. From now we we will "forget" the underlying structure.
\end{remark}
We collect all the data established about the polynomial ring in the following theorem
\begin{theorem}\label{PolynomialRingActsLikePolynomialRing}
    The rings $R\llbracket x_1,\dots,x_n\rrbracket$ and
    $$R[x_1,\dots,x_n] = \left\{ \sum_{v\in \N^n} a_v\mathbf{x}^v : a_v\in R, a_v = 0 \text{ whenever } \vert v \vert > N \text{ for some } N \geq 0  \right\},$$
    are rings containing $R$ as a subring. $R[x_1,\dots,x_n]$ is generated by $x_1,\dots,x_n$. Furthermore, $x_1,\dots,x_n$ are algebraically independent over $R$.
\end{theorem}
\subsubsection{Specializations of Polynomials}
\begin{proposition}
    Let $R$ and $S$ be commutative rings and consider a ring homomorphism $\sigma : R \rightarrow S$. Then $\sigma$ induces a well-defined ring homomorphism given by
    \begin{gather*}
        \overline{\sigma} : R[x_1,\dots,x_n]\rightarrow S[x_1,\dots,x_n]\\
        \sum_{v\in \N^n} a_v\mathbf{x}^v \mapsto \sum_{v\in \N^n}\sigma(a_v)\mathbf{x}^v
    \end{gather*}
\end{proposition}
\begin{proof}
    Suppose $\sum_{v\in \N^n} a_v\mathbf{x}^v = \sum_{v\in \N^n} b_v\mathbf{x}^v\in R[\mathbf{x}]$ then $a_v=b_v$ for every $v\in \N^n$ hence $\sigma(a_v)=\sigma(a_v)$ for every $v\in \N^n$, meaning 
    $$\overline{\sigma}\left(\sum_{v\in \N^n} a_v\mathbf{x}^v\right)=\sum_{v\in \N^n } \sigma(a_v)\mathbf{x}^v= \sum_{v\in \N^n } \sigma(b_v)\mathbf{x}^v = \overline{\sigma}\left(\sum_{v\in\N^n}b_v\mathbf{x}^v\right),$$
    we thus conclude that $\overline{\sigma}$ is well-defined.\\
    Consider arbitrary $\sum_{v\in \N^n} a_v\mathbf{x}^v, \sum_{v\in \N^n} b_v\mathbf{x}^v\in R[\mathbf{x}]$. Then 
    \begin{align*}
        \overline{\sigma}\left(\sum_{v\in \N^n} a_v\mathbf{x}^v + \sum_{v\in \N^n}b_v\mathbf{x}^v\right) &= \sum_{v\in \N^n} \sigma(a_v+b_v)\mathbf{x}^v = \sum_{v\in \N^n} \left(\sigma(a_v)+\sigma(b_v)\right) \mathbf{x}^v\\
        &=\sum_{v\in \N^n} \sigma(a_v)\mathbf{x}^v + \sum_{v\in \N^n} \sigma(b_v)\mathbf{x}^v = \overline{\sigma}\left(\sum_{v\in\N^n} a_v\mathbf{x}^v\right)+\overline{\sigma}\left(\sum_{v\in\N^n} b_v\mathbf{x}^v\right)
    \end{align*}
    hence $\overline{\sigma}$ is additive. It also multiplicative. Indeed,
    \begin{align*}
        \overline{\sigma}\left(\left[\sum_{v\in\N^n} a_v\mathbf{x}^v\right]\left[ \sum_{v\in\N^n} b_v\mathbf{x}^v\right]\right) &= \sum_{u\in \N^n}\left[\sum_{v,w\in \N^n : v+w=u} \sigma(a_vb_w)\right] \mathbf{x}^u\\ 
        &= \sum_{u\in \N^n}\left[\sum_{v,w\in \N^n : v+w=u}\sigma(a_v)\sigma(b_w)\right] \mathbf{x}^u\\
        &=\left[\sum_{v\in\N^n} \sigma(a_v)\mathbf{x}^v\right]\left[ \sum_{v\in\N^n} \sigma(b_v)\mathbf{x}^v\right]\\
        &= \overline{\sigma}\left(\sum_{v\in\N^n} a_v\mathbf{x}^v\right)\overline{\sigma}\left(\sum_{v\in\N^n} b_v\mathbf{x}^v\right).
    \end{align*}
    Lastly, $\overline{\sigma}(1)=\sigma(1)=1$.
\end{proof}
\begin{definition}
    For a ring extension $R\supset K$ where $K$ is a field, given a ring map $\sigma : R \rightarrow K$, we call $\overline{\sigma}$ a \textit{specialization of $R$ in $K$}.
\end{definition}
\begin{lemma}
    Let rings $R,S,T$ be given and consider $\sigma\in \Hom(R,S)$, $\tau\in \Hom(S,T)$. Then $\overline{\tau\circ \sigma} = \overline{\tau} \circ \overline{\sigma}$. We also have that $\overline{\text{id}_R}=\textit{id}_{R[x_1,\dots,x_n]}$. In other words $(R,\sigma)\mapsto (R[x_1,\dots,x_n],\overline{\sigma})$ is a covariant functor for every $n\geq 1$.  
\end{lemma}
\begin{proof}
    Indeed, for $\sum_{v\in \N^n} a_v\mathbf{x}^v\in R[\mathbf{x}]$
    \begin{align*}\overline{\tau \circ \sigma}\left(\sum_{v\in \N^n} a_v\mathbf{x}^v\right) = \sum_{v\in \N^n} \tau(\sigma(a_v))\mathbf{x}^v = \overline{\tau}\left(\sum_{v\in\N^n}\sigma(a_v)\mathbf{x}^v \right) = \overline{\tau}\circ\overline{\sigma}\left(\sum_{v\in\N^n}a_v\mathbf{x}^v \right),\end{align*}
    and lastly
    $$\overline{\text{id}_R}\left(\sum_{v\in \N^n} a_v\mathbf{x}^v \right) = \sum_{v\in \N^n} \text{id}_{R}(a_v)\mathbf{x}^v=\sum_{v\in \N^n} a_v\mathbf{x}^v= \text{id}_{R[\mathbf{x}]}\left( \sum_{v\in \N^n} a_v\mathbf{x}^v\right).$$
\end{proof}
\begin{corollary}\label{RingsIsomorphicImpliesPolynomialRingsIsomorphic}
    Suppose $R\overset{\sigma}{\simeq}R$. Then $R[x_1,\dots,x_n]\overset{\overline{\sigma}}{\simeq} S[x_1,\dots,x_n]$.
\end{corollary}
\begin{proof}
    An immediate consequence of functoriality. 
\end{proof}
\begin{example}
    It is in general not true that if $R[x_1,\dots,x_n]\simeq S[x_1,\dots,x_n]$, then $R\simeq S$. {\LARGE Reference example}.
\end{example}
\subsubsection{Degree, Evaluation \& Roots}
\begin{definition}
    Let $S\supset R$ be a ring extension. We define \textit{evaluation} to be the map
    \begin{gather*}
        \ev : S^n \times R[x_1,\dots,x_n]\\
        ((s_1,\dots,s_n),f)=\left((s_1,\dots,s_n),\sum_{v\in\N^n} a_v\mathbf{x}^v\right)\mapsto \sum_{v\in \N^n} a_vs_1^{v_1}\cdots s_n^{v_n}
    \end{gather*}
    Let $s_1,\dots,s_n\in S$. We define \textit{evaluation in $(s_1,\dots,s_n)\in S^n$} as the map 
    \begin{gather*}
        \ev_{s_1,\dots,s_n} : R[x_1,\dots,x_n] \rightarrow S\\
        f = \sum_{v\in\N^n} a_v \mathbf{x}^v \mapsto \ev((s_1,\dots,s_n),f)=\sum_{v\in\N^n} a_vs_1^{v_1}\cdots s_n^{v_n} 
    \end{gather*}
    For an $f\in R[x_1,\dots,x_n]$ we define $f(s_1,\dots,s_n) := \ev_{s_1,\dots,s_n}(f)$.
\end{definition}
\begin{lemma}\label{EvaluationIsARingHomomorphism}
    Evaluation is a well-defined map. Moreover, the map $\ev_{s_1,\dots,s_n}$ is a well-defined ring homomorphism such $\ev_{s_1,\dots,s_n}(r) = r$ for every $r\in R$, therefor it is also an $R$-module homomorphism. In other words, evaluation in an element is an $R$-algebra homomorphism. 
\end{lemma}
\begin{proof}
    \textbf{The map is well-defined:} Let $(s_1,\dots,s_n):=(t_1,\dots,t_n)\in S^n$ and $f = \sum_{v\in \N^n} a_v \mathbf{x}^v$, $g = \sum_{v\in \N^n} b_v\mathbf{x}^v$ be polynomials in $R[x_1,\dots,x_n]$ such that $f=g$. Note that for a polynomial $h:=\sum_{v\in \N^n} c_v\mathbf{x}^v$ that 
    $$\ev((s_1,\dots,s_n),h)=\sum_{v\in\N^n} c_vs_1^{v_1}\cdots s_n^{v_n}= \sum_{v\in\N^n} c_vt_1^{v_1}\cdots t_n^{v_n} = \ev((t_1,\dots,t_n),h).$$
    By Theorem~\ref{PolynomialRingActsLikePolynomialRing} it follows that $a_v = b_v$ for every $v\in \N^n$. Thus in particular
    $$\ev(s_1,\dots,s_n),f) = \sum_{v\in \N^n} a_v s_1^{v_1}\cdots s_n^{v_n} = \sum_{v\in \N^n} b_v s_1^{v_1}\cdots s_n^{v_n} = \ev((s_1,\dots,s_n),g) = \ev((t_1,\dots,t_n),g),$$
    which means the evaluation map is well defined.\\
    \textbf{evaluation is a ring homomorphism:} Let $f = \sum_{v\in \N^n} a_v \mathbf{x}^v$ and $g = \sum_{v\in \N^n} b_v\mathbf{x}^v$ be polynomials in $R[x_1,\dots,x_n]$ and $s_1,\dots,s_n\in S$. The map is additive
    \begin{align*}
        \ev_{s_1,\dots,s_n}(f+g) &= \sum_{v\in\N^n} (a_v+b_v)s_1^{v_1}\cdots s_n^{v_n} = \sum_{v\in \N^n} a_vs_1^{v_1}\cdots s_n^{v_n} +\sum_{v\in \N^n}b_vs_1^{v_1}\cdots s_n^{v_n}\\ &= \ev_{s_1,\dots,s_n}(f)+\ev_{s_1,\dots,s_n}(g).
    \end{align*}
    The map is multiplicative: 
    \begin{align*}
        \ev_{s_1,\dots,s_n}(fg) &= \sum_{v,w\in \N^n} a_vb_w s_1^{v_1+w_1}\cdots s_n^{v_n+w_n} = \sum_{v,w\in \N^n} a_vs_1^{v_1}\cdots s_n^{v_n}b_vs_1^{w_1}\cdots s_n^{w_n}\\
        &= \left(\sum_{v\in \N^n} a_vs_1^{v_1}\cdots s_n^{v_n}\right)\left(\sum_{v\in \N^n} b_vs_1^{w_1}\cdots s_n^{w_n}\right) = \ev_{s_1,\dots,s_n}(f)\ev_{s_1,\dots,s_n}(g).
    \end{align*}
    \textbf{Evaluation fixes $R$:}
    Let $r\in R$. Then
    $$\ev_{s_1,\dots,s_n}(r)= \ev_{s_1,\dots,s_n}\left(r\mathbf{x}^{(0,\dots,0)}\right) = r s_1^{0}\cdots s_n^{0} = r.$$
\end{proof}
\begin{remark}
     Given a commutative $R$-algebra $S$ and elements $s_1,\dots,s_n\in S$, we note that the $R$-algebra generated by these elements, i.e. $R[s_1,\dots,s_n]$ is given as the image of $\ev_{s_1,\dots,s_n}: R[x_1,\dots,x_n]\rightarrow S$.
\end{remark}
\begin{lemma}\label{EvaluationCommutesWithTakingQuotient}
    Let $R$ be a ring and $J\subset R[y_1,\dots,y_m]$ be an ideal. Consider $\overline{f_1}:=f_1+J,\dots,\overline{f_n}:=f_n+J\in R[\mathbf{y}]/J$. Then for each $f\in R[\mathbf{x}]$
    $$\ev_{\overline{f_1},\dots,\overline{f_n}}(f) = \ev_{f_1,\dots,f_n}(f)+J$$
\end{lemma}
\begin{proof}
    This is a simple matter of using the definition addition and multiplication in the quotient ring. Indeed we can write $f=\sum_1^k a_{v_i}\mathbf{x}^{v_i}$ for suitable distinct $v_1,\dots,v_k\in \N^n$ and $a_{v_i}\in R$. Then 
    \begin{align*}
        \ev_{\overline{f_1},\dots,\overline{f_n}}(f) &= \sum_1^k a_{v_i}\left(f_1+J \right)^{{v_i}_1}\cdots \left(f_n+J \right)^{{v_i}_n} =\sum_1^k a_{v_i}\left(f_1^{{v_i}_1}+J \right)\cdots \left(f_n^{{v_i}_n}+J \right)\\ &= \left[\sum_1^k a_{v_i} f_1^{{v_i}_1}\cdots f_n^{{v_i}_n}\right] + J
        = \ev_{f_1,\dots,f_n}(f)+J
    \end{align*}
\end{proof}
\begin{proposition}\label{ClasificationOfAlgebraMaps}
    Let $S$ be a commutative $R$-algebra. Let $\sigma : R[x_1,\dots,x_n] \rightarrow S$ be an $R$-algebra homomorphism. Then $\sigma = \ev_{\sigma(x_1),\dots,\sigma(x_n)}$. Hence any element of $\Hom^{R-\text{alg}}(R[x_1,\dots,x_n],S)$ is uniquely determined by it's behavior on the variables of $R[x_1,\dots,x_n]$.  
\end{proposition}
\begin{proof}
    Let $f= \sum_{v\in \N^n} a_v\mathbf{x}^v\in R[x_1,\dots,x_n]$. Then using the multiplicativity and additivity of $\sigma$ we have that 
    $$\sigma(f)= \sigma\left(\sum_{v\in \N^n} a_v\mathbf{x}^v\right) = \sum_{v\in \N^n} \sigma(a_v) \sigma\left(x_1^{v_1}\cdots x_n^{v_n}\right) = \sum_{v\in \N^n} a_v\sigma\left(x_1^{v_1}\right)\cdots\sigma\left(x_n^{v_n}\right)= \ev_{\sigma(x_1),\dots,\sigma(x_n)}(f)$$
\end{proof}
\begin{corollary}\label{AlgebraHomomorphismFromPolynomialRingIsJustEvaluation}
    Let $S$ be a commutative $R$-algebra. Then 
    $$\Hom^{R-\text{Alg}}(R[x_1,\dots,x_n],S) = \left\{ \ev_{s_1,\dots,s_n} : (s_1,\dots,s_n)\in S^n \right\}.$$
\end{corollary}
\begin{corollary}
    Let $I\subset R[x_1,\dots,x_n]$, $J\subset R[y_1,\dots,y_n]$ be ideals. Then $\Hom^{R-\text{Alg}}(R[\mathbf{x}]/I,R[\mathbf{y}]/J)$ is equal to  
    $$\left\{ f+I\mapsto \ev_{f_1,\dots,f_n}(f)+J : f_1,\dots,f_n\in R[\mathbf{y}], \ev_{f_1,\dots,f_n}(f)=0 \text{ for every } f\in I \right\} =: F$$
\end{corollary}
\begin{proof}
    It is easy to check that $F\subset \Hom^{K-\text{Alg}}(R[\mathbf{x}]/I,R[\mathbf{y}]/J)$. Indeed, consider such a map $\sigma$ for given $f_1,\dots,f_n\in R[\mathbf{y}]$. Consider $$\sigma' : R[\mathbf{x}] \rightarrow R[\mathbf{y}]/J, f \mapsto \ev_{f_1+J,\dots,f_n+J}(f) = \ev_{f_1,\dots,f_n}(f)+J,$$
    is clearly a ring homomorphism satisfying $\sigma'(r)=r$ for $r\in R$, since it is equal to $\pi\circ\ev_{f_1,\dots,f_n}$, where $\pi: R[\mathbf{y}\rightarrow R[y]/J$ is the canonical surjection. Since $\ev_{f_1,\dots,f_n}(f)=0$ for every $f\in I$, it thus follows that $\sigma$ is a well-defined ring homomorphism satisfying $\sigma(r)=r$ for every $r\in R$ and hence an $R$-algebra homomorphism.\\
    Let $\sigma \in\Hom^{R-\text{Alg}}(R[\mathbf{x}]/I,R[\mathbf{y}]/J)$. Then $\sigma\circ \pi \in \Hom^{R-\text{Alg}}(R[\mathbf{x}],R[\mathbf{y}]/J)$, where $\pi: R[\mathbf{x}]\rightarrow R[\mathbf{x}]/I$ is the canonical surjection. Hence by the prior corollary, $\sigma \circ \pi = \ev_{f_1+J,\dots,f_n+J}$ for suitable $f_1+J,\dots,f_n+J\in R[\mathbf{y}]/J$. Let $f+I \in R[\mathbf{x}]/I$. Then by Lemma~\ref{EvaluationCommutesWithTakingQuotient}
    \begin{align*}
        \sigma(f+I)=\sigma\circ \pi (f) = \ev_{f_1+J,\dots,f_n+J}(f) = \ev_{f_1,\dots,f_n}(f) + J,
    \end{align*}
    hence $\sigma \in F$.
\end{proof}
\begin{lemma}
    Let $S\supset R$ be a ring extension and $s_1,\dots,s_n$ be algebraically independent over $R$. When $R[x_1,\dots,x_n]$ denotes the polynomial over $R$ in $n$ variables then $R[s_1,\dots,s_n]\simeq R[\mathbf{x}]$.
\end{lemma}
\begin{proof}
    $\ev_{s_1,\dots,s_n} : R[\mathbf{x}]\rightarrow R[s_1,\dots,s_n]$ defines a surjective ring homomorphism. Let $f\in R[\mathbf{x}]$. By the definition of algebraic independence 
    $$\ev_{s_1,\dots,s_n}(f)=0 \iff f = 0.$$. $\ev_{s_1,\dots,s_n}$ is therefor injective, implying $R[s_1,\dots,s_n]\simeq R[\mathbf{x}]$
\end{proof}
\begin{corollary}
    Consider the ring extension $R[x_1,\dots,x_n,y_1,\dots,y_m]\supset R$. Then the subring of $R[\mathbf{x},\mathbf{y}]$ generated by $x_1,\dots,x_n$ is isomorphic to the polynomial ring in $n$ variables. 
\end{corollary}
\begin{corollary}
    Consider the ring extension $R[x_1,\dots,x_n,y_1,\dots,y_m]\supset R$. Then $R[\mathbf{x},\mathbf{y}]=R[\mathbf{x}][\mathbf{y}]$. Furthermore $R[\mathbf{x},\mathbf{y}]\simeq R[z_1,\dots,z_n][w_1,\dots,w_m]$.
\end{corollary}
\begin{definition}
    Let $f\in R[x_1,\dots,x_n]\setminus\{0\}$, we define the \textit{degree} of $f=\sum_{v\in \N^n} a_v\mathbf{x}^v$, denoted $\deg \ f$, as the non-negative integer 
    $$\max \left\{ \vert v\vert : v\in \N^n, a_v \neq 0 \right\}.$$
\end{definition}
\begin{remark}
    For a polynomial $f = \sum_{v\in \N^n} a_v\mathbf{x}^v\in K[x_1,\dots,x_n]$ with $d:= \deg\ f$ we may write 
    $$f = \sum_{v\in \N^n : \vert v \vert \leq d} a_v\mathbf{x}^v.$$
\end{remark}
\begin{definition}
    Let $f\in R[x_1,\dots,x_n]\setminus\{0\}$, we define a \textit{leading coefficient} of $f=\sum_{v\in \N^n} a_v\mathbf{x}^v$, to be a coefficient $a_v\in R\setminus 0$, where $\vert v\vert = \deg \ f$. 
\end{definition}
\begin{lemma}\label{PolynomialRingOverDomainIsDomain}
    If $R$ is an integral domain, then $R[x_1,\dots,x_n]$ is an integral domain.
\end{lemma}
\begin{proof}
    We proceed by induction in $n$. Suppose $n=1$ and let $f,g\in R[x]\setminus 0$. Then $f=\sum_0^d a_ix^i$, $g=\sum_0^{d'}b_ix^i$, for $d,d'\geq 0$, $a_d\neq0\neq b_{d'}$. Then 
    $$fg = \sum_{k=0}^{d+d'} \left(\sum_{0\leq i\leq d, 0\leq j\leq d' : i+j=k} a_ib_j\right) x^k.$$
    Note that for $i\leq d$ and $j\leq d'$, $i+j = k$ if and only if $i=d$ and $j=d'$, hence
    $\sum_{0\leq i\leq d, 0\leq j\leq d' : i+j=k} a_ib_j = a_db_{d'} \neq 0$, using that $R$ is a domain. Since $\{x^i : i\in\N\}$ is linearly independent over $R$ it follows that $fg \neq 0$.
    Suppose $R[x_1,\dots, x_n]$ is a domain for some $n\geq 1$. Then by the one variable case $R[x_1,\dots,x_{n+1}]\simeq (R[x_1,\dots,x_n])[x_{n+1}]$ is a domain. 
\end{proof}

\begin{lemma}\label{PropertiesOfDegreeFunction}
     The function 
    \begin{gather*}
        \deg : R[x_1,\dots,x_n]\setminus 0 \rightarrow \N\\
        f\mapsto \deg \ f
    \end{gather*}
    has the following properties 
    \begin{enumerate}
        \item The degree function is sub-additive for pairs of distinct polynomials, i.e. $\deg \ f + g \leq \max(\deg\ f, \deg \ g)$ for every $f,g\in R[\mathbf{x}]\setminus 0$ with $f\neq g$.
        \item For every $f,g\in R[\mathbf{x}]\setminus 0$, $\deg \ f >\deg \ g\implies \deg \ f+g = \deg \ f$.
        \item The degree function is sub-multiplicative, i.e. $\deg \ fg \leq \deg \ f+ \deg \ g$ for every $f,g\in R[\mathbf{x}]\setminus 0$
        \item Suppose $R$ is an integral domain. Then $\deg \ fg = \deg \ f+ \deg \ g$ for every $f,g\in R[\mathbf{x}]\setminus 0$.
    \end{enumerate}
\end{lemma}
\begin{proof}
Put $d = \deg \ f$ and $d' = \deg \ g$, and write $f = \sum_{v\in \N^n : \vert v\vert \leq d} a_v \mathbf{x}^v$, $g= \sum_{v\in \N^n : \vert v \vert \leq d'} b_v \mathbf{x}^v$
    1.  Let $v\in \N^n$ such that $\vert v\vert > \max(d,d')$. Then in particular $\vert v\vert > d$ and $\vert v\vert > d'$, meaning $a_v = 0$ and $b_v = 0$, hence $a_v+b_v =0$. This means 
    $$\max\{\vert v \vert : v\in \N^n , a_v+b_v = 0\} \leq \max(d,d')$$\\
    2. From 1. we have that $\deg \ f +g \leq \max(d,d') = d$, hence it suffices to show that $a_v+b_v \neq 0$ for some $v\in \N^n$ with $\vert v \vert = d$. There exists a $v\in \N^n$ with $\vert v\vert = d$ and $a_v \neq 0$. Since $\vert v \vert = d > d'$, $b_v = 0$ hence $a_v+b_v = a_v \neq 0$.\\
    3. Let $u\in \N^n$ be given such that $\vert u\vert > d+d'$. Consider $v\in \N^n$ and $w\in \N^n$ with $v+w = u$. Then $\vert v\vert + \vert w\vert = \vert u \vert >d+d'$, hence $\vert v\vert > d$ or $\vert w\vert > d'$, implying $a_v = 0$ or $b_w = 0$, hence $\sum_{v,w\in \N^n: v+w = u} a_vb_w = 0$, implying 
    $$\deg \ fg = \max\left\{\vert u\vert : \sum_{v,w\in \N^n : v+w = u} a_vb_w\neq 0 \right\} \leq d+d' = \deg \ f + \deg \ g.$$\\
    4. Let $f' = \sum_{v\in \N^n : \vert v \vert = d} a_v\mathbf{x}^v$ and $g' = \sum_{w\in \N^n : \vert w \vert = d'}b_w\mathbf{x}^w$. For some $v,w\in\N^n$ with $\vert v \vert = d$ and $\vert w\vert =d'$ we have that $a_v\neq 0$ and $b_w \neq 0$, hence $f' \neq 0$ and $g'\neq 0$ implying that $f'g'\neq 0$ by Lemma~\ref{PolynomialRingOverDomainIsDomain}. Furthermore $\deg \ f'g' = d+d'$. Let $r_f =  \sum_{v\in \N^n : \vert v \vert < d} a_v\mathbf{x}^v$ and $r_g = \sum_{w\in \N^n : \vert w \vert < d'}b_w\mathbf{x}^w$. Note that $\deg \ r_f < d$ and $\deg \ r_g < d'$, hence by 3. 
    \begin{gather*}
        \deg \ f'r_g \leq d + \deg\ r_g <d+d',\\
        \deg \ g'r_f \leq d' + \deg\ r_f <d+d',\\
        \deg \ r_fr_g \leq \deg \ r_f + \deg \ r_g < d+d'.
    \end{gather*}
    We thus get that 
    $$\deg \  f'r_g + g'r_f+ r_fr_g \leq \max(f'r_g,g'r_f,r_f,r_g) < d+d' = \deg \ f'g'.$$
    By 2. we get 
    $$\deg \ fg = \deg \ (f'+r_f)(g'+r_g) = \deg \ f'g'+( f'r_g + g'r_f+ r_fr_g) = \deg \ f'g' = d+d'=\deg\  f + \deg \ g.$$
\end{proof}
\begin{definition}
    Let $S\supset R$ be a commutative ring extension. Let $f\in R[x_1,\dots,x_n]$, we say that $(s_1,\dots, s_n)\in S^n$ is a \textit{zero (over $S$)} of $f$ if $f(s_1,\dots,s_n)= 0$. If $f\in R[x]$ and $s\in S$ is a zero of $f$ we call it a \textit{root (in $S$)}.\\  
\end{definition}
\begin{definition}
    Let $S\supset R$ be a commutative ring extension. Given a polynomial $f\in R[x_1,\dots,x_n]$, we denote the set of zeroes over $S$ of $f$ by 
    $$V_S(f).$$
\end{definition}
The above definitions are central to the classical treatment of algebraic geometry, since the geometric objects considered are build from set zeroes of polynomials over a field $K$. 
\begin{proposition}\label{AZeroIffPolynomialIsInPointIdeal}
    Let $S$ be an $R$-algebra. Let $f\in R[x_1,\dots,x_n]\subset S[x_1,\dots,x_n]$ and $(s_1,\dots,s_n)\in S^n $, set $I :=\langle x_1-s_1,\dots,x_n-s_n\rangle\subset S[x_1,\dots,x_n] $. Then $(s_1,\dots,s_n)$ is a zero of $f$ if and only if $f\in I$. We call $I$ the \textbf{point ideal of $(s_1,\dots,s_n)$}.
\end{proposition}
\begin{proof}
    Write $f = \sum_{v\in \N^n} a_v \mathbf{x}^v$.
    "$\implies$": Suppose $(s_1,\dots,s_n)$ is a zero of $f$. Then, since $x_i + I = s_i + I$ for each $i$, 
    \begin{align*} 
        f + I &= \left(\sum_{v\in \N^n} a_v \mathbf{x}^v\right) + I = \sum_{v\in \N^n} a_v(x_1+I)^{v_1}\cdots(x_n+I)^{v_n} = \sum_{v\in \N^n} a_v(s_1+I)^{v_1}\cdots(s_n+I)^{v_n}  \\ 
        &= \left(\sum_{v\in \N^n} a_v s_1^{v_1}\cdots s_n^{v_n}\right) +I = f(s_1,\dots,s_n) + I = 0 + I,
    \end{align*}
    thus $f \in I$\\
    "$\impliedby$": Suppose $f\in I$. Then there are $\lambda_1,\dots,\lambda_n\in S[x_1,\dots,x_n]$ such that $f = \sum_1^n \lambda_i\cdot(x_i-s_i)$. It then follows that 
    $$f(s_1,\dots,s_n) = \sum_1^n \lambda_i(s_1,\dots,s_n)(s_i-s_i) = 0,$$
    hence $(s_1,\dots, s_n)$ is a zero of $f$.
\end{proof}
\begin{corollary}\label{MaximalIdealsOfPolynomialRings}
    Let $(r_1,\dots,r_n)\in R^n$. If $\ev_{r_1,\dots,r_n}$ is surjective, then $R[x_1,\dots,x_n]/\langle x_1-r_1,\dots,x_n-r_n\rangle \simeq R$. Hence if $R$ is a field, then $\langle x_1-r_1,\dots,x_n-r_n\rangle$ is maximal. 
\end{corollary}
\begin{corollary}\label{RootIffMinimalPolynomialDivides}
    Let $S\supset R$ be a commutative ring extension and consider $f\in R[x]$ and $a \in S$. Then $a$ is a root of $f$ if and only if $x-a\mid f$ in $S[x]$. 
\end{corollary}
The following theorem is useful when one wants to eliminate certain variables in a finitely generated $R$-algebra.  
\begin{corollary}\label{EliminationOfVoidVariablesInFinitelyGeneratedAlgebra}
    Let $f_1,\dots,f_m,g_1,\dots,g_l\in R[x_1,\dots,x_n]$, where $R$ is a commutative ring. Set $I:=\langle g_1,\dots,g_l\rangle \subset R[\mathbf{x}]$ and $J:= \langle g_1,\dots,g_l,y_1-f_1,\dots,y_m-f_m\rangle\subset R[\mathbf{x},y_1,\dots,y_m]$. Then $R[\mathbf{x},\mathbf{y}]/J \simeq R[\mathbf{x}]/I$ 
\end{corollary}
\begin{proof}
    Consider the surjective ring homomorphism
    \begin{gather*}
        \sigma := \ev_{\mathbf{x},f_1,\dots,f_m} : R[\mathbf{x},\mathbf{y}]\rightarrow R[\mathbf{x}]/I\\
        h \mapsto h(\mathbf{x},f_1,\dots,f_m) + I 
    \end{gather*}
    Clearly $J\subset \ker \ \sigma$. Let $h \in \ker\ \sigma$. Then $h(x_1,\dots,x_n,f_1,\dots,f_m)\in I$ and hence is also an element of $J$. It follows that
    $$h + J = h(\mathbf{x},f_1,\dots,f_m) + J = 0 + J\implies h\in J.$$
    We thus see by the isomorphism theorem that 
    $$R[\mathbf{x},\mathbf{y}]/J \simeq R[\mathbf{x}]/I$$
\end{proof}
\begin{lemma}\label{RootsProductIsUnionOfRootsOfFactors}
    Let $S\supset R$ be an integral domain extension. Let $f,g\in R[x_1,\dots,x_n]$ and $v\in S^n$. Then $v$ is a zero of $fg$ if and only if $v$ is a zero of $f$ or $g$. In other words $V_S(fg)=V_S(f)\cup V_S(g)$.
\end{lemma}
\begin{proof}
    Since $R$ is an integral domain,
    $$0 = (fg)(v) = f(v)g(v) \iff f(v) = 0 \text{ or } g(v) = 0.$$
\end{proof}
\begin{proposition}\label{InIDNumberOfRootsBoundedByDegree}
    Let $R$ be an integral domain. Consider $f \in R[x]\setminus\{0\}$ with $d := \deg \ f$. Then there at most $d$ roots of $f$ in $R$.  
\end{proposition}
\begin{proof}
    We proceed by induction in $d$. Let $d = 1$. If $f$ has no roots we are done. Suppose it does have a root $a\in R$. Then Corollary~\ref{RootIffMinimalPolynomialDivides} tells us that $f = q(x-a)$, for some $q \in R[x]$. Since $f \neq 0$, we have that $q \neq 0$. By Lemma~\ref{PropertiesOfDegreeFunction} 4. it follows that
    $$1 = \deg \ f = \deg\ q(x-a) =\deg \ q + \deg x-a = q +1 \implies \deg \ q = 0,$$
    hence $q$ is a non-zero constant. It follows from \ref{RootsProductIsUnionOfRootsOfFactors} that $f$ has exactly $1$ root.\\
    Now consider a polynomial $f\in R[x]$ of degree $d+1$ for some $d\geq 1$. If $f$ has no roots, we are done. Suppose then that $f$ has a root $a\in R$. Then by Corollary~\ref{RootIffMinimalPolynomialDivides} $(x-a)\mid f$, hence $f = g\cdot(x-a)$ for some $g\in R[x]$, again since $f\neq 0$, we have that $g\neq 0$, by Lemma~\ref{PropertiesOfDegreeFunction} 4. it follows that 
    $$d+1 = \deg \ f = \deg \ g + \deg \ x-a = \deg \ g + 1 \implies \deg \ g = d$$
    it follows by induction hypothesis that $g$ has at most $d$ roots. By Lemma~\ref{RootsProductIsUnionOfRootsOfFactors} $V(f)=V(g)\cup V(x-a)$, hence $\#V(f) = \#\left(V(g)\cup V(x-a)\right)\leq \#V(g)+\#V(x-a)  \leq d + 1 $ 
\end{proof}
\begin{lemma}\label{WeightedBoundHasLargerDegreeThenEvaluation}
    Let $R$ be an integral domain. Consider $f\in R[x_1,\dots,x_n]\setminus 0$ and $f_1,\dots,f_n\in R[y_1,\dots,y_m]\setminus 0$ with $d_i:=\deg\ f_i$. Then 
    $$\deg \ f(f_1,\dots,f_n)\leq \deg\ f(x_1^{d_1},\dots,x_n^{d_n})$$
\end{lemma}
\begin{proof}
    Write $f= \sum_{v\in \N^n} a_v\mathbf{x}^v$. Let $v\in \N^n$ such that $a_v\neq 0$ and set $M_v = \mathbf{x}^v$. Then 
    \begin{align*} 
        \deg \ M_v(x_1^{d_1},\dots,x_n^{d_n}) = \sum_1^n v_id_i = \sum_1^n \deg \ f_i^{v_i} = \deg f_1^{v_1}\cdots{f_n}^{v_n} = \deg \ M_v(f_1,\dots,f_n).
    \end{align*}
    Note that the map $(v_1,\dots,v_n)\mapsto (v_1d_1,\dots,v_nd_n)$ is injective hence,
    $$\deg \ f(f_1,\dots,f_n)\leq \max_{v\in \N^n : a_v\neq 0}\ \deg \ M_v(f_1,\dots,f_n) = \max_{v\in \N^n :a_v\neq 0}\ \deg\ M_v(x_1^{d_1},\dots,x_n^{d_n}) = \deg \ f(x_1^{d_1},\dots,x_n^{d_n}).$$
\end{proof}
\begin{remark}
    The above result doesn't always hold with equality. take for instance $f=x_1x_2-x_3$, take $f_1=y_1$, $f_2=y_2$ and $f_3=-y_1y_3$. Then $f(f_1,f_2,f_3)=0$, while $f(x_1,x_2,x_3^2)=x_1x_2-x_3^2$.
\end{remark}
\subsubsection{Some Results about Polynomials that I proper subsubsections for}
\begin{lemma}\label{DimensionOfQuotientSpace}
    Let $f=\sum_0^d a_ix^i \in K[x]\setminus0$ and set $I=\langle f\rangle$. Then $K[x]/I$ is a vector space of dimension $d$ with basis $\left\{x^i+I : i\in\{0,\dots,d-1\}\right\}$.
\end{lemma}
\begin{proof}
    One finds that 
    $$0 = \left[\sum_0^d a_ix^i\right] + I  \implies x^d = -\sum_0^{d-1} \left(a_d^{-1}a_ix^i+I\right),$$
    so $\left\{x^i+I : i\in\{0,\dots,d-1\}\right\}$ generates $K[x]/I$ over $K$. Suppose $g+I=\left[\sum_0^{d-1} b_ix^i\right] + I=0$. Then $g\in I$. This means either $g=0$ or $\deg \ g \geq d$, hence $g=0$ and $a_i = 0$ for $i\in\{1,\dots,d-1\}$. So $\left\{x^i+I : i\in\{0,\dots,d-1\}\right\}$ is linearly independent over $K$ and is thus a basis for $K[x]/I$, which means $\dim_K \ K[x]/I = d$.
\end{proof}
\subsubsection{Polynomials over Infinite Rings}
\begin{proposition}\label{OnInfiniteIDPolynomialVanishesEverywhereIfAndOnlyIfIsZero}
    Let $R$ be an infinite integral domain an $f\in R[x_1,\dots,x_n]$. Then $f = 0$ if and only if $f(v)=0$ for every $v\in K^n$. 
\end{proposition}
\begin{proof}
    "$\implies$": This is trivial
    "$\limplies$":We prove that if $f \neq 0$, then there is a $v\in R^n$ such that $f(v)\neq 0$. We prove this by induction in $n$.\\
    \textit{Base case:} Consider first the case $n = 1$. Since $f \neq 0$, the number of roots is bounded by the non-negative integer $\deg\ f$ by Proposition~\ref{InIDNumberOfRootsBoundedByDegree}. Then since $\#R=\infty$, there is an $a\in R$ such that $f(a)\neq 0$.\\ 
    \textit{Induction hypothesis:} Suppose that there is an $n \geq 1$ s.t. if $h\in R[x_1,\dots,x_n]\setminus 0 $, then there is a $v \in R^n$ such that $f(v)\neq 0$.\\
    \textit{Induction Step:} Let $f\in R[x_1,\dots,x_{n+1}]\setminus 0$. We can write 
    $$f = \sum_0^d f_ix_{n+1}^i,$$
    for some $d\geq 0$ and suitable $f_0,\dots,f_d\in R[x_1,\dots,x_n]$ where $f_j\neq 0$ for some $j\in \{0,\dots,d\}$. By the induction hypothesis, there is a $(v_1,\dots,v_n)\in R^n$ such that $f_j(v)\neq 0$. Then 
    $$R[x_{n+1}]\ni f':= f(v_1,\dots,v_n,x_{n+1}) = \sum_0^d f_i(v_1,\dots,v_n)x_{n+1}^i \neq 0.$$ 
    By the base case there is a $v_{n+1}\in R$ such that $f'(v_{n+1})\neq 0$. Hence upon putting $v= (v_1,\dots,v_n,v_{n+1})$ we get that 
    $$f(v) = f'(v_{n+1}) \neq 0.$$
\end{proof}
\subsubsection{The Hilbert Basis Theorem}
\begin{theorem}(Hilbert Basis Theorem)\label{EveryPolynomialIdealIsFinitelyGenerated}
    Let $R$ be a left/right noetherian ring. Then $R[x]$ is left/right noetherian. Furthermore $R[x_1,\dots,x_n]$ is left/right noetherian. 
\end{theorem}
\begin{proof}
    We prove the contrapositive. Suppose That $R[x]$ is not noetherian, or equivalently by Theorem~\ref{NoetherianIffEverySubmoduleFinitelyGenerated} suppose there is an ideal $I\subset R[x]$ that is not finitely generated. Let $d_1 = \min\{\deg \ f : f \in I\}$. Let $f_1 \in I$ such that $\deg \ f_1 = d_1$. We then let $I_1 = R[x]f_1$ and recursively define $I_n = \sum_1^n R[x]f_i$, where $f_n \in I\setminus I_{n-1}$ where $\deg \ f_n = d_n = \min\{ \deg \ f: f \in I \setminus I_{n-1}\}$. Note that since $I\setminus I_n \supset I\setminus I_{n+1}$, $d_n\leq d_{n+1}$ for each $n\geq 1$. For each $n$ we can write 
    $$f_n = \sum_{i=0}^{d_n} a_i^{(n)}x^i,$$
    for suitable $a_i^{(n)}\in R$. Set $a(n) = a_{d_n}^{(n)}$
    We then have an ascending chain $J_1\subset J_2\subset\dots$ in $R$ where $J_n =\sum_1^n Ra(i)$.
    Suppose for a contradiction that $J_n = J_{n+1}$ for some $n\geq 1$. Then 
    $$a(n+1) = \sum_1^n b_ia(i),$$
    for suitable $b_1,\dots,b_n \in R.$ Put 
    $$g = f_{n+1} - \underbrace{\sum_{i=1}^n\alpha_i x^{d_{n+1}-d_i}f_i}_{h}.$$
    Then $g \in I$, $h\in I_n$ and $f_{n+1} = g + h\in I\setminus I_n$, thus $g \in I \setminus I_n$. However, upon further inspection, we find
    \begin{align*}
        g &= a(n+1)x^{d_{n+1}} -\sum_{i=1}^n\alpha_ix^{d_{n+1}-d_i}\sum_{j=1}^{d_i} a_j^{(i)} x^j + \underbrace{\sum_{i=1}^{d_{n+1}-1} a_i^{(n+1)}x^i}_{r}\\ 
        &= a(n+1)x^{d_{n+1}}-\sum_{i=1}^n\alpha_ia(i) x^{d_{n+1}-d_i}x^{d_i}-\underbrace{\sum_{i=1}^n\sum_{j=1}^{d_i-1} a_j^{(i)}x^i}_{r'} + r \\
        &= a(n+1)x^{d_{n+1}}-\left(\sum_{i=1}^n\alpha_ia(i) \right)x^{d_{n+1}}+r'+r =a(n+1)x^{d_{n+1}}-a(n+1)x^{d_{n+1}} +r'+r\\ &= \sum_{i=1}^{n+1}\sum_{j=1}^{d_i-1}a_j^{(i)}x^{i}.
    \end{align*}
    Thus $\deg \ g = \max\{d_1-1,\dots,d_{n+1}-1\} = d_{n+1}-1< d_{n+1} =\min\{ \deg \ f : f \in I\setminus I_n\}$, leading to a contradiction. This means that $J_1\subset J_2\subset \dots$ is a non-stabilizing ascending chain hence $R$ is not noetherian.\\
    Suppose that $R$ is noetherian. Then by induction $R[x_1,\dots,x_n]\simeq \left(R[x_1,\dots,x_{n-1}]\right)[x_n]$ is noetherian.
\end{proof}
\begin{corollary}\label{PolynomialIdealOverFieldIsFinitelyGenerated}
    Let $K$ be a field. Then $K[x_1,\dots,x_n]$ is noetherian.
\end{corollary}
\subsubsection{Polynomials over Fields}
\begin{definition}
    A field $K$ is called algebraically closed if every non-constant $f\in K[x]\setminus 0$ has a root $a\in K$. 
\end{definition}
\begin{lemma}
    Let $K$ be a field. Then $K[x]$ is a PID. 
\end{lemma}
\begin{proof}
    The trivial ideals are trivially principal. So consider a non-zero proper ideal $I\subset K[x]$. Let $d:= \min \{\deg \ f: f\in I\} \geq 1$. Pick an $f\in I$ of degree $d$. Let $g\in I$. Then there is a $q,r\in K[x]$ where $r = 0$ or $\deg \ r <\deg \ f$ such that $g=qf+r$. By minimality $r=0$, hence $f\mid g$, hence $I=\langle f\rangle$ 
\end{proof}
\begin{lemma}\label{OverFieldQoutientRingWithRespectToIrreduciblePolynomialIsAField}
    Let $K$ be a field and $f=\sum_0^d a_ix^i\in K[x]$ an irreducible polynomial. Set $I:=\langle f\rangle$. Then $F:=K[x]/I$ is a field and $x+I$ is a root of $g:= \sum_0^d a_iy^i\in F[y]$.
\end{lemma}
\begin{proof}
    Lemma~\ref{InPIDPrimeIdealsAreMaximal} and Lemma~\ref{InPIDIrreduciblesArePrimes} shows that $I$ is maximal. Then $K[x]/I$ is a field by Proposition~\ref{InCommutativeRingMaximalIdealIffQuotientRingField}. Secondly, 
    $$g(x+I)=\sum_0^d a_i(x+I)=\left(\sum_0^d a_ix\right)+I=0+I.$$
\end{proof}
\subsubsection{More on Power Series}
\begin{lemma}
    Let $R$ be any commutative ring. Then $$R\llbracket x_1,\dots,x_n,y_1,\dots,y_m \rrbracket\simeq R\llbracket x_1,\dots,x_n\rrbracket\llbracket y_1,\dots,y_m\rrbracket := (R\llbracket x_1,\dots,x_n\rrbracket)\llbracket y_1,\dots,y_m\rrbracket.$$
\end{lemma}
\begin{proof}
    Consider the map $\sigma : R\llbracket \mathbf{x},\mathbf{y}\rrbracket \rightarrow R\llbracket \mathbf{x}\rrbracket\llbracket \mathbf{y}\rrbracket, \sum_{v\in \N^n,w\in \N^m} a_v\mathbf{x}^v\mathbf{y}^w\mapsto \sum_{w\in \N^n}\left(\sum_{v\in \N^n} a_v\mathbf{x}^v\right)\mathbf{y}^w.$
    Note that $\sigma$ is actually just currying of functions $\N^m\times \N^n \rightarrow R$. This is trivially a ring homomorphism. The inverse is given by uncurrying. 
\end{proof}
\begin{lemma}
    If $R$ is an integral domain, so is $R\llbracket x_1,\dots,x_n \rrbracket$. 
\end{lemma}
\begin{proof}
    In the case $n=1$. Consider $f=\sum_{i\in \N} a_ix^i ,g=\sum_{i\in\N} b_ix^i \in R\llbracket x\rrbracket$. Let $k,l\geq 0$ be the smallest integers such that $a_i\neq 0$, $b_i\neq 0$. Consider $i,j\in \N$ such that $i+j=k+l$. If $i>k$ then $j<l$ and vice versa, hence  
    $$\sum_{i,j\in \N : i+j=k+l} a_ib_j = a_kb_l\neq 0.$$
    By the prior lemma $R\llbracket x_1,\dots,x_{n+1}\rrbracket \simeq R\llbracket x_1,\dots,x_n\rrbracket\llbracket x_{n+1}\rrbracket$, which by induction and the case $n=1$ is an integral domain. 
\end{proof}
\subsubsection{Formal Power Series \& DVRs}
\begin{lemma}
    Suppose $R$ is an integral domain. Then $$R\llbracket x \rrbracket ^\ast=  \left\{\sum_{i\in \N} a_i\in R\llbracket x\rrbracket : a_i \in R^\ast \right\}$$
\end{lemma}
\begin{proof}
    Let $s = \sum_{i\in\N}a_ix^i$  be an element of the right-hand side. Set $b_0 := a_0^{-1}$ and $b_k := -a_0^{-1}\sum_{j=1}^{k} a_jb_{k-j}$ for $k\geq 1$. Define $t =  \sum_{i\in \N^n} b_ix^i$. We prove by induction that $\sum_{j,k\in\N : j+k=i} a_jb_k=0$ for $i\geq 1$. For $i=1$ we have that 
    $$\sum_{j,k\in\N : j+k=1} a_jb_k=0 = a_0b_1+a_1b_0 = -a_0a_0^{-1}\sum_{h=1}^1 a_hb_{1-h}+a_1a_0^{-1} = -a_1a_0^{-1}+a_1a_0^{-1}=0.$$
    Then for $i\geq 0$,
    $$\sum_{j,k\in\N : j+k=i+1}a_jb_k = \sum_{j,k\in\N : j+k=i+1}-a_ja_0^{-1}\sum_{h=1}^ka_hb_{k-h}$$
\end{proof}
\begin{lemma}
    For an integral domain $R$, $x\in R\llbracket x \rrbracket$ is irreducible.  
\end{lemma}
\begin{proof}
    $x$ is a non-zero, non-unit. Suppose $x=ab$ for $a,b\in R\llbracket x\rrbracket$. Then $a_0b_0=0$, hence $a_0=0$ or $b_0 =0$. Furthermore, $a_0b_1+a_1b_0 = 1$, hence $a_0b_1=1$ or $a_1b_0 = 1$, hence $a_0\in R^\ast$ or $b_0\in R^\ast$, hence either $a$ or $b$ is a unit in $R\llbracket x\rrbracket$. We thus have that $x$ is irreducible in $R\llbracket x\rrbracket$    
\end{proof}
\begin{proposition}
    The ring of power series $K\llbracket x\rrbracket$ is a DVR with uniformizing parameter $x$ when $K$ is a field.
\end{proposition}
\begin{proof}
    $x$ is irreducible by the above lemma. Let $t\in K\llbracket x\rrbracket$. Put $n := \max (\{k\geq 1: x^k\mid t\}).$ Note for $h\in K\llbracket x\rrbracket$, $x\mid h $ if and only if $h$ is not a unit, hence $t = sx^n$, where $s$ a unit. uniqueness of this representation follows from the maximality of $n$ and the irreducibility of $x$. It thus follows that $K\llbracket x\rrbracket$ is a DVR with uniformizing parameter $x$ by Proposition~\ref{EquivalentDefinitionOfDVR}.
\end{proof}
\begin{definition}
    For an integral domain $R$, we take $R\llparenthesis x_1,\dots,x_n\rrparenthesis$ to mean $Q(R\llbracket x_1,\dots,x_n\rrbracket)$. 
\end{definition}
\begin{proposition}
    Consider the setup and statement of Proposition~\ref{PropositionTowardsPowerSeriesDescriptionOfCerationDVRs}. Then to each $z\in R$ there is a unique (possibly infinite) sequence $(\lambda_i)\in \prod_{i\in \N} K$. In other words the map 
    \begin{gather*}\sigma: R\rightarrow K\llbracket x\rrbracket\\
    z\mapsto \sum_{i\in\N} \lambda_ix^i\end{gather*}
    is a well-defined map.  It is furthermore an injective ring $K$-algebra homomorphism. It extends to a homomorphism of $L=Q(R)$ onto $K\llparenthesis x\rrparenthesis.$ 
\end{proposition}
\begin{proof}
    clearly map fixes $K$. Let $z,w\in R$ be given with associated power series $\sum_{i\in \N}\lambda_ix^i$ resp. $\sum_{i\in\N} \mu_ix^i$. 
    Then for any $n\geq 0$, $z=\sum_0^n \lambda_it^i + z_nt^{n+1}$, $w = \sum_0^n \mu_it^i+w_nt^{n+1}$ for suitable unique $z_{n+1},w_{n+1}\in R$, hence \begin{align*}z+w &= \sum_0^n (\lambda_i+\mu_i)t^i + (w_n+z_n)\implies \sigma(z+w) = \sum_{i\in\N} (\lambda_i+\mu_i)x^i\\ &= \sum_{i\in\N^n} \lambda_i x^i +\sum_{i\in\N^n} \mu_ix^i=\sigma(z)+\sigma(w).\end{align*} 
    Let $n\geq 0$. Then 
    $$zw = \sum_0^{2n} \left(\sum_{i,j\in \N : i+j=h} \lambda_i\mu_j\right)t^h + \underbrace{\sum_0^n (\lambda_iw_k+\mu_iz_k)t^{i+n+1} + w_nz_nt^{n+2}}_{r}.$$
    Since $\ord(r)\geq n+1$, it follows that the $n$'th coefficient of the formal power series of $zw$ is equal to $\sum_{i+j=n}\lambda_i\mu_i$, hence $$\sigma(zw)=\sum_{h\in\N} \left(\sum_{i,j\in\N : i+j=h}\lambda_i\mu_j\right)x^h = \sigma(z)\sigma(w).$$
    Injectivity follows from the uniqueness of the coefficients in the power series. Hence $\ker\sigma = 0,$ hence Lemma~\ref{ExtendingRingHomomorphismToLocalization} implies that $$\overline{\sigma} : L\rightarrow K\llparenthesis x\rrparenthesis, \frac{z}{w}\mapsto \frac{\sigma(z)}{\sigma(w)}$$
    is the unique extension of $\sigma$ to a $K$-algebra homomorphism between the fraction fields of $R$ and $K\llbracket x\rrbracket$.
\end{proof}
\begin{remark}
    The unique formal power series $\sum_{i\in \N} \lambda_i\in K\llbracket x\rrbracket$ associated with $z\in R$ is called \textit{the power series expansion of $z$ in terms of $t$}, 
\end{remark}

\subsubsection{Term Orders \& a Polynomial Division Algorithms}

\begin{definition}
    A \textit{term order} is total order $\leq$ on $\N^n$ such that 
    \begin{align*}
        &1. \quad 0\leq v \text{ for every } v\in \N^n,\\
        &2. \quad \text{for every } v_1,v_2,v\in \N^n, v_1\leq v_2\implies v_1+v\leq v_2+v.
    \end{align*}
\end{definition}

\begin{example}
    \begin{enumerate}
        \item A simple example of a term order is $\leq$ on $\N$. 
        \item \textit{The lexicographic term order}, denoted $\lex$, on $\N^n$ for $v=(v_1,\dots,v_n),w=(w_1,\dots,w_n)\in \N^n$ is defined by $v\lex w$ if $v=w$ or there is an $i\in \{1,\dots,n\}$ such that $v_j = w_j$ for $j<i$ and $v_i < w_i$. For example $\left(2,10^6,10^{10^6}\right)\lex (3,1,1)$ since $2<3$. This is indeed a term order: We first check that it is a total order. By definition it is reflexive. Let $v,w,u\in \N^n$.\\
        Note that if $v \neq w$, then there is a minimal $i$ such that $v_i \neq w_i$, hence either $v <_\text{lex} w$ or $w <_\text{lex} v$. Hence in general $v\lex w$ or $w\lex v$.\\ 
        If there is an $i$ such $v_i < w_i$ and $v_j=w_j$ for $j<i$ then $v\neq w$. Hence if $v\lex w$ and $w\lex v$, then necessarily $v=w$.\\
        Suppose $v\lex w$ and $w\lex u$. We check by cases that $v \lex u$.\\
        Case 1: Suppose first $v = w$ and $w = u$. Then $v=u$, implying $v\lex u$.\\
        Case 2: Suppose $v=w$ and that there is an $i$ such that $w_i<u_i$ and $w_j=u_j$ for every $j<i$. Then $v_i = w_i < u_i$ and $v_j = w_j = u_j$ for $j<i$ hence $v \lex u$.\\
        Case 3: Suppose there are $h,i\in \{1,\dots,n\}$ such that $v_h < w_h$, $w_i <u_i$ and $v_j=w_j$, $w_k = u_k$ for $h<j$, $i<k$. If $h\leq i$, then $v_h < w_h \leq u_h$ and $v_j = w_j = u_j$ for $j<h$. If $i< h$, then $v_i = w_i < u_i $ and $v_j=w_j=u_j$ for $j<i$. In any case $v\lex u$.\\
        Case 4: Suppose there is an $i$ such that $v_i<w_i$ and $v_j=w_j$ for every $j<i$ and $w = u$. Then $v_i < w_i = u_i$ and $v_j=w_j = u_j$ for every $j<i$, hence $v\lex u$.\\
        In conclusion $\lex$ is a total order. Note that $0 \leq v_i$ for every $i\in \{1,\dots,n\}$. Hence either $0=v_i$ for every $i$ or there is an $i$ such that $0< v_i$, meaning $0\lex v$. Suppose $v\lex w$. If $v=w$ then, $v+u = w+ u$, hence $v+u\lex w+u$. Suppose there is an $i$ such that $v_i<w_i$ and $v_j = w_j$ for each $j<i$. Then $v_i+u_i < w_i + u_i$ and $v_j+u_j = w_j + u_j$ for every $j<i$, which implies $v + u \lex w + u$. 
    \end{enumerate} 
\end{example}
For $v\in \N^n$ define 
$$v + \N^n = \left\{ v +w  : w\in \N^n\right\}.$$
\begin{theorem}\label{Dickson's lemma}(Dickson's Lemma)\\
    Let $S\subset \N^n$ be non-empty. Then there are vectors $v_1,\dots,v_m\in S$ such that 
    $$S\subset \bigcup_1^m \left(v_i +\N^n\right)$$
\end{theorem}
\begin{proof}
    We proceed by induction in $n$. For $n=1$, $S$ has a minimal element $s$ by the well ordering of the natural numbers, hence any element of $S$ can be written as $s+t$ for some $t\in \N$. Suppose Dickson's lemma is true for some $n\geq 1$. Let $S$ be some non-empty subset of $\N^{n+1}$. Consider the canononical surjection 
    \begin{gather*}
        \pi : \N^{n+1}\rightarrow \N^n\\
        (v_1,\dots,v_n,v_{n+1})\mapsto (v_1,\dots,v_n)
    \end{gather*}
    Consider the set 
    $$S' := \pi(S) = \left\{(x_1,\dots,x_n)\in \N^n : (x_1,\dots,x_n,x_{n+1})\in S \text{ for some }x_{n+1}\in \N \right\}.$$
    By induction there are $s_1=(s_{11},\dots,s_{1,n+1}),\dots,s_m=
    (s_{m1},\dots,s_{m,n+1})\in S$ such that upon defining $s_i' := 
    (s_{i1},\dots,s_{in})\in S'$ 
    $$S' \subset \bigcup_1^m \left(s_i'+\N^n\right).$$
    Let $s_\text{max} = \max_{i\in \{1,\dots,m\}} s_{i,(n+1)}$.
    Define 
    $$S_i :=\left\{v=(v_1,\dots,v_{n+1})\in S : v_1 = i\right\} \quad \left(i\in\left\{0,\dots, s_\text{max}\right\}\right)$$
    and put 
    $$S_\text{max} := \left\{ v=(v_1,\dots,v_{n+1}) \in S : v_{n+1} \geq s_\text{max}\right\}.$$
    Note that $S_{\max} \subset \bigcup_1^m\left(s_i+\N^{n+1}\right).$ Indeed, if $x\in S_{\max}$, then $(x_1,\dots,x_n)\in \bigcup_1^m \left(s_i'+\N^n\right)$. In particular, for some $i\in\{1,\dots,m\}$  $x = (s_{i1}+v_1,\dots,s_{in}+v_n)\in s_i'+\N^n$. Since $x_{n+1}\geq s_{\max}$, it follows that $x_{n+1} = s_{i,n+1}+v_{n+1}$, and thus that 
    $$x = \left(s_{i1}+v_1,\dots,s_{in}+v_n,s_{i,n+1}+v_{n+1}\right)\in s_i+\N^{n+1}\subset \bigcup_1^m \left( s_j+\N^{n+1}\right).$$.
    We furthermore have that $S = S_\text{max} \cup \bigcup_0^{s_\text{max}-1} S_i$. Again using induction there are $s_1^{(i)},\dots,s_{m_i}^{(i)}\in \pi(S_i)$ such that 
    $$\pi(S_i)\subset  \bigcup_{j=1}^{m_i} \left(s_j^{(i)} + \N^n\right)$$
    Then 
    $$S_i \subset \bigcup_{j=1}^{m_i} \left( \left(s_{j1}^{(i)},\dots,s_{jn}^{(i)},i\right) + \N^{n+1}\right).$$
    We also have that 
    $$S_{\max} \subset \bigcup_1^m \left(s_j + \N^{n+1}\right).$$
    It thus follows that 
    $$S \subset \bigcup_1^m \left(s_j + \N^{n+1}\right) \cup \bigcup_{i=0}^{s_{\max}} \bigcup_{j=1}^{m_i}\left( \left(s_{j1}^{(i)},\dots,s_{jn}^{(i)},i\right) + \N^{n+1}\right).$$
\end{proof}
\begin{corollary}
    A term ordering $\leq$ on $\N^n$ is a well-ordering. 
\end{corollary}
\begin{proof}
    Let $S\subset \N^n$ be a non-empty subset. By Dickson's lemma there are $s_1,\dots,s_m\in S$ such that $S\subset \bigcup_1^m \left(s_i+\N^n\right)$. Since $\leq$ is a total order, we can define $s_{\min} := \min_{i\in\{1,\dots,m\}}\ s_i$. Let $s\in S$. For some $j\in \{1,\dots,m\}$, $s= s_j + v$ for some $v\in \N^n$. Now we have the following implications using properties of term orders,
    $$0\leq v \text{ and } s_{\min}\leq s_j \implies s_{\min} \leq s_{\min}+v \leq s_j + v = s,$$
    hence $s_{\min}$ is a least element of $S$, implying $\leq$ is a well-ordering. 
\end{proof}
A term order on $\N^n$ defines a total order on the monomials in 
$R[x_1,\dots,x_n]$ by defining $x^v\leq x^w$ if $v\leq w$. This total order will have the property that $x^{v_1} \leq x^{v_2}\implies x^{v_1+v}\leq x^{v_2+v}$ and $1 \leq x^v$. From this definition we gain a way of comparing polynomials using initials terms with respect to a term order. 
\begin{definition}
    Let $f=\sum_{v\in\N^n} a_v\mathbf{x}^v\in R[x_1,\dots,x_n]\setminus 0$ and $\leq$ a term order on $\N^n$. We define the \textit{initial term of $f$ with respect to $\leq$} to be the monomial
    $$\inn\ f := \max_{v\in \N^n : a_v \neq 0} a_v\mathbf{x}^v.$$
\end{definition}
\begin{lemma}\label{InitialTermProperties}
    Let $f,g\in R[x_1,\dots,x_n]\setminus 0$. Then one finds that
    \begin{enumerate}
        \item $\left(\inn \ f+g\right) \leq \max(\inn\ f, \inn\ g)$
        \item If $\inn \ f< \inn\ g$, then $\left(\inn\ f + g\right) = \inn\ g$.
        \item If the leading terms of $f$ and $g$ are equal then $\inn  \left(f-g \right) < \inn\ f = \inn\ g$.
        \item $\inn\ fg\leq \left(\text{lm}_\leq\ f\right)\left( \text{lm}_\leq\ g\right).$ 
        \item Suppose $R$ is an integral domain. Then $\inn\ fg = \left(\inn\ f\right)\left(\inn\ g\right)$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    Write $f=\sum_{v\in\N^n} a_v\mathbf{x}^v$ and $g=\sum_{v\in \N^N} b_v\mathbf{x}^v$. Let $w,u\in \N^n$ be given such that $a_v\mathbf{x}^w = \inn\ f$ and $b_w\mathbf{x}^u = \inn\ g$. The proofs here are very similar Lemma~\ref{PropertiesOfDegreeFunction} in some respects.
    1. Let $v\in \N^n$ be given such that $v>\max(w,u)$. Then $a_v,b_v=0$ hence $a_v+b_v=0$. It thus follows that 
    $$\left(\inn\ f+g\right) = \max_{v\in \N^n : a_v+b_v \neq 0} (a_v+b_v)\mathbf{x}^v \leq \max(\inn\ f,\inn\ g).$$
    2. When $\inn\ f < \inn\ g$, $\left(\inn\ f+g\right) \leq \max(\inn\ f,\inn\ g)$. Note that $a_u+b_u = b_u\neq 0$, hence $\left(\inn\ f + g\right) = \inn\ g$.\\
    3. Since leading terms of $f$ and $g$ are equal $w=u$ and $a_w = b_w$. Hence if $v\in \N^n$ is given such that $v\geq w$, we have that $a_v-b_v = 0$, thus it follows that 
    $$\inn\left(f-g\right) = \max_{v\in \N^n : a_v-b_v\neq 0} (a_v-b_v)\mathbf{x}^v< \inn\ f.$$
    4. \& 5.
    Let $v_1,v_2\in \N^n$ such that $v_1+v_2 = w+u $ and $v_1\neq w$. If $v_1>w$ or $v_1<w$. Hence 
    $$a_{v_1}=0\implies a_{v_1}b_{v_2}=0.$$
    In the other case $v_2>u$, because otherwise $v_1+v_2<u+w$. Hence $$b_{v_2}=0 \implies a_{v_1}b_{v_2}=0.$$
    It thus follows that 
    $$\sum_{v_1,v_2\in\N^n : v_1+v_2 = w+u} a_{v_1}b_{v_2}= a_wb_u.$$
    Let $v > u+w$. Let $v_1,v_2\in \N^n$ such that $v_1+v_2 = v$. Then $a_{v_1}b_{v_2} = 0$, implying $\sum_{v_1,v_2\in \N^n : v_1+v_2=v} a_{v_1}b_{v_2} = 0$. We thus have that 
    \begin{align*} 
        \inn\ fg &= \max_{v\in\N^n : \sum_{v_1,v_2\in\N^n : v_1+v_2=v} a_{v_1}b_{v_2}\neq 0} \left[\sum_{v_1,v_2\in \N^n : v_1+v_2 = v}a_{v_1}b_{v_2}\right]\mathbf{x}^v = \mathbf{x}^{w+u}=\left(\inn\ f\right) \left(\inn\ g\right).
    \end{align*}
    If $R$ is an integral domain we get that 
    $$\inn\ fg = \left[\sum_{v_1,v_2\in \N^n : v_1+v_2=w+u} a_{v_1}b_{v_2}\right]\mathbf{x}^{w+u} = \left(a_w\mathbf{x}^w\right)\left(b_u\mathbf{x}^u\right) =  \left(\inn\ f \right)\left( \inn\ g\right).$$
\end{proof}
The upshot of introducing this tool of bookkeeping is that it allows to do polynomial division. For $\mathbf{x}^v\mid \mathbf{x}^w$ we define $\frac{a\mathbf{x}^w}{b\mathbf{x}^v} := \frac{a}{b}\mathbf{x}^{w-v}$.
\begin{theorem}
    Let $R$ be an integral domain. Let $f,f_1,\dots,f_m\in R[x_1,\dots,x_n]\setminus 0$. Put $F= \{f_1,\dots,f_m\}$. Then there are $\lambda_1,\dots,\lambda_m, f^F\in R[x_1,\dots,x_n]$ such that 
    $$f = \left[\sum_1^m \lambda_if_i\right]+f^F,$$
    and $\inn\ \lambda_if_i\leq \inn\ f$ for every $i\in\{1,\dots,m\}$ with $\lambda_i\neq 0$ and $f^F = 0$ or $\inn \ f_i\nmid f^F$ for every $i$.
\end{theorem}
\begin{proof}
    We aim to provide a division algorithm that produces the desired the $\lambda_1,\dots,\lambda_m,f^F$. Define $\lambda_i^{(0)} := 0$ for every $i\in\{1,\dots,m\}$, $r^{(0)}=0$ and $s^{(0)} = f$. We note that 
    $$f = \left[\sum_1^m\lambda_i^{(0)}f_i\right] +\left(r^{(0)}+s^{(0)}\right).$$
    We want to recursively define $\lambda_1^{(j)},\dots,\lambda_m^{(j)},r^{(j)},s^{(j)}\in R[\mathbf{x}]$ such that 
    \begin{gather}\label{DivInvariant} 
        f = \left[\sum_1^m \lambda_i^{(j)}f_i\right] + \left((r^{(j)}+s^{(j)}\right)
    \end{gather}
    for every $j$ and have that $s^{(N)}=0$ at some $N$ such that putting $\lambda_i = \lambda_i^{(N)}$, $f^F=s^{(N)}$ these polynomials will have the remaining desired properties. For $j\geq 0$ if $s^{(j)}=0$ put $N=j$ and terminate, otherwise if there is an $i\in\{1,\dots,m\}$ such $\inn\ f_i\mid \inn\ s^{(j)}$, and pick the smallest such. We then define 
    \begin{equation}\label{reduction}
        \begin{cases}
            s^{(j+1)} := s^{(j)}-\frac{\inn\ s^{(j)}}{\inn\ f_i}f_i,\\
            \lambda_i^{(j+1)}:= \lambda_i^{(j)}+ \frac{\inn\ s^{(j)}}{\inn\ f_i},\\
            \lambda_k^{(j+1)} = \lambda_k^{(j)} \text{ for } k\neq i,\\
            r^{(j+1)}:= r^{(j)}.
        \end{cases}
    \end{equation}
    We note that indeed the identity (\ref{DivInvariant}) is fulfilled for $j+1$ since it is obtained by adding and subtracting $\frac{\inn\ s^{(j)}}{\inn\ f_i}f_i$.
    If no such $i$ exists we instead define 
    \begin{equation}\label{residue}
        \begin{cases}
            r^{(j+1)} := r^{(j)}+\inn\ s^{(j)},\\
            s^{(j+1)} := s^{(j)}-\inn\ s^{(j)},\\
            \lambda_i^{(j+1)} := \lambda_i^{(j)}.
        \end{cases}
    \end{equation}
    Again clearly (\ref{DivInvariant}) is still true for $j+1$, since $r^{(j+1)}+ s^{(j+1)} = r^{(j)}+s^{(j)}$.\\
    We now show that the above algorithm terminates. Let $j\geq 0$ such that $s^{(j+1)}\neq 0$.\\ 
    Consider that we land in case (\ref{reduction}). We denote $\inn\ s^{(j)} = \alpha_v\mathbf{x}^v$ and $\inn\ f_i = \beta_w\mathbf{x}^w$, where $i$ is the minimal index for which the initial term of $f_i$ divides the initial term of $s^{(j)}$. Then   
    $$\inn\left(\frac{\inn\ s^{(j)}}{\inn\ f_i}f_i\right) = \frac{a_v}{b_w}\mathbf{x}^{v-w}b_w\mathbf{x}^w = a_v\mathbf{x}^v=\inn\ s^{(j)}$$
    we have thus have that
    $$\inn\ s^{(j+1)} = \inn \left( s^{(j)}- \frac{\inn\ s^{(j)}}{\inn\ f_i}f_i \right)< \inn \ s^{(j)}.$$
    Landing in case (\ref{residue}) we have that 
    $$\inn\ s^{(j+1)}=\inn\left(s^{(j)}- \inn\ s^{(j)}\right) < \inn\ s^{(j)}.$$
    Then sequence of non-zero $s^{(j)}$ is thus a strictly decreasing sequence. Let $S$ denote the set of these elements. Since $s^{(0)} = f\neq 0$, $S\neq \emptyset$. This means $S$ has a minimal element $s^{(N-1)}$, since a term order is a well-ordering. Then $s^{(N)}=0$, for otherwise $s^{(N)}< s^{(N-1)}$.\\
    As advertised we put $a_i := a_i^{(N)}$ for $i\in \{1,\dots,m\}$ and $f^F := r^{(N)}$. For each $j\geq 0$ for each $i\in\{1,\dots,m\}$ for which $a_i^{(j)} = 0 $ and $a_i^{(j+1)} \neq 0$ we have that
    $$\inn\ a_i^{(j+1)}f_i = \inn\left(\left(a_i^{(j)}+\frac{\inn\ s^{(j)}}{\inn \ f_i}\right)f_i\right)= \inn\left(\frac{\inn\ s^{(j)}}{\inn\ f_i} f_i\right) = \inn\ s^{(j)}\leq \inn\ f$$
    It thus follows by induction in the $j$ for which $a_i^{(j)}\neq 0$ that
    \begin{align*}
        \begin{cases} 
            \inn\  a_i^{(j+1)}f_i = \inn\ a_i^{(j)}f_i\leq \inn\ f, \\
            \text{or}\\
            \inn\ a_i^{(j+1)}f_i = \inn\left(\left(a_i^{(j)}+\frac{\inn\ s^{(j)}}{\inn \ f_i}\right)f_i\right)\leq 
            \max\left(a_i^{(j)}f_i,\inn\ s^{(j)}\right)\leq \inn\ f.
        \end{cases}
    \end{align*}
    It thus follos that if $a_i\neq 0$,
    $$\inn\ a_if_i \leq \inn\ f.$$
    Note lastly that each $r^{(j)}$ is $0$ or a sum of terms not divisible by any $\inn\ f_i$, and hence $f^F$ is either $0$ or not divisible by any $\inn\ f_i$.
\end{proof}
\subsubsection{Gröbner Bases and Buchbergers Algorithm}
For the exploration of Gröbner bases we fix a field $K$.
\begin{definition}\label{GBDef}
    Let $I\subset K[x_1,\dots,x_n]$ be an ideal. Let $\leq$ be a term ordering on $K[\mathbf{x}]$. A finite set of polynomials $G\subset K[\mathbf{x}]\setminus 0$ is called a \textit{Gröbner basis} for $I$ with respect to $\leq$, if $G\subset I$ and for every $f\in I\setminus 0$ there is a $g\in G$ such that $\text{in}_\leq\ g \mid \text{in}_\leq \ f$.\\
    A finite set $G=\{f_1,\dots,f_m\}\subset K[\mathbf{x}]\setminus 0$ is called a Gröbner basis with respect to $\leq$ if it is a Gröbner basis for $\langle f_1,\dots,f_m\rangle$ with respect to $\leq$.
\end{definition}
\begin{proposition}\label{GBCriterionForCheckingMembership}
    Let $G=\{f_1,\dots,f_m\}\subset I$ be a Gröbner basis for an ideal $I\subset K[x_1,\dots,x_n]$ with respect to a term order $\leq$. For $f\in K[\mathbf{x}]$,
    $$f\in I \iff f^G = 0$$
\end{proposition}
\begin{proof}
    "$\impliedby$": If $f^G=0$ there are $\lambda_1,\dots,\lambda_m\in K[x_1,\dots,x_n]$ such that $f = \sum_1^m \lambda_if_i\in I$ using the division algorithm.\\
    "$\implies$": Suppose $f\in I$. Suppose for a contradiction that $f^G\neq 0$. Using the division algorithm with respect to $\leq$ we obtain $\lambda_1,\dots,\lambda_m\in K[\mathbf{x}]$ such that 
    $$f = \left[\sum_1^m \lambda_if_i\right]+f^G\implies f^G = f-\sum_1^m \lambda_if_i\in I.$$
    Then since $G$ is a Gröbner basis there is some $i\in\{1,\dots,m\}$ such that $\inn f_i\mid f^G$, but since $f^G\neq0$ this is not possible.
\end{proof}
As a corollary we obtain that every Gröbner basis of an ideal with respect to some term order will be a generating set for said ideal.
\begin{corollary}
    Let $I\subset K[x_1,\dots,x_n]$ be an ideal and $G\subset I$ a Gröbner basis for $I$ with respect to some term order $\leq$. Then $I=\langle G\rangle$.
\end{corollary}
\begin{proof}
    By definition $G\subset I\implies \langle G\rangle \subset I$. Let $f\in I$. By the above proposition, $f^G=0$, meaning there are $\lambda_1,\dots,\lambda_m\in K[\mathbf{x}]$ such that 
    $$f = \sum_1^m \lambda_if_i\in \langle G\rangle$$
\end{proof}
A somewhat curious consequence of the introduction of Gröbner bases is that it provides us with a rather simple way to prove the Hilbert basis theorem over fields. I.e. one can prove that any polynomial ideal over a field has a Gröbner basis, which by the above corollary constitutes a finite generating set.
\begin{corollary}(Hilbert's basis theorem over fields)
    Let $I\subset K[x_1,\dots,x_n]$ be a non-zero ideal and $\leq$ a term order. Then there is a Gröbner basis $G=\{f_1,\dots,f_m\}\subset I$ for $I$ with respect to $\leq$, hence $I = \langle f_1,\dots,f_m\rangle$ by the prior corollary. 
\end{corollary}
\begin{proof}
    Put $S= \left\{ v\in \N^n : \mathbf{x}^v = \inn\ f \text{ for some } f\in I\right\}$. Clearly $S\neq \emptyset$, hence by Dickson's lemma we may find $v_1,\dots,v_m\in S$ such that 
    $$S\subset \bigcup_1^m \left(v_i+\N^n\right)$$
    Let $f_i\in I$ be given such that $\inn\ f_i = \mathbf{x}^{v_i}$ for $i\in\{1,\dots,m\}$ and put $G=\{f_1,\dots,f_m\}\subset I$. Let $f\in I\setminus 0$, and pick $v\in \N^n$ such that $a_v\mathbf{x}^v= \inn\ f$. Since, $v\in S$, $v = v_j+w$ for some $j\in\{1,\dots,m\}$ and $w\in\N^n$. Then 
    $$\inn\ f = a_v\mathbf{x}^v = a_v\mathbf{x}^{v_j+w} = \left(a_v\mathbf{x}^w\right)\mathbf{x}^{v_j}=\left(a_v\mathbf{x}^w\right)\inn\ f_j \implies \inn\ f_j \mid \inn\ f.$$
    This verifies that $G$ is a Gröbner basis for $I$.
\end{proof}
The machinery of Gröbner bases provides a way to perform the division algorithm with respect to a term order in a fashion that ensures uniqueness of remainders and the indifference of the order of the divisor polynomials.
\begin{theorem}
    Let $\leq$ be a term order on $K[x_1,\dots,x_n]$ and $G = \{f_1,\dots,f_m\}\subset K[\mathbf{x}]\setminus 0$ a Gröbner basis with respect to $\leq$. Let $f\in K[\mathbf{x}]\setminus0$. Then any polynomial $r$ satisfying the properties of $f^G$ obtained from the division algorithm of $f$ by $f_1,\dots,f_m$ is equal to $f^G$. Furthermore, the remainder outputted by the division algorithm remains unchanged after a permutation of $f_1,\dots,f_m$.     
\end{theorem}
\begin{proof}
    Let $\lambda_1,\dots,\lambda_m\in K[\mathbf{x}]$ such that 
    $$f= \left[\sum_1^m \lambda_if_i\right] + f^G.$$
    Suppose there is an $r\in K[\mathbf{x}]$ with $r=0$ or $r$ is not divisible by the initial term of any $f_i$ such that there are $\lambda_1',\dots,\lambda_m'\in K[\mathbf{x}]$ satisfying 
    $$f = \left[\sum_1^m\lambda_i'f_i\right] + r.$$
    Then 
    $$f^G-r = \sum_1^m (\lambda_i-\lambda_i')f_i\in I.$$
    Suppose for a contradiction $f^G-r\neq 0$. Then there is a $j\in\{1,\dots,m\}$ such that $\inn\ f_j \mid \inn\left(f^G-r\right)$ implying $\inn\ f_j \mid \inn\ f^G$ or $\inn\ f_j\mid \inn\ r$ leading to a contradiction. It follows that $f^G =r$.\\
    Let $\omega \in \mathcal{S}(m)$ be a permutation. 
    Let $\lambda_1',\dots,\lambda_m',(f^G)'\in K[\mathbf{x}]$ be the 
    outcome of the division with respect to $\leq$ of $f$ with $f_{\omega(1)},\dots,f_{\omega(m)}$. Then by uniqueness of the remainder $f^G=(f^G)'$.
\end{proof}
 We have now to some extend motivated the usefulness of Gröbner bases (even though we are yet to see the most impressive applications!). However, as a computational tool, they are unimpressive if there is no way to compute. The introduction of $S$-polynomials and Buchberger's $S$-criterion will lead us to Buchberger's algorithm for computing Gröbner bases.
\begin{definition}
    Let $f\in K[x_1,\dots,x_n]$ and $F=\{f_1,\dots,f_m\}\subset K[\mathbf{x}]\setminus 0$. We say that \textit{$f$ reduces to zero modulo $F$} if there are $\lambda_1,\dots,\lambda_m\in K[\mathbf{x}]$ such that 
    $$f = \sum_1^m \lambda_if_i$$
    and $\inn\ \lambda_if_i \leq \inn\ f$ for $i\in\{1,\dots,m\}$ with $a_if_i\neq 0$. This will be denoted $f\to_F 0$.
\end{definition}
Note that this definition does not depend on a term order, however this definition leads us to the following reformulation of Proposition~\ref{GBCriterionForCheckingMembership}. Before formulating this consider the following lemmas 
\begin{lemma}\label{ConditionsForVanishingInitialTerms}
    Let $F=\{f_1,\dots,f_m\}\subset K[\mathbf{x}]\setminus 0$ and let $f \in I:=\langle F\rangle$ be non-zero with initial term $a_v\mathbf{x}^v$. Consider $\lambda_1,\dots,\lambda_m\in K[\mathbf{x}]$ such that 
    $$f=\sum_1^m \lambda_if_i.$$
    For each $i\in\{1,\dots,m\}$ where $\lambda_i\neq 0$, pick $v_i,w_i\in\N^n$ such that $b_{i}\mathbf{x}^{v_i}= \inn\ \lambda_i$ and $c_{i}\mathbf{x}^{w_i}=\inn\ f_i$. set 
    $$\kappa = \max\ \left\{v_i+w_i\in\N^n : i\in\{1,\dots,m\},\lambda_i\neq0 \right\}.$$
    Then $v\leq \kappa$ and the following statements hold
    \begin{enumerate}
        \item $v = \kappa \iff \inn\ f_i\lambda_i \leq \inn\ f \text{ for every } i\in\{1,\dots,m\} \text{ such that } \lambda_i\neq 0.$
        \item $v = \kappa \implies \inn\ f_i \mid \inn f$ for some $i\in\{1,\dots,m\}$
    \end{enumerate}
\end{lemma}
\begin{proof}
    Forgetting briefly that we assumed $f=\sum_1^m \lambda_if_i$, if $v>\kappa$, then
    $$\inn\ f > \max_{i\in\{1,\dots,m\}: \lambda_i\neq 0} \inn\ \lambda_if_i =\inn\left( \sum_1^m \lambda_if_i\right) \implies f\neq \sum_1^m \lambda_if_i.$$
    It thus follows that since we assumed $f=\sum_1^m\lambda_if_i$, we get the bound $v\leq \kappa$.\\
    1.
    For every $i\in\{1,\dots,m\}$ such that $\lambda_if_i\neq0 $ we have that
    $$b_ic_i\mathbf{x}^{v_i+w_i}=\inn \lambda_i \leq \inn\ f=a_v\mathbf{x}^v \iff \mathbf{x}^{v_i+w_i} \leq \mathbf{x}^v \iff v_i+w_i \leq v$$
    hence 
    $$\kappa = v \iff \inn\ \lambda_if_i\leq.$$
    2. Suppose $v = \kappa$. Then $v = v_i+w_i$and hence  $b_ic_i = a_v$ for some $i\in\{1,\dots,m\}$. WLOG we may then write 
    $$\inn\ f = a_v\mathbf{x}^v = \left[\sum_1^l b_ic_i\right]\mathbf{x}^{v_1}\mathbf{x}^{w_1} = \left(\left[\sum_1^l b_i\frac{c_i}{c_1}\right]\mathbf{x}^{v_1}\right)c_1\mathbf{x}^{w_1} = \left(\left[\sum_1^l b_i\frac{c_i}{c_1}\right]\mathbf{x}^{v_1}\right)\inn\ f_i \implies \inn\ f_i \mid \inn f.$$
    for some $l\leq m.$\\
\end{proof}
\begin{lemma}\label{CriterionForCheckingSetBeingGB}
    Let $F=\{f_1,\dots,f_m\}\subset K[x_1,\dots,x_n]\setminus 0$ and set $I=\langle F\rangle$. The following statements hold
    \begin{enumerate}
        \item If $f\to_F 0$ for every $f\in I$ then $F$ is a Gröbner basis.
        \item If $F$ is a Gröbner basis then for $f\in I\setminus 0$,
    $$f^F = 0 \iff f\to_F 0 $$
    \end{enumerate}
\end{lemma}
\begin{proof}
    1. Let $f\in I\setminus 0$. Then there are $\lambda_1,\dots,\lambda_m\in K[\mathbf{x}]$ such that 
    $$f = \sum_1^m \lambda_if_i,$$
    and $\inn\ \lambda_if_i\leq \inn\ f$ for every $i\in\{1,\dots,m\}$ with $\lambda_i\neq0$. Then by the above lemma 
    $$\kappa:= \max\ \left\{\inn \ \lambda_if_i : i\in\{1,\dots,m\}, \lambda_i\neq0\right\} = \inn \ f$$
     and by the same lemma we then have that $\inn\ f_i\mid \inn\ f$ for some $i\in \{1,\dots,m\}$, hence $F$ is a Gröbner basis.\\
     2. "$\implies$": If $f^F=0$ then there are $\lambda_1,\dots,\lambda_i\in K[\mathbf{x}]$ such that 
     $$f= \sum_1^m \lambda_if_i$$
     and $\inn\ \lambda_if_i \leq \inn\ f$ for $i\in\{1,\dots,m\}$ with $\lambda_i\neq0$ hence by definition $f \to_F 0$.\\
     "$\impliedby$": This follows from Proposition~\ref{GBCriterionForCheckingMembership}.
\end{proof}
We now introduce $S$-polynomials
\begin{definition}
    Let $f,g\in K[x_1,\dots,x_n]\setminus 0$. Pick $w\in \N^n$ such that $\mathbf{x}^w = \lcm(\inn\ f,\inn\ g)$. We define \textit{the $S$-polynomial} or \textit{the syzygy} of $f$ and $g$ to be 
    $$S(f,g):= \frac{\mathbf{x}^w}{\inn\ f }f-\frac{\mathbf{x}^w}{\inn 
 g }g.$$
\end{definition}
\begin{remark}
    Recall that $w = \left(\max\left(w^f_1,w^g_1\right),\dots,\max\left(w^f_n,w^g_n\right) \right)$, where $a_{w^f}\mathbf{x}^{w^f} = \inn\ f$ and $b_{w^g}\mathbf{x}^{w^g}= \inn\ g$.
\end{remark}
Note this simple fact about $S$-polynomials
\begin{lemma}\label{LeadingTermsOfTermsInSPolynomialVanish}
    Let $f,g\in K[x_1,\dots,x_n]\setminus 0$. Pick $w\in \N^n$ such that $\mathbf{x}^w = \lcm(\inn\ f,\inn\ g)$. Then $\inn\ S(f,g)< \mathbf{x}^w$. In other words, the initial term of $\frac{\mathbf{x}^w}{\inn\ f}f$ cancels with the initial term of $-\frac{\mathbf{x}^w}{\inn\ g}g$.
\end{lemma}
\begin{proof}
    Indeed, note that  
    \begin{align*} 
        \inn\ \frac{\mathbf{x}^w}{\inn\ f}f -\inn\ \frac{\mathbf{x}^w}{\inn\ g}g &= \left(\inn\ \frac{\mathbf{x}^w}{\inn\ f}\right)\left(\inn\ f\right) - \left(\inn\ \frac{\mathbf{x}^w}{\inn\ g}\right)\left(\inn\ g\right)\\ 
        &= \frac{\mathbf{x}^w}{\inn\ f}\inn\ f - \frac{\mathbf{x}^w}{\inn\ g}\inn\ g 
        = \mathbf{x}^w-\mathbf{x}^w= 0 
    \end{align*}
    hence the result follows from Lemma~\ref{InitialTermProperties} 3.
\end{proof}
These polynomials will be make the criterion for checking that a generating set is a Gröbner basis that Lemma~\ref{CriterionForCheckingSetBeingGB} 1. provides more practical. To be precise, we can reduce this criterion to just check that a finite set of S-polynomials reduce to zero modulo $F$.
\begin{lemma}\label{TechnicalLemmaForBuchbergerCriterion}
    Let $F=\{f_1,\dots,f_m\}\subset K[x_1,\dots,x_n]\setminus 0$ and let $1\leq l \leq m$ and $\sum_1^m\lambda_if_i \in \langle F\rangle$ with 
    $b_{i}\mathbf{x}^{v_i}=\inn \lambda_i$, $c_{i} \mathbf{x}^{w_i}=\inn\ f_i$ be given such that $$b_ic_ix^{v_i+w_i}=\inn \ \lambda_if_i =\kappa := \max_{j\in\{1,\dots,m\}}\ \inn\ \lambda_jf_i$$
    for every $i\in\{1,\dots,l\}$ and $\sum_1^l b_ic_i \neq 0$. Define $$\mu_{\lambda_1,\dots,\lambda_m,F} := \sum_1^m \left(\inn\ \lambda_i\right)f_i.$$
    Then $\mu_{\lambda_1,\dots,\lambda_m,F} \in I:=\left\langle S(f_1,f_2),S(f_2,f_3),\dots,S(f_{l-1},f_l)\right\rangle,$ and $\inn\ \mu_{\lambda_1,\dots,\lambda_m,F} < \kappa$. 
\end{lemma}
\begin{proof}
    Put $g_i := \mathbf{x}^{v_i}\frac{f_i}{c_i}$ for $i\in \{1,\dots,l\}$. Then 
    \begin{align*}
        \mu_{\lambda_1,\dots,\lambda_m,F} &= \sum_1^l b_ic_i\left(\mathbf{x}^{v_i+w_i}+\dots\right) = \sum_1^l b_ic_ig_i\\ 
        &= \left[\sum_{j=1}^{l-1}\left[ \sum_{i = 1}^j b_ic_i \right](g_j-g_{j+1})\right] + \underbrace{\left[\sum_1^l b_ic_i\right]}_{=0} g_l. 
    \end{align*}
    Put $x^{u_{ij}} := \lcm\left(\mathbf{x}^{w_i},\mathbf{x}^{w_j} \right)$ for $i,j\in\{1,\dots,l\}$ and note that 
    $$g_i-g_j = \frac{\mathbf{x}^{v_i}}{c_i}f_i-\frac{\mathbf{x}^{v_j}}{c_j}f_j = \frac{\mathbf{x}^{v_i+w_i}}{c_i\mathbf{x}^{w_i}}f_i-\frac{\mathbf{x}^{v_j+w_j}}{c_j\mathbf{x}^{w_j}}f_j \overset{(\ast)}{=} \mathbf{x}^{\xi_{ij}}\left( \frac{\mathbf{x}^{u_{ij}}}{\inn\ f_i}f_i-\frac{\mathbf{x}^{u_{ij}}}{\inn\ f_j}f_j\right) = \mathbf{x}^{\xi_{ij}}S(f_i,f_j),$$
    where we in step $(\ast)$ use that $u_{ij} < w_i+v_i=w_j+v_j$  to see that 
    $$v_i+w_i = v_j+w_j \implies \underbrace{v_i+w_i-u_{ij}}_{\xi_{ij}} = v_j+w_j-u_{ij}.$$
    Upon setting $\xi_{i}:=\xi_{i,i+1}$ we find
    $$\mu_{\lambda_1,\dots,\lambda_l,F} = \sum_1^{l-1} \mathbf{x}^{\xi_i}S(f_i,f_{i+1})\in I.$$
    Set $u_i = u_{i(i+1)}$. Then additionally we have that 
    $$\inn \ \mu_{\lambda_1,\dots,\lambda_m,F} = \max_{i\in\{1,\dots,l-1\}}\ x^{\xi_i}\inn\ S(f_i,f_{i+1}) < \max_{i\in\{1,\dots,l-1\}}\ \mathbf{x}^{\xi_i+u_i} = \max_{i\in\{1,\dots,l-1\}}\ \mathbf{x}^{v_i+w_i} = \kappa.$$
\end{proof}
\begin{theorem}\label{BuchBergersCritbeta}
    Let $F= \{ f_1,\dots,f_m\}\subset K[x_1,\dots,x_n]\setminus 0$. If $S(f_i,f_j)\to_F 0$ for every $i,j\in\{1,\dots,m\}$, then $f\to_F 0$ for every $f\in I:=\langle F\rangle$ meaning $F$ is a Gröbner basis (cf. Lemma~\ref{CriterionForCheckingSetBeingGB}).
\end{theorem}
\begin{proof}
    Let $f = \sum_1^m \lambda_if_i\in I$. If $\inn\ \lambda_if_i\leq \inn\ f$ for every $i$, we are done.\\
    Suppose this is not the case. We aim to re-express $f$ as an element of $I$. We do this via a \textbf{right-hand side initial term reduction} (this is non-standard terminology), which we will describe now. WLOG we may assume, adopting the notation from Lemma~\ref{TechnicalLemmaForBuchbergerCriterion}, that 
    $$b_ic_ix^{v_i+w_i}=\inn \ \lambda_if_i =\kappa.$$
    Then we have that $\inn\ f < \kappa$ by Lemma~\ref{ConditionsForVanishingInitialTerms}, hence necessarily $\sum_1^l b_ic_i =0$, which implies $\mu_{\lambda_1,\dots,\lambda_m,F} \in \langle S(f_1,f_2),S(f_2,f_3),\dots,S(f_{l-1},f_l)\rangle$ and $\inn\ \mu_{\lambda_1,\dots,\lambda_m}< \kappa$. By assumption there are $\psi_1^{(i)},\dots,\psi_m^{(i)}\in K[\mathbf{x}]$ such that $S(f_i,f_{i+1})=\sum_{j=1}^m \psi_j^{(i)}f_j$ with $\inn\ \psi^{(i)}_jf_j \leq \inn\ S(f_i,f_{i+1})$ for every $i\in \{1,\dots,l-1\}$ and $j\in\{1,\dots,m\}$. This means that 
    $$\mu_{\lambda_1,\dots,\lambda_m,F} = \sum_{j=1}^m\underbrace{\left[\sum_{i=1}^{l-1} \mathbf{x}^{\xi_i}\psi_j^{(i)}\right]}_{\chi_j}f_j$$
    with 
    \begin{align*} 
        \inn\ \chi_jf_j &= \max_{i\in\{1,\dots,l-1\}}\ \mathbf{x}^{\xi_i}\left(\inn\ \psi_j^{(i)}\right)\left(\inn\ f_j\right)\leq \max_{i\in\{1,\dots,l-1\}}\ \mathbf{x}^{\xi_i}\inn\ S(f_i,f_{i+1}) = \inn\ \mu_{\lambda_1,\dots,\lambda_m,F} < \kappa.
    \end{align*}
    Now note that 
    \begin{align*}
        f = \mu_{\lambda_1,\dots,\lambda_m,F} + \sum_1^l (\lambda_i-\inn\ \lambda_i)f_i+\sum_{l+1}^{m} \lambda_if_i,
    \end{align*}
    and that every term on the right-hand side of the above expression is strictly smaller then $\kappa$. We now obtain another expression for $f$: Upon putting $\delta_j = 1$ if $j\leq l$ and $\delta_j = 0$ otherwise we have
    $$f = \sum_{j=1}^m\underbrace{\left(\chi_j+\lambda_j-\delta_j\inn\ \lambda_j\right)}_{\lambda_j'}f_j.$$
    This is exactly the right-hand side initial term reduction we wanted to describe.
    If 
    $$\inn \ f = \kappa' := \max_{i\in\{1,\dots,m\}} \ \lambda_i'f_i,$$
    we have $\inn\ \lambda_i'f_i\leq \inn\ f $ for every $i$ and we are done. Otherwise we perform another right-hand side initial term reduction. Note that $\kappa'< \kappa$. If we follow the algorithm of terminating if the right-hand side expression leads to concluding $f\to_F 0$ or otherwise performing a right-hand side reduction we see that by the well-ordering of term orders we can only perform a finite number of iterations of right-hand side reductions, hence this algorithm will have to terminate. We thus conclude that $f\to_F 0$.   
\end{proof}
from this theorem we readily collect Buchberger's criterion for checking that a generating set is a Gröbner basis
\begin{corollary}\label{BuchbergersCriterion}(Buchberger's S-criterion)
    Let $F\subset K[x_1,\dots,x_n]\setminus 0$. Then $F$ is a Gröbner basis if and only if $S(f_i,f_j)\to_F 0$ or equivalently $S(f_i,f_j)^F=0$ (cf. \ref{CriterionForCheckingSetBeingGB}) for every $i,j\in\{1,\dots,m\}$.
\end{corollary}
\begin{proof}
    If $F$ is a Gröbner basis then $S(f_i,f_j)^F=0$ for every $i,j\in\{1,\dots,m\}$ since $S(f_i,f_j)\in I:=\langle F\rangle$ by Proposition~\ref{GBCriterionForCheckingMembership}, hence $S(f_i,f_j)\to_F 0$ by Lemma~\ref{CriterionForCheckingSetBeingGB}. If conversely $S(f_i,f_j)\to_F 0$ for every $i$ and $j$, it follows from Theorem~\ref{BuchBergersCritbeta} that $F$ is a Gröbner basis and hence that $S(f_i,f_j)^F = 0$ by Proposition~\ref{GBCriterionForCheckingMembership}.  
\end{proof}
This leads to Buchberger's algorithm for finding a Gröbner basis for an ideal $I=\langle f_1,\dots,f_m\rangle\subset K[x_1,\dots,x_n]\setminus 0$, which we will discuss in the following remark
\begin{remark}(Buchberger's algorithm)
    We now describe an algorithm for computing a Gröbner basis given an arbitrary generating set for an ideal $I$. Let $F_0 = \left\{f_1^{(0)},\dots,f_{m(0)}^{(0)}\right\}\subset I$ where $m(0)\geq 1$ be a generating set for $I$. For $i\geq 0$ if $S\left((f_j^{(i)},f_k^{(i)}\right)^{F_i} =0$ for every $f_j^{(i)},f_k^{(i)}\in F_i=\left\{f_1^{(i)},\dots,f_{m(i)}^{(i)}\right\}$ put $F_{i+1} = F_i$ or simply terminate, for then $F_i$ is a Gröbner basis. Otherwise if there are $f_j^{(i)},f_k^{(i)}\in F_i$ such that $S\left(f_j^{(i)},f_k^{(i)}\right)^{F_i}\neq 0$ put $$F_{i+1} = F_i\cup\left\{S\left(f_j^{(i)},f_k^{(i)}\right)^{F_i}\right\}.$$ 
    Note that $S\left(f_j^{(i)},f_k^{(i)}\right)^F = \sum_1^m \lambda_if_i-S\left(f_j^{(i)},f_k^{(i)}\right)\in I$, hence $\langle F_{i+1}\rangle = I$. The claim is that the ascending chain $F_0\subset F_1 \subset \dots$ will in fact stabilize, hence there we produce a Gröbner basis at some point. We check this claim in the next theorem.   
\end{remark}
\begin{lemma}
    Let $\{t_i\}_{i\geq 0} \subset K[x_1,\dots,x_n]$ be some sequence of which an element is either a term or $0$, i.e. $t_i = a_i\mathbf{x}^{v_i}$ for some $a_i\in K$, $v_i\in \N^n$. Then for some $N\geq 0$ for every $i\geq N$, $t_j\mid t_i$ for some $j< N$ 
\end{lemma}
\begin{proof}
    If $a_i=0$ for every $i\geq 0$ then the statement is trivial. Suppose this is not the case and put $S= \{v_i  :  i\geq 0, a_i\neq 0\}\subset \N^n$, then by Dickson's lemma there are $v_{i(1)},\dots,v_{i(k)}\in S$ such that 
    $$S\subset\bigcup_{j=1}^k \left(v_{i(j)}+\N^n\right).$$
    Set $N= \max_{j\in\{1,\dots,k\}}\ i(j)$ and let $i\geq N$. Then $v_i = v_{i(j)}+w$ for some $j\in\{1,\dots,k\}$, $w\in \N^n$, hence 
    $$t_i = a_i\mathbf{x}^{v_i} = a_{i}\mathbf{x}^{v_{i(j)}+w} = \left(a_{i(j)}\mathbf{x}^{v_{i(j)}}\right)\left(\frac{a_i}{a_{i(j)}}\mathbf{x}^{w}\right) = t_{i(j)}\left(\frac{a_i}{a_{i(j)}}\mathbf{x}^{w}\right) \implies t_{i(j)}\mid t_i$$
\end{proof}
\begin{theorem}
    Buchberger's algorithm terminates and outputs a Gröbner basis. 
\end{theorem}
\begin{proof}
    Buchberger's gives rise to an infinite sequence of polynomials in the following way: start with the initial elements of  $F_0=\{f_1,\dots,f_m\}$. For $i\geq m+1$ if $F_{i-m}$ is the union of $F_{i-m-1}$ and $\left\{S(f_j,f_k)^{F_{i-m-1}}\right\}$ for some $j,k\in\{1,\dots,i-1\}$ then put $f_i = S(f_j,f_k)^{F_{i-m-1}}$ otherwise put $f_i = 0$. We then put $t_i = \inn\ f_i$ if $f_i \neq 0$ or $t_i = 0$ otherwise for every $i\geq 0$. By the above lemma there is an $N\geq 0$ such that for every $l\geq N$, $t_h\mid t_l$ for some $h<N$. For each $i\geq m$, if $f_i = S(f_j,f_k)^{F_{i-m-1}}$, then any term of $f_i$ is not divisible by $\inn\ f_q$ for any $q\in\{1,\dots,i-1\}$. Consider then $l\geq \max(m,N)$ if $t_h \mid t_l$ for $h<N$, then $t_l$ cannot be a term of some $S(f_j,f_k)^{F_{l-m-1}}$, hence $t_l = 0$, implying $f_l =0$ and hence that $F_{l-m}$ satisfies Buchberger's criterion.     
\end{proof}
The below proposition will give an easy criterion for checking whether two polynomials in a generating reduce modulo said generating set. 
\begin{proposition}\label{SufficientToCheckInitialTermsCoPrime}
    Let $\leq$ be a term order on $K[x_1,\dots,x_n]$. Let $f,g\in K[\mathbf{x}]\setminus0$. Suppose $\gcd(f,g) = 1$, then $S(f,g)\to_{\{f,g\}} 0$.
\end{proposition}
\begin{proof}
    By assumption, 
    $$\lcm(\inn\ f,\inn\ g) = \left(\inn\ f\right) \left(\inn\ g\right) = \inn\ fg.$$
    Put $r = f- \inn\ f$ and $s = g -\inn \ g$. Then $\inn\ r < \inn\ f$ or $\inn\ r =0$ and $\inn\ s < \inn\ g$ or $\inn\ s =0$. Then 
    $$S(f,g) = \left(\inn\ g\right)f -\left(\inn\ f\right)g = (g-s)f-(f-r)g=rg-sf.$$
    Suppose $r =0$, then $S(f,g)=-sf$, implying $\inn\ f \leq \inn\ S(f,g)$ and hence $S(f,g)\to_{\{f,g\}} 0$. Suppose $\inn\ r < \inn\ f$.  Suppose for a contradiction
    $$\left(\inn\ r \right)\left(\inn\ g \right) = \left(\inn\ s \right)\left(\inn\ f \right).$$
    Then $\inn\ f \mid \inn\ r$ (since $\inn\ g \nmid \inn\ f$), but then $\inn\ f \leq \inn\ r$ leading to a contradiction. We then find that 
    $$\inn\ S(f,g) = \inn\left( rg - sf\right) = \max(\inn\ rg, \inn\ sf)$$
    hence by Lemma~\ref{ConditionsForVanishingInitialTerms}, $S(f,g)\to_{\{f,g\}} 0.$
\end{proof}
\begin{definition}
    A Gröbner basis $G=\{f_1,\dots,f_m\}\subset K[x_1,\dots,x_n]\setminus 0$ is called \textit{minimal}, if
    \begin{enumerate}
        \item $\inn\ f_i\nmid \inn\ f_j$ for every $i,j\in\{1,\dots,m\}$ with $i\neq j$.
        \item $\inn\ f_i = \mathbf{x}^{v_i}$ for some $v_i\in \N^n$ for every $i\in\{1,\dots,m\}$.
    \end{enumerate}
    $G$ is \textit{reduced} if it is minimal and if every term in $f_i$ is not divisible by $\inn\ f_j$ for every $i,j\in\{1,\dots,m\}$ with $i\neq j$.  
\end{definition}
\begin{remark}
    We describe an algorithm for computing a minimal Gröbner basis for a non-zero ideal $I\subset K[x_1,\dots,x_n]$. Let $G=\{f_1,\dots,f_m\}\subset I\setminus 0$ be a Gröbner basis. Let $a_i$ be the leading coefficient for $f_i$ for each $i$. Then define 
    $$G_0 := \left\{a_1^{-1}f_1,\dots,a_m^{-1}f_m\right\}$$
    For $k\geq 0$, if there are some $f,g\in G_k\setminus 0$ with $f\neq g$ such that that $\inn\ g \mid \inn\ f$, define $G_{k+1}:=G_k\setminus \{f\}$, otherwise terminate.
\end{remark}
\begin{lemma}
    Every ideal $0\neq I\subset K[x_1,\dots,x_n]$ has a minimal Gröbner basis. 
\end{lemma}
\begin{proof}
    We prove that the algorithm described above always produces a minimal Gröbner basis. Let $G=\{f_1,\dots,f_m\}\subset I\setminus0$ be a Gröbner basis for $I$. We prove the statement by induction in $m$. For $m=1$, $G=\{g\}$ for some $g\in I\setminus 0$. Let $a\in K\setminus 0$ be the leading coefficient of $g$. Then $G_0 = \{a^{-1}g\}$ defines a minimal Gröbner basis. Suppose the algorithm always terminates with a minimal Gröbner basis with mm elements for some $m\geq 1$. Let $G = \{f_1,\dots,f_{m+1}\}$ be a Gröbner basis and assume WLOG that the coefficient of the polynomials in $G$ are all $1$, i.e. that $G_0 = G$. Then if there are no $i,j\in\{1,\dots,m+1\}$ with $i\neq j$ such that $\inn\ f_j\mid \inn\ f_i$, $G_0$ we terminate and indeed $G_0$ is a minimal Gröbner basis. Otherwise we put $G_1 = G_0\setminus \{f_i\}$. Let $f\in I$, then for some $l\in\{1,\dots,m+1\}$, $\inn\ f_l\mid\inn \ f$. If $l = i$, then $\inn\ f_j\mid \inn\ f$, hence in any case the initial term of $f$ is divisible by the initial term of some polynomial in $G_1$, implying $G_1$ is a Gröbner basis. Since $G_1$ has $m$ elements it follows by induction that $G_k$ is a minimal Gröbner basis.  
\end{proof}
\begin{proposition}
    Every ideal $0\neq I\subset K[x_1,\dots,x_n]$ has a unique reduced Gröbner basis. 
\end{proposition}
\begin{proof}
    \textbf{Uniqueness:} Consider two reduced Gröbner bases $G=\{f_1,\dots,f_m\}$ and $G' = \{f_1',\dots,f_{m'}'\}$. We first check that the cardinality of these Gröbner bases match. Let $i\in\{1,\dots,m\}$, then for some $\tau(i)\in\{1,\dots,m'\}$, $\inn\ f'_{\tau(i)}\mid \inn\ f_i$. Let $j\in \{1,\dots,m'\}$ then for some $\omega(j)\in \{1,\dots,m\}$, $\inn \ f_{\omega(j)} \ \inn\ f'_{j}$. Then we have that  
    $$\inn\ f_{\omega(\tau(i))}\mid \inn\ f'_{\tau(i)} \text{ and }    \inn\ f'_{\tau(i)} \mid \inn\ f_{i} \implies \inn\ f_{\omega(\tau(i))} \mid \inn\ f_i$$
    by minimality of the Gröbner bases $i = \omega(\tau(i))$. A similar argument shows that $\tau(\omega(j))$ for every $j\in\{1,\dots,m'\}$, thus $\tau$ is a bijection, implying $m=m'$. We proceed by checking that the sets are equal. Note that the above argument also shows that $\inn\ f'_{\tau(i)} = \inn\ f_i$ for every $i\in\{1,\dots,m\}$ since the coefficient of every initial term is $1$.\\
    Let $i\in\{1,\dots,m\}$. Since $\inn\ f_i=\inn\ f_{\tau(i)}'$, either $f_i=f_{\tau(i)}$ or $\inn\left( f_i-f_{\tau(i)}'\right)<\inn\ f_i$. We shall that the second case implies $f_i=f_{\tau(i)}$ as well. In this case no term in $f_i-f_{\tau(i)}'$ is divisible by $\inn\ f_i$. Any term in $f_i-f'_{\tau(i)}$ is a term in $f'_{\tau(i)}$ subtracted from $f_i$, where at least one of these terms in non-zero. Then by the Gröbner bases being reduced, $\inn\ f_j$ does not divide such a term for any $j\in\{1,\dots,m\}\setminus\{i\}$. Then $f_i-f_{\tau(i)}' = \left(f_i-f_{\tau(i)}'\right)^G$, but since $f_i-g_i \in I$, this must imply that $f_i=f_{\tau(i)}'$ by Proposition~\ref{GBCriterionForCheckingMembership}.\\ 
    \textbf{Existence}:
    By the prior lemma there is a minimal for Gröbner basis $G=\{f_1,\dots,f_m\}$ for $I$. Define $g_i := f_i^{\{g_1,\dots,g_{i-1},f_i,\dots,f_m\}\setminus\{f_i\}}$ for every $i\in\{1,\dots,m\}$. Since $\inn\ f_i$ is divisible by any $\inn\ f_j$ for $j\in\{i+1,\dots,m-1\}$ we see that $g_i$ is of the form $\inn\ f_i+\dots$. Thus if $f\in I$, there is some $g_j$ such that $\inn\ g_j = \inn\ f_j\mid \inn\ f$, meaning that each set $\{g_1,\dots,g_\{k-1\},f_k,\dots,f_m\}$ and in particular $G' := \{g_1,\dots,g_m\}$ is a Gröbner basis. The $g_i$'s being residues following the division algorithm by a set of polynomials with initial terms coming from $\{f_1,\dots,f_m\}\setminus \{f_i\}$ implies that no term in $g_i$ is divisible by any $\inn\ f_j$ for $j\neq i$, thus $G'$ is a reduced Gröbner basis.    
\end{proof}
\begin{remark}
    Note that the existence proof above is of an algorithmic nature. I.e. given an ideal, use Buchberger's algorithm to produce a Gröbner basis, then use the already presented algorithm for producing a minimal gröbner basis, then apply the division algorithmic in the way we described above to produce the elements of the reduced Gröbner basis. 
\end{remark}
\begin{theorem}\label{GBElimTheorem}
    Let $G$ be a Gröbner basis for an ideal $I\subset K[x_1,\dots,x_n]$ with respect to the lexicographic term order with $x_1<\dots<x_n$. Then $G\cap K[x_1,\dots,x_i]\subset K[x_1,\dots,x_i]$ is a Gröbner basis for the ideal $I\cap K[x_1,\dots,x_i]\subset K[x_1,\dots,x_i]$ with respect to the lexicographic term order with $x_1<\dots< x_i$ for every $i\in\{1,\dots,i\}$.
\end{theorem}
\begin{proof}
    Let $G'=G\cap K[x_1,\dots,x_i]$. Let $f\in I' = I\cap K[x_1,\dots,i]$.  For some $g\in G$, $\inn\ g \mid \inn \ f$. Let $t=a\mathbf{x}^v$ be a term of $g$ and write $b\mathbf{x}^w = \inn\ f$. Then 
    $a\mathbf{x}^v\leq b\mathbf{x}^w$, or in other words $v \leq w$. Let $u\in \N^n$. If $u_j \neq 0$ for $j\in\{i+1,\dots,m\}$, then $w<_\text{lex} u$. Hence we conclude that $v_j = 0$ for every $j\in\{i+1,\dots,m\}$, implying $t\in K[x_1,\dots,x_i]$ and ultimately that $g\in K[x_1,\dots,x_i]$.
\end{proof}
The above theorem is a great tool for computing solutions to complicated polynomial equations. 
\subsubsection{Polynomials over UFD's}
We aim to prove that polynomials over unique factorization domains are unique factorization domains. For this reason we fix a UFD $R$ (unless something else is explicitly stated). 
\begin{lemma}\label{PrimeInRingImpliesPrimePolynomialRing}
    Let $R$ be an integral domain. If $p\in R$ is prime then $p\in R[x_1]$ is prime. Therefor if $p\in R$ is prime, then $p\in R[x_1,\dots,x_n]$ is prime.  
\end{lemma}
\begin{proof}
    $p\neq0$ and $p\notin R[x]^\ast = R^\ast$. Let $f=\sum_0^k a_ix^i,g=\sum_0^h b_ix^i\in R[x]$ such that $p\nmid f,g$. Set $s := \max\ \{i\in\{1,\dots,k\} : p\nmid a_i\}$ and $t:= \max\ \{i\in\{1,\dots,h\} : o\nmid b_i\}$. Note that if $i>s$ or $j>r$, then $p\mid a_i$ or $p\mid b_j$. This means that since $i+j = s+t$ implies $i\geq s$ or $j\geq t$, one finds that $p$ divides every term in $\sum_{i=1}^h\sum_{j=1}^k \sum_{i+j = s+t} a_ib_j$ other than $a_sb_t$, hence $$p\nmid \sum_{i=1,\dots,h,j=1,\dots,k i+j, = s+t} a_ib_j \implies p\nmid fg.$$  
\end{proof}
\begin{definition}
    A polynomial $f=\sum_0^n a_ix^i\in R[x]$ is said to be \textit{primitive} if $\gcd(a_0,\dots,a_n:a_i\neq 0)=1$.
\end{definition}
\begin{lemma}\label{CanConstructPrimitivePolynomialOverUFDFromMonicPolynomialOverQuotientField}
    Let $R$ be a UFD, $K:=Q(R)$ and consider a monic $f=x^d+\sum_0^{d-1} \frac{a_i}{b_i}x^i$. Let $c$ be the least common multiple of the $b_i$'s. If $\gcd(a_i,b_i)=1$ whenever $a_i\neq 0$ then $g:=cf$ is primitive. 
\end{lemma}
\begin{proof}
    Take any prime divisor $p$ of $c$. Pick $i$ where $p$ has maximal multiplicity among all $b_i$'s. Then $p\nmid \frac{c}{b_i}$ and $p\nmid a_i$, since $c= \prod_{p \in \mathcal{P}} p^{\max(\nu_{p}(b_0),\dots,\nu_p(b_0))}$. Then $p\nmid c\frac{a_i}{b_i}$. So for any divisor of $c$ there is some $i$ for which this is not a divisor of $c\frac{a_i}{b_i}$, implying $\gcd(c,ca_i/b_i: a_i\neq 0)=1$, hence $cf$ is primitive.
\end{proof}
\begin{lemma}\label{GaussLemma}(Gauss' Lemma)
    If $f,g\in R[x]$ is primitive, then $fg$ is primitive. This property extends to multivariable polynomials by induction.
\end{lemma}
\begin{proof}
    Suppose $fg$ is not primitive then the greatest common divisor of the coefficients of $fg$ is divisible by some prime $p\in R$. This means $p\mid fg$, hence $p\mid f$ or $p\mid g$ by Lemma~\ref{PrimeInRingImpliesPrimePolynomialRing}, hence the $p$ divides all of the coefficients $f$ or $g$, hence $f$ primitive or $g$ is primitive. 
\end{proof}
\begin{lemma}\label{PrimitivePolynomialOverFractionFieldDivisorImpliesDivisorInBaseRing}
    Let $f,g\in R[x]$. If $f$ is primitive and $f\mid g$ in $Q(R)[x]$, then $f\mid g$ in $R[x]$.
\end{lemma}
\begin{proof}
    By assumption we can find an $h\in Q(R)[x]$ such that $g = hf$. If $h=0$, then $g=0$ and we are done. Suppose $h\neq 0$. Then for some $c\in R\setminus 0$, $ch\in R[x]$. For some $d\in R\setminus 0$ and primitive $h'\in R[x]$, $ch=dh'$, implying $cg = chf=dh'f$. Note $h'f$ is primitive by the prior lemma, hence $d$ is the greatest common divisor for the coefficients of $dh'f$. This implies $d$ is the greatest common divisor of the coefficients of $cg$. Since $c\mid cg$, it follows that $c\mid d$, hence $\frac{d}{c}\in R$. This implies that                                     $$g = \frac{d}{c}h'f\in R[x],$$                               
    which means $f\mid g$ in $R[x]$.
\end{proof}
In the following lemma we classify all the irreducible polynomials in $R[x]$.
\begin{lemma}\label{ClassificationOfIrreducibles}
    \begin{enumerate}
        \item Let $f\in R[x]$ be primitive. If $f$ is irreducible in $Q(R)[x]$, then $f$ is prime in $R[x]$. 
        \item Any non-zero, non-unit element in $R[x]$ can be written as a product of irreducible elements. 
        \item The irreducible elements in $R[x]$ are the primes in $R$ and the polynomials described in 1.
    \end{enumerate}
\end{lemma}
\begin{proof}
    1. Let $a,b\in R[x]$ such that $f\mid ab$ in $R[x]$. Using {\Large Result that shows $K[x]$ is UFD} we get that $f$ is prime by Proposition~\ref{EquivalentFormulationOfUFD} in $K[x]$, hence $f \mid a$ or $f \mid b$ in $K[x]$, hence by Lemma~\ref{PrimitivePolynomialOverFractionFieldDivisorImpliesDivisorInBaseRing} $f\mid a$ or $f\mid b$ in $R[x]$.\\
    2. Let $f\in R[x]$ be a non-zero, non-unit element. If $\deg \ f =0$ it is an element in $R$, which is a UFD, hence $f$ has a factorization into irreducibles. If $\deg \ f>0$, then there primes/irreducibles $g_1,\dots,g_m\in Q(R)[x]$ such that $f = \prod_1^m g_i$. For suitable $a_1,\dots,a_m\in K$ and primitive $f_1,\dots,f_m\in R[x]$ such that $g_i = a_if_i$ for each $i\in\{1,\dots,m\}$. Since $g_i$ is irreducible in $Q(R)[x]$ for each $i$, so is $f_i$ in $Q(R)[x]$. Set $a := \prod_1^m a_i$. Then $f = a\prod_1^m f_i$, hence $\prod_1^m f_i \mid f$ in $Q(R)[x]$, and hence also in $R[x]$ by Lemma~\ref{PrimitivePolynomialOverFractionFieldDivisorImpliesDivisorInBaseRing}. This means $a \in R$. If $a$ is a unit, set $f_1' = af_1$. Then we get a factorization into irreducibles,
    $$f = f_1'\prod_2^mf_i.$$
    If $a$ is not a unit, we write $a = \prod_1^l p_i$ for primes/irreducibles in $R$, getting a factorization intio irreducibles
    $$f = \left(\prod_1^l p_i \right)\left(\prod_1^m f_i\right).$$
    3. We have already established that the two types of elements in question are irreducible in $R[x]$ (cf. 1 and Lemma~\ref{PrimeInRingImpliesPrimePolynomialRing}). Let $f$ be irreducible in $R[x]$. If $\deg\ f = 0$, then $f\in R$, hence $f$ is prime as $R$ is a UFD. If $\deg f>0$, then we saw in 2. that  $f$ can be written as a product of primes in $R$ and primitive polynomials in $R[x]$ irreducible in $Q(R)[x]$. Writing $f = a f_1\cdots f_m $ for $f_1,\dots,f_m$ polynomials being of the second type of irreducibles. We see that $m=1$ and $a$ is a unit for otherwise this would contradict the irreducibility of $f$.   
\end{proof}
\begin{theorem}
    $R[x]$ is a UFD. By induction $R[x_1,\dots,x_n]$ is a UFD.
\end{theorem}
\begin{proof}
    Every non-zero non-unit element is a product irreducible elements in $R[x]$ by Lemma~\ref{ClassificationOfIrreducibles} 2. Let $f$ be an irreducible element in $R[x]$. Then either $f$ is a prime in $R$ or is primitive in $R[x]$ and irreducible in $Q(R)[x]$ by Lemma~\ref{ClassificationOfIrreducibles} 3. In the second case $f$ is also prime by Lemma~\ref{ClassificationOfIrreducibles} 1. It follows by Proposition~\ref{EquivalentFormulationOfUFD} that $R[x]$ is a UFD. Since $R[x_1,\dots,x_{n}]\simeq (R[x_1,\dots,x_{n-1}])[x_n]$ it follows by induction that $R[x_1,\dots,x_n]$ is a UFD. 
\end{proof}
\begin{proposition}
    Let $f\in R[x]$ such that $f$ is monic and $\deg\ f \in\{2,3\}$. Then $f$ is irreducible if and only if $f$ has not roots. 
\end{proposition}
\begin{proof}
    "$\implies$": Suppose $f$ has a root $\alpha\in R$. Then $f\in \langle x-\alpha\rangle $, hence $f = (x-\alpha)g$ for some $g \in R[x]$. In either case of $\deg f = 2$ or $\deg \ f =3$, we have that $\deg \ g \geq 1$, implying $g$ is not a unit. This means $f$ is reducible.\\
    "$\impliedby$": Suppose $f$ is reducible. Since the irreducible non-constant polynomials in $R[x]$ are monic, there is, In any case of $f$ having degree $2$ or $3$, a polynomial $g = (x-\alpha)\in R[x]$, such that 
    $$f = gh,$$
    for some $h\in R[x]$. It hence follows that $f$ has a root $\alpha$.
\end{proof}
\begin{corollary}
    The polynomial $x^2-a\in R[x]$ is irreducible if and only if $a$ is not a square. 
\end{corollary}
\subsubsection{Eisenstein's Criterion}
\subsubsection{Homogeneous Polynomials}
\begin{definition}
    A polynomial $f\in R[x_1,\dots,x_n]$ is \textit{homogeneous of degree $d$}, if there is a $d\geq 0$ such that $f = \sum_{v\in \N^n: \vert v\vert =d} a_v \mathbf{x}^v$.
\end{definition}
\begin{remark}
    Note that if $f\neq0$ then $f$ is homogeneous if and only if every non-zero term is equal to $d$, hence $\deg\ f=d$. Note also that $0$ is homogeneous of degree $d$ for every $d\geq 0$.\\
    One readily verifies that the set of degree $d$ homogeneous polynomials in $R[\mathbf{x}]$ is an $R$-module, which we will denote $V_R(d,n)$. The set $\left\{\mathbf{x}^v\in R[x_1,\dots,x_n] : \vert v\vert =d\right\}$ forms a basis for $V_R(d,n)$. 
\end{remark}
\begin{lemma}\label{ProductAndSumOfHomogeneousPolynomials}
    Let $f,g\in R[x_1,\dots,x_n]$. 
    \begin{enumerate}
        \item If $f,g$ are homogeneous of degree respectively $d$ and $e$, then $fg$ is homogeneous of degree $d+e$. Suppose $R$ is an integral domain and $f,g\neq0$. Then if $fg$ is homogeneous, so is $f$ and $g$.  
        \item If $f,g$ are homogeneous of degree $d$, so is $f+g$.   
    \end{enumerate}
\end{lemma}
\begin{proof}
    1. Write $f=\sum_{v\in \N^n:\vert v\vert =d}a_v\mathbf{x}^v$ and $g=\sum_{w\in\N^n:\vert w \vert = e}b_w\mathbf{x}^w$. Then 
    $$fg=\sum_{v\in \N^n : \vert v\vert = d}\sum_{w\in \N^n : \vert w\vert =e} a_vb_w\mathbf{x}^{v+w}=\sum_{u\in \N^n : \vert u \vert = d+e}\left[\sum_{v,w\in \N^n : v+w=u} a_vb_w\right]\mathbf{x}^u.$$
    Suppose $f$ is not homogeneous. Since $f$ is not homogeneous the set $\left\{l\geq 0 : \text{ there is a } u\in \N^n \text{ with } \vert u\vert =  l, a_u\neq0\right\}$ has at least $2$ elements. Let $m$ be the minimum of this set and $M$ the maximum. Note that $m\neq M$. Pick $u_m,u_M\in\N^n$ such that $\vert u_m\vert =m$ and $\vert u_M\vert = M$. Pick $b_{q_k}\mathbf{x}^{q_k}$, $b_{q_K}\mathbf{x}^{q_K}$ to be respectively a lowest degree term and a highest degree term of $g$. Then $\sum_{v,w\in \N^n : v+w = u_k + u_m} a_vb_w = a_{u_m}b_{u_k}\neq 0$ and $\sum_{v,w\in \N^n : v+w = u_K+u_M} a_vb_w =a_{u_M}b_{u_K} \neq 0$. $fg$ therefor has two non-zero monomial terms of different degree and therefor is not homogeneous.\\    
    2. We get that 
    $$f+g=\sum_{v\in \N^n:\vert v\vert=d} a_v\mathbf{x}^v + \sum_{v\in \N^n : \vert v\vert =d} b_v\mathbf{x}^v = \sum_{v\in \N^n : \vert v\vert =d} (a_v+b_v)\mathbf{x}^v.$$
\end{proof}

\begin{definition}
    Let $f\in R[x_1,\dots,x_{n}]$. We define \textit{the dehomogenization of $f$ at $x_i$} to be the polynomial 
    $$f_{\ast,i}:=f(x_1,\dots,x_{i-1},1,x_{i+1},\dots,x_n).$$
\end{definition}
\begin{remark}
    We define $f_\ast := f_{\ast,n}$
\end{remark}
\begin{lemma}
    Let $f\in R[x_1,\dots,x_n]$. There are unique homogeneous polynomials $f_0,\dots,f_d\in R[\mathbf{x}]$ where $\deg\ f_i = i$ for $f_i\neq0$ such that
    $$f=\sum_1^d f_i.$$
    If $f\neq 0$, then $f_d\neq 0$
\end{lemma}
\begin{proof}
    If $f=0$ the statement is trivial. So suppose $f\neq 0$. Set $d=\deg \ f$ and write $f=\sum_{v\in \N^n} a_v\mathbf{x}^v$. Set $f_i=\sum_{v\in \N^n:\vert v\vert =i} a_v\mathbf{x}^v$. Then clearly $f=\sum_1^d f_i$. For some $v\in \N^n$ with $\vert v \vert = d$, $a_v\neq0 $, hence $f_d\neq0$. Uniqueness follows from uniqueness of the monomial representation of a polynomial.  
\end{proof}
\begin{corollary}
    $R_d[x_1,\dots,x_n]=\sum_0^d V_R(n,i)$. This sum is direct. 
\end{corollary}
\begin{definition}
    Let $f\in R[x_1,\dots, x_n]$, write $f=\sum_0^df_i$ (cf. the above lemma). Then \textit{the homogenization of $f$} is the polynomial
    $$f^\ast := \sum_0^d x_{n+1}^{d-i}f_i $$
\end{definition}
\begin{remark}\label{AnAlternativeDefinitionAndPreimageOfHomogenizationOfZeroIsZero}
    An alternative definition is that $f^\ast = x_{n+1}^df\left(\frac{x_1}{x_{n+1}},\dots,\frac{x_n}{x_{n+1}}\right)$ (in $Q(R[x_{n+1}])[x_1,\dots,x_n]$ for instance). Indeed,
    \begin{align*}
        x_{n+1}^df\left(\frac{x_1}{x_{n+1}},\dots,\frac{x_n}{x_{n+1}}\right) &= x_{n+1}^d\sum_{v\in \N^n} a_v\frac{x_1^{v_1}}{x_{n+1}^{v_1}}\cdots \frac{x_n^{v_n}}{x_{n+1}^{v_n}}\\ &=x_{n+1}^d\sum_{0}^d \sum_{v\in \N^n: \vert v\vert =i} a_v\frac{x_1^{v_1}\cdots x_n^{v_n}}{x_{n+1}^{v_1+\dots + v_n}}\\ 
        &= \sum_0^d \sum_{v\in\N^n:\vert v\vert =i} x_{n+1}^{d-i} a_v\mathbf{x}^v = \sum_0^d x_{n+1}^{d-i}f_i.
    \end{align*}
    Note also $x_{n+1}^{d-i}f_i$ is homogeneous of degree $d-i+i=d$, hence $f^\ast=x_{n+1}^d f_0+x_{n+1}^{d-1}f_1+\dots+f_d$ is homogeneous of degree $d$. Note that $f^\ast = 0$ if and only if $f = 0$. Indeed if $f\neq 0$, Then $x_{n+1}^{d-i}f_i\neq 0$, furthermore any monomial in $x_{n+1}^{d-i}f_i$ is different from any monomial in $x_{n+1}^{d-j} f_j$ since their $x_{n+1}$-degree is $d-i$ resp. $d-j$. 
\end{remark}
We observe the following facts about homogenization and de-homogenization.
\begin{proposition}
    Let $f,g\in R[x_1,\dots,x_n]$ and $F\in R[x_1,\dots,x_{n+1}]$ be homogeneous.
    \begin{enumerate}
        \item $x_{n+1}^{\deg\ f +\deg \ g - \deg( f+g)}(f+g)^\ast=x_{n+1}^{\deg\ g}f^\ast + x_{n+1}^{\deg\ g}g^\ast$. Suppose additionally that $R$ is an integral domain. Then $(fg)^\ast=f^\ast g^\ast$.
        \item $(fg)_{\ast,i} = f_{\ast,i} g_{\ast,i}$ \& $(f+g)_{\ast,i} = F_{\ast,i}+G_{\ast,i}$. 
        \item $(f^\ast)_\ast = f$.
        \item Let $r:=\max\left(\left\{ j\geq0: x_{n+1}^j\mid F \right\}\right).$ Then $x_{n+1}^r(F_\ast)^\ast=F$
    \end{enumerate}
\end{proposition}
\begin{proof}
    1. For the first identity one finds that 
    \begin{align*}
        x_{n+1}^{\deg\ f +\deg \ g - \deg( f+g)}(f+g)^\ast &=x_{n+1}^{\deg\ f +\deg \ g - \deg( f+g)+\deg(f+g)}\left[f\left(\frac{x_1}{x_{n+1}},\dots,\frac{x_n}{x_{n+1}}\right) + g\left(\frac{x_1}{x_{n+1}},\dots,\frac{x_n}{x_{n+1}}\right)\right]\\ &= x_{n+1}^{\deg\ g}\left[x_{n+1}^{\deg\ f}f\left(\frac{x_1}{x_{n+1}},\dots,\frac{x_n}{x_{n+1}}\right)\right] + x_{n+1}^{\deg\ f}\left[x_{n+1}^{\deg\ g}g\left(\frac{x_1}{x_{n+1}},\dots,\frac{x_n}{x_{n+1}}\right)\right]\\
        &= x_{n+1}^{\deg\ g}f^\ast+x_{n+1}^{\deg f}g^\ast.
    \end{align*}
    For the second see that
    \begin{align*}
        \left(fg\right)^\ast &= x_{n+1}^{\deg\ f+ \deg \ g} f\left(\frac{x_1}{x_{n+1}},\dots,\frac{x_n}{x_{n+1}}\right)g\left(\frac{x_1}{x_{n+1}},\dots,\frac{x_n}{x_{n+1}}\right)\\  &= x_{n+1}^{\deg \ f}f\left(\frac{x_1}{x_{n+1}},\dots,\frac{x_n}{x_{n+1}}\right)x_{n+1}^{\deg\ g}g\left(\frac{x_1}{x_{n+1}},\dots,\frac{x_n}{x_{n+1}}\right)=f^\ast g^\ast.
    \end{align*}
    2. This follows from evaluation being a ring homomorphism.\\
    3. Indeed
    $$(f^\ast)_\ast = \left(\sum_0^d x_{n+1}^{d-i}f_i\right)_\ast= \sum_0^df_i =f.$$
    4. Write $F=\sum_{v\in\N^{n+1}} a_v\mathbf{x}^v$. Note that if $x_{n+1}^j\mid F$, then $F= x_{n+1}^j Q$ for some $Q\in R[x_1,\dots,x_{n+1}]$. By Lemma~\ref{ProductAndSumOfHomogeneousPolynomials} 1. $Q$ is homogeneous of degree $d-j$ where $d := \deg \ F$. Then $x_{n+1}^j$ divides every term $F$. Set 
    $$d' = \deg \ F(x_1,\dots,x_n,1) = \max_{v\in\N^{n+1} : a_v\neq 0}\ \sum_1^n v_i = \max_{v\in\N^{n+1}:a_v\neq0} d-v_{n+1}.$$
    For some $w\in\N^{n+1}$ with $a_w\neq0$, $w_{n+1} = r$. Then $d' = d-w_{n+1}=d-r$, since if there were a $u\in \N^{n+1}$ with $a_u\neq0 $ and $u_{n+1}<r$ then $x_{n+1}^r\nmid a_ux_{n+1}^{u_{n+1}}.$ As $F$ is homogeneous $d = \sum_1^{n+1} v_i$ for every $v\in \N^n$ with $a_v\neq0$, hence $d' = -r+\sum_1^{n+1}v_i$. We get
    \begin{align*}
        x_{n+1}^r(F_\ast)^\ast &= \sum_{v\in \N^{n}} a_v x_1^{v_1}\cdots x_{n}^{v_n}x_{n+1}^{d'+r-\sum_0^n v_i}\\
        &=\sum_{v\in \N^{n}} a_v x_1^{v_1}\cdots x_{n}^{v_n}x_{n+1}^{\sum_1^{n+1}v_i -r+r-\sum_0^n v_i}\\ 
        &= \sum_{v\in \N^{n}} a_v x_1^{v_1}\cdots x_{n}^{v_n}x_{n+1}^{v_{n+1}}=F.
    \end{align*}
\end{proof}
\begin{corollary}\label{LinearFactoringOfForms}
    Suppose $R$ is an integral and consider $F\in R[x_1,\dots,x_{n+1}]$ a homogeneous polynomial. Then a factorization of $F$ determines a factorization of $F_\ast$ up to a factor $x_{n+1}^r$. If $R=K$ an algebraically closed field and $F\in K[x,y]$ then $F$ factors into a product of linear factors.  
\end{corollary}
\begin{proof}
    Indeed, $F=x_{n+1}^rQ$ for some homogeneous $Q\in R[x_1,\dots,x_n,x_{n+1}]$. Then suppose $Q=q_1\cdots q_l$ for some $q_1,\dots,q_l\in R[x_1,\dots,x_n,x_{n+1}]$. Then $F_\ast = {q_1}_\ast \cdots {q_l}_\ast$. Conversely, if $F_\ast=q_1\cdots q_l$, then $F=x_{n+1}^\ast q_1^\ast\cdots q_l^\ast$.\\
    For the second statement we can again write $F=y^rQ$ for some homogeneous $Q\in K[x,y]$. Then $Q_\ast = a\prod_1^l(x-a_i)^{r_i}$ for some $a,a_1,\dots,a_l\in K$ where $a\neq0$. Then $F=y^r(F_\ast)^\ast = ay^r\prod_1^l (x-a_iy)^{r_i}$.
\end{proof}
\begin{proposition}\label{SumsOfCoprimeHomogeneousPolynomialOfConsecutiveDegree}
    Let $R$ be an integral domain. Consider $f,g\in R[x_1,\dots,x_n]$ homogeneous of degree $d$ respectively degree $d+1$ with $\gcd(f,g)=1$. Then $f+g$ is irreducible.
\end{proposition}
\begin{proof}
    We proof that if $f+g$ is reducible, then $f$ and $g$ has common factor. Let $a,b\in R[\mathbf{x}]$ with $\deg \ a, \deg \ b>1$ such that $f+g=ab$. We can write $a = \sum_m^M a_m$ and $b=\sum_l^L b_j$ where $m,l>1$ and $a_m,\dots,a_M,b_l,\dots,b_L\in R[\mathbf{x}]$ are homogeneous with degree being the index such that $a_m,a_M,b_l,b_L\neq 0$. Note that $d= \deg\ a_mb_l = m+l$. Note also that $d+1=\deg\ a_Mb_L = L+M$, hence (WLOG) $L=l+1$ and $M=m$. We thus find that $ab=a_mb_l+a_mb_{l+1}$. Then $f=a_mb_l$ and $g=a_mb_{l+1}$, hence $f,g$ has a common factor $a_m$. 
\end{proof}
\begin{definition}\label{(De)HomogenizationOfIdeals}
    Let $I\subset R[x_1,\dots,x_{n+1}$ and $J\subset R[x_1,\dots,x_n]$. We define the \textit{dehomogenization of $I$ at $i\in\{1,\dots,n+1\}$} and the \textit{homogenization of $J$} to be 
    $$I_{\ast,i} = \{ f_{\ast,i} : f\in I\} \text{ resp. } J^\ast := \langle\{f^\ast : f \in J\}\rangle \subset R[x_1,\dots,x_{n+1}].$$
    We furthermore define $I_\ast := I_{\ast, n+1}$
\end{definition}
\begin{lemma}
    $I_{\ast,i}$ is an ideal. $I=\langle f_1,\dots,f_m\rangle$, then $I_{\ast,i} = \langle (f_1)_{\ast,i},\dots, (f_m)_{\ast,i}\rangle$. If $J=\langle f\rangle$, then $J^\ast = \langle f^\ast\rangle$. 
\end{lemma}
\begin{proof}
    The first statement is a trivial consequence of evaluation being a ring homomorphism. The second statement is a matter of checking the definition.
\end{proof}
\begin{example}\label{HomogenizationPathology}
    Consider $I:=\langle y-x^2,z-x^3\rangle R[x,y,z]$, Note that $f:=z-xy=z-x^3-x(y-x^2)\in I$, hence $f^\ast = zw-xy \in I^\ast$, however $f^\ast \notin J:= \langle (y-x^2)^\ast,(z-x^3)^\ast\rangle =\langle yw-x^2,zw^2\rangle$, since any term containing $z$ in a polynomial in $J$ has $w$-degree $\geq 2$ or $y$-degree $\geq 1$ or $x$-degree $\geq 2$, therefor no polynomial in $J$ can contain the term $zw$. We thus see that for a finitely generated ideal $I=\langle f_1,\dots,f_m\rangle$, while trivially $\langle f_1^\ast,\dots,f_m^\ast \rangle \subset I^\ast$, it is not necessarily the case that this holds with equality.
\end{example}
\begin{lemma}\label{SimpleBinomialLemma}
    For every $n\geq 1$, $m\geq 1$
    $$\sum_0^m {k +n \choose n} = {m+n+1\choose n+1}$$
\end{lemma}
\begin{proof}
    One readily verifies the $m=1$ case. By induction we get that
    $$\sum_0^{d+1} {k+n\choose n} = {m+n+1\choose n+1}+ {m+n +1\choose n} = {(m+1)+n+1\choose n+1}.$$
\end{proof}
\begin{lemma}\label{NumberOfWaysOfPickingNnumbsersSummingToD}
    For every $n\geq 1$, $d\geq 0$, the set  
    $$\Delta_{n,d}:=\left\{v\in \N^n : \sum_1^n v_i=d\right\}.$$
    is of size ${d+n-1\choose n-1}$.
\end{lemma}
\begin{proof}
    Fix $n\geq 1$. In the case $d=0$, then clearly $\Delta_{n,0} = \{\textbf{0}\}$, hence $\#\Delta =1={0+n-1\choose n-1}.$
    Suppose the statement is true for some $d\geq 0$. Then for $n=1$, we see that $\#\Delta_{1,d+1} = 1 = { d +1 +(1-1) \choose 0}$. So for arbitrary $n\geq 1$,
    $$\Delta_{n+1,d+1} =\bigcup_0^{d+1} \Delta_j,$$
    where 
    $$\Delta_j := \left\{ v\in \N^{n+1}: v_{n} = j, \sum_0^{n+1} v_i = d+1 \right\}.$$
    Note that these are pairwise disjoint sets and that each for $j$, $\Delta_j$ is in bijection with 
    $$\left\{ v\in \N^{n} : \sum_1^n v_i = d+1-j \right\},$$
    hence by induction $\#\Delta_j = {d+1-j+n-1 \choose n-1}$ for $j=0,\dots, d+1$. We thus have that 
    $$\#\Delta_{n+1,d+1} = \sum_0^{d+1} \#\Delta_j = \sum_0^{d+1} {j+n-1 \choose n-1} = {d+n+1 \choose n} = {(d+1)+(n+1)-1\choose (n+1)-1}.$$
\end{proof}
\begin{proposition}\label{DimensionOfVectorSpaceOfHomogeneousPolynomialsOfDegreeDInNVariables}
    For a field $K$, the dimension of $V_K(d,n)$ is ${d+n-1 \choose n-1}$.
\end{proposition}
\begin{proof}
    With the notation of the above lemma $\{ \mathbf{x}^v \in K[x_1,\dots,x_n] : v\in \Delta_{n,d}\}$ forms a basis of $V_K(d,n)$, hence by said lemma
    $$\dim_K \ V_K(d,n) = \#\Delta_{n,d}= {d+n-1 \choose n-1}$$
\end{proof}
\begin{example}
    $\dim\ V_K(d,1)=1$, $\dim\ V_K(d,2)=d+1$, $\dim\ V_K(d,3) = {d+2\choose 2} = \frac{(d+2)(d+1)}{2}$
\end{example}
\begin{proposition}\label{DimensionOfVectorSpaceOfPolynomialsInNVariablesOfAtMostDegreeD}
    For each $n\geq 1$, $d\geq 0$, 
    $$\dim\ K_{\leq d}[x_1,\dots,x_n] = {d+n\choose d}$$
\end{proposition}
\begin{proof}
    One readily verifies that $K_{\leq d}[x_1,\dots,x_n] = \sum_0^d V_K(d,n)$, hence by Proposition~\ref{DimensionOfVectorSpaceOfHomogeneousPolynomialsOfDegreeDInNVariables} and Lemma~\ref{SimpleBinomialLemma}.
    $$\dim\ K_{\leq d}[x_1,\dots,x_n]=\sum_0^d \dim\ V_K(d,n)=\sum_0^d \#\Delta_{n,j} = \sum_0^d {j+n-1 \choose n-1}={d+n\choose n} = \frac{(d+n)!}{d!n!}={d+n\choose d}.$$
\end{proof}
\begin{example}\label{ImportantExamplePolynomialVectorSpaceDimension}
    We in particular get for $d\geq 1$ that $\dim\ K_{\leq d-1}[x,y] = {d+1\choose d-1} = \frac{(d+1)!}{(d+1-d+1)!(d-1)!}=\frac{d(d+1)}{2}=\sum_1^di.$
\end{example}
\begin{definition}
    Let $K$ be any field and $f\in K[x_1,\dots,x_n]\setminus0 $. A point $[v]\in \Pp^n$ is said to be a \textit{zero of $f$} if $f(\lambda v)=0$ for every $\lambda\in K\setminus 0$. We thus write $f([v])=0$.
\end{definition}
\begin{remark}
    For a fixed $[v]\in\Pp^n$ we thus get a well-defined evaluation function on the space of polynomials for which $[v]$ is a zero, mapping to $0$. If $f$ is homogeneous of degree $d$, then if $v\in K\setminus 0$ is a zero of $f$, $[v]$ is a zero of $f$. Indeed, for any non-zero $\lambda\in K\setminus 0$ and an $s=(s_1,\dots,s_{n+1})\in S^{n+1}$ where $S\supset K$ is a $K$-algebra. Then 
    $$f(\lambda s)=\sum_{v\in\N^n} a_v\lambda^{v_1}s_1^{v_1}\cdots \lambda^{v_{n+1}}s_{n+1}^{v_{n+1}}= \lambda^{\sum_1^{n+1} v_i}\sum_{v\in\N^n} a_v s_1^{v_1}\cdots s_{n+1}^{v_{n+1}}=\lambda^df(s).$$
    In particular, if $v$ is a zero of $f$, then 
    $$f(\lambda v)=\lambda^df(v)=0\implies f([v])=0.$$
    Note that if $[v]\in\Pp^n$ is a zero of $f,g$ then $(f+g)([v])=f([v])+g([v])=0$ and $(fg)([v])=f([v])g([v])=0$, hence $[v]$ is a zero of $f+g$ and $fg$.
\end{remark}
\begin{lemma}\label{FormsInFormDecompVanish}
    Let $K$ be an infinite field. Consider $f=\sum_0^d f_i\in K[x_1,\dots,x_{n+1}]$ where $f_i$ is homogeneous of degree $i$. Let $[v]\in \Pp^n$ be a zero of $f$. Then $[v]$ is a zero of $f_i$ for each $i$. 
\end{lemma}
\begin{proof}
    Fix $v\in [v]$ and consider 
    $$g:= f(tv_1,\dots, tv_{n+1})=\sum_0^f t^if_i(v)\in K[t].$$
    Then $g(\lambda)=0$ for every $\lambda \in K\setminus 0$, hence $g=0$. This implies that $f_i(tv_1,\dots,tv_{n+1})=t^{i}f_i(v)=0$ for each $i$, meaning $f_i(\lambda v)=0$ for each $\lambda \in K\setminus 0$. We thus conclude that $f_i([v])=0$. 
\end{proof}
\begin{definition}
    Let $R$ be any commutative ring. An ideal $I\subset R[x_1,\dots,x_n]$ is called \textit{homogeneous} if for every $f=\sum_0^d f_i\in R[\mathbf{x}]$ where $f_i$ is homogeneous of degree $i$, then $f_i\in I$.
\end{definition}
\begin{lemma}\label{HomogeneousIdealsAreGeneratedByForms}
    For a commutative ring $R$ and a finitely generated $I\subset R[x_1,\dots,x_n]$,  $I$ is a homogeneous if and only if $I$ is finitely generated by a finite set of homogeneous polynomials. 
\end{lemma}
\begin{proof}
    "$\implies$":
    Write $I=\langle f_1,\dots,f_m\rangle$ and $f_i=\sum_0^{d_i} f_{ij}$ with $f_ij$ being homogeneous of degree $j$. Then $I=\langle \{f_{ij} \}\rangle$. Indeed $I\subset \langle \{f_{ij} \}\rangle$ obviously and $\langle \{f_{ij} \}\rangle\subset I$ since $f_{ij}\in I$ by the assumption that $I$ is homogeneous.\\ 
    "$\impliedby$": Suppose $I=\langle F_1,\dots,F_m\rangle$ for homogeneous $F_i$ of degree $d_i$. Write $f=\sum_0^d f_i\in I$ with $f_i$ homogeneous of degree $i$. Write also $f= \sum_1^m \lambda_iF_i$ for $\lambda_1,\dots,\lambda_m\in K[\mathbf{x}]$. If we consider $\lambda_i=\sum_0^{\delta_i} \lambda_{ij}$ with $\lambda_{ij}$ homogeneous of degree $j$. Then $f_d= \sum_1^m \lambda_{i,d-d_i}F_i\in I$. By induction in number of non-zero homogeneous $f_i$, it follows that $f_i\in I$, hence $I$ is homogeneous. 
\end{proof}
\begin{remark}
    Note that it's very clear that an ideal is homogeneous if and only if it is (not necessarily finitely) generated by a set of homogeneous polynomials. We therefor note that the homogenization of an ideal is a homogeneous ideal.
\end{remark}
\begin{lemma}\label{CheckingHomogeneousIdealPrimeSufficesToCheckForms}
    Let $I\subset R[x_1,\dots,x_n]$ be a homogeneous ideal. Then $I$ is prime if and only if $fg\in I$ implies $f\in I$ or $g\in I$ for every form $f,g\in R[\mathbf{x}]$. 
\end{lemma}
\begin{proof}
    "$\implies$": This follows from the definition of prime ideals.\\
    "$\impliedby$": Let $\lambda,\mu\in R[\mathbf{x}]$ such that $\lambda\mu\in I$. Write $\lambda=\sum_0^{d} \lambda_i$ and $\mu = \sum_0^e \mu_i$. Then $\lambda\mu= \sum_{i,j} \lambda_i\mu_j$. Since $I$ is homogeneous $\lambda_d\mu_e\in I$, hence $\lambda_d\in I$ for $\mu_e\in I$. Suppose $\lambda_d\in I$. Then $(\lambda-\lambda_d)\mu \in I$. By induction in the degree of $\lambda \mu$ it follows that $\lambda \in I$ or $\mu\in I$.
\end{proof} 
\begin{lemma}\label{HomogenizationOfPrimeIdealIsPrime}
    If $I\subset R[x_1,\dots,x_n]$ is prime, then $I^\ast\subset R[x_1,\dots,x_{n+1}]$ is prime
\end{lemma}
\begin{proof}
    Let $a,b\in R[x_1,\dots,x_{n+1}]$ such that $ab\in I^\ast$. Then $a_\ast b_\ast = (ab)_\ast \in I$, hence $a_\ast \in I$ or $b_\ast\in I$. WLOG $a_\ast \in I$. Then $(a_\ast)^\ast\in I^\ast$, meaning for a suitable $r\geq 0$, $a=x_{n+1}^r(a_\ast)^\ast \in I^\ast$.
\end{proof}
\begin{proposition}
    If $I\subset R[x_1,\dots,x_n]$ is homogeneous, then $\rad(I)$ is homogeneous.  
\end{proposition}
\begin{proof}
    Let $f=\sum_0^d f_i \in \rad(I)$. We must prove that $f_i\in \rad(I)$. Note that $f^n = f_d^n + r\in I$ where $\deg \ r < dn$. Then $f_d^n\in I$, hence $f_d\in \rad(I)$. We thus have that $f - f_d \in \rad(I)$ by induction degree it follows that $f_i\in \rad(I)$ for the remaining $i$.
\end{proof}

\begin{lemma}
    If $\{I_\alpha\}_{\alpha\in A}$ is a family of homogeneous ideals in $R[x_1,\dots,x_n]$, then so is $\sum_{\alpha\in A} I_\alpha$ and $\bigcap_{\alpha} I_\alpha$.  
\end{lemma}
\begin{proof}
    Indeed, for $(f_\alpha)\in \bigoplus_{\alpha\in A} I_\alpha$, we may for some $d\geq 0$ write $(f_\alpha)=(\sum_0^d f_{\alpha,i})$, where $f_{\alpha,i}$ is homogeneous of degree $i$ for each $i$ and $\alpha$. Then 
    $$\sum_{\alpha\in A} f_\alpha = \sum_0^d \sum_{\alpha\in A} f_{\alpha,i}.$$
    Note that since each $I_\alpha$ is homogeneous $f_{\alpha,i}\in I_\alpha$. Then $\sum_{\alpha\in A} f_{\alpha,i}\in \sum_{\alpha\in A} I_\alpha$, which means $\sum_{\alpha\in A} I_\alpha$ is homogeneous.\\
    Consider $f=\sum_0^d f_i\in \bigcap_{\alpha\in A} I_\alpha$. Then for each $\alpha \in A$, $f\in I_\alpha$, hence $f_i\in I_\alpha$. We thus have that $f_i\in \bigcap_{\alpha\in A} I_\alpha$, which means $\bigcap_{\alpha \in A} I_\alpha$ is homogeneous.   
\end{proof}
\begin{lemma}
    Let $I,J\subset R[x_1,\dots,x_n]$ be homogeneous ideals. Then $IJ$ is homogeneous.
\end{lemma}
\begin{proof}
    Let $f=\sum_0^d f_i\in I$ and $g=\sum_0^e g_j\in J$. Then each $f_i\in I$ and each $g_j\in J$, meaning $f_ig_j\in IJ$ for each $0\leq i\leq d$ and $0\leq j\leq e$. Then since $fg = \sum_{k=0}^{d+e}\sum_{i,j:i+j= k} f_ig_j$ where $\sum_{i,j:i+j= k} f_ig_j$ is a homogeneous polynomial of degree $k$ for each $0\leq k\leq d+e$ we get that $IJ$ is homogeneous.
\end{proof}
\begin{definition}
    Let $I\subset R[x_1,\dots,x_n]$ be a homogeneous ideal. An element $\alpha\in R[\mathbf{x}]/I$ is called \textit{homogeneous of degree $d$} if there is a homogeneous polynomial of degree $d$, $f\in R[\mathbf{x}]$, such that $\alpha = f + I$. 
\end{definition}
\begin{lemma}
    Let $I\subset R[x_1,\dots,x_n]$ be a homogeneous polynomial. Let $\alpha \in R[\mathbf{x}]/I$. Then for some unique $d\geq 0$, there are unique $\alpha_i\in R[\mathbf{x}]/I$, $i\in\{0,\dots,d\}$ homogeneous of degree $i$ such that 
    $$\alpha = \sum_0^d \alpha_i.$$
\end{lemma}
\begin{proof}
    \textbf{Existence:} Let $f+I\in R[\mathbf{x}]$, then $f = \sum_0^d f_i$ where $f_i$ is homogeneous of degree $i$ for each $i$. Then we are done picking $\alpha_i := f_i+ I$.
    \textbf{Uniqueness:} Suppose we are given two such representations $\sum_0^d (f_i+I) = \sum_0^d (g_i +I)$ (we can always let $d$ be the largest of the two degrees obtained from each respective representation and then set undefined forms to be equal to $0$). Then $f_i-g_i$ is a form of degree $i$ for each $i$, hence $f_i-g_i\in I$ using the fact that $I$ is homogeneous. Consequently $f_i+I=g_i+I$ for each $i$. 
\end{proof}
\begin{remark}\label{VectorSpaceOfForms}
    Consider $V_R(d,n,I)=\{ \alpha \in R[\mathbf{x}]/I : \alpha \text{ homogeneous of degree } d\}$ is an $R$-submodule of $R[\mathbf{x}]/I$ finitely generated by $\{\mathbf{x}^v + I:\vert v\vert =d\}.$ In particular $V_K(d,n,I)$ is a finite dimensional vector space for fields $K$. In general, it takes some work to say anything about the dimension of this vector space, below we give an example in the case $n=3$ and $d>n$ {\Large This is exercise 4.10}. 
\end{remark}
\begin{example}
    
\end{example}
\begin{lemma}\label{TechnicalLemmaForNoetherLemma}
    Let $f\in K[x_1,\dots,x_n]$ be a non-zero form of degree $d$. Then $f_\ast$ is non-zero.
\end{lemma}
\begin{proof}
    $0\neq f = x_{n+1}^r(f_\ast)^\ast$ for some $r\geq 0$, hence $(f_\ast)^\ast \neq 0$, so by Remark~\ref{AnAlternativeDefinitionAndPreimageOfHomogenizationOfZeroIsZero}, $f_\ast \neq 0$.     
\end{proof}
\subsubsection{Multi- and Bihomogeneous Polynomials}
    \begin{definition}
        Let $\{x_{ij} : 1\leq i\leq m, 1\leq j \leq n_i\}$ be algebraically independent variables over a ring $R$. A polynomial in $R[\mathbf{x}]$ is called an \textit{$m$-homogeneous polynomial} or an \textit{$m$-form} of \textit{$m$-degree} $(d_1,\dots,d_m)\in \Z_{\geq 0}^m$, if it is form of degree $d_i$ when seen as an element in $R[x_{kj} : k\neq i][x_{i1},\dots,x_{in_i}]$ for each $i$. When $m=2$ an $2$-form is called a \textit{bihomogeneous polynomial} or a \textit{biform} of \textit{bidegree} $(d_1,d_2)$.
    \end{definition}
    \begin{lemma}
        When the same notation as above a polynomial $f \in K[\mathbf{x}]\setminus 0$ of degree $d$ has unique decomposition
        $$f = \sum_{(i_1,\dots,i_m): \sum_1^m i_j \leq d} f_{i_1,\dots,i_m},$$
        where $f_{i_1,\dots,i_m}$ is an $m$-form of $m$-degree $(i_1,\dots,i_m)$ such that for some $(i_1,\dots,i_m)$ with $\sum_1^m i_j = d$, $f_{i_1,\dots,i_m}\neq 0$. 
    \end{lemma}
    \begin{proof}
        Write 
        $$f = \sum_{\substack{(i_1,\dots,i_m):\sum_1^m i_j\leq d}} 
        \underbrace{\sum_{\substack{\mathbf{v}\in\prod_1^m \N^{n_k} :\\ \forall k, \sum v_{kj} = i_k}} a_\mathbf{v} \mathbf{x_1}^{v_1}\cdots \mathbf{x_m}^{v_m}}_{f_{i_1,\dots,i_m}},$$
        each monomial in $f_{i_1,\dots,i_m}$ is a homogeneous of degree $i_k$ in $R[x_{kj} : k\neq i][x_{i1},\dots,x_{in_i}]$ for each $k$, hence $f_{i_1,\dots,i_m}$ is $m$-homogeneous of $m$-degree $(i_1,\dots,i_m)$. Uniqueness follows from $\{\mathbf{x}^\mathbf{v}\}$ being linearly independent over $R$.
    \end{proof}
    \begin{definition}
        For an $f\in R[\mathbf{x_1},\dots,\mathbf{x_m}]$ we say that $([v_1],\dots,[v_m])\in \Pp^{n_1}\times \Pp^{n_m}$ is \textit{a zero of $f$} if for every $(\lambda_1,\dots,\lambda_m)\in K^m$, $\lambda_i\neq 0$, $f(\lambda_1v_1,\dots,\lambda_mv_m) = 0$.
    \end{definition}
    \begin{remark}
        If $f$ were an $m$-form of $m$-degree $(d_1,\dots,d_m)$ note that $f(\lambda_1v_1,\dots,\lambda_mv_m)=\left(\prod_1^m \lambda_i^{d_i}\right)f(v_1,\dots,v_m)$. Hence if $(v_1,\dots,v_m)$ is a zero of $f$, then so is $([v_1],\dots,[v_m])$.
    \end{remark}
    \begin{definition}
        An ideal $I\subset R[\mathbf{x_1},\dots,\mathbf{x_m}]$ is called $m$-homogeneous if for each $f=\sum_{i_1,\dots,i_m} f_{i_1,\dots,i_m}\in I$, $f_{i_1,\dots,i_m}\in I$
    \end{definition}
    \begin{remark}\label{ImporatantRemarkForMultiprojectiveSpace}
        The above is equivalent to $I$ being a homogeneous ideal in $R[\mathbf{x_1},\dots, \widehat{\mathbf{x_k}},\dots,\mathbf{x_m}]$ for each $k$. Any result proven about homogeneous ideals therefor naturally generalizes to $m$-homogeneous ideals. 
    \end{remark}
\subsubsection{Differentiation of Polynomials}
\begin{definition}
    We define \textit{differentiation (with respect to $x$)} in polynomial ring $R[x]$ as the $R$-module map
    \begin{gather*}
        D_x : R[x]\rightarrow R[x]
    \end{gather*}
    mapping $1$ to $0$ and $x^n$ to $x^{n-1}$ for $n\geq 1$.For a polynomial $f\in R[x]$, we call the polynomial $D_xf$ the \textit{derivative of $f$ (with respect to $x$)}, which we may denote by $f'$.
\end{definition}
\begin{remark}
    One easily checks that $D_x fg = (D_xf)g+f(D_xg)$, i.e. that is satisfies the Leibniz rule. It also satisfies the chain rule, i.e. 
    $$D_x(f(g))=(D_xg)\cdot(D_xf)(g).$$
    By definition {\large \textbf{over an integral domain of characteristic $0$}}, $\deg \ f' = \deg \ f -1$ when $\deg \ f\geq 1$. In positive characteristic this may not be the case. For instance, when $\Char\ R = p >0$, we get that $D_x x^p= px^{p-1}=0$. We can also come up with pathological examples over commutative rings of characteristic $0$. Indeed take $R=\Z/2\Z \times \Z$ with the usual structure of product ring. Then $D_x (1,0)x^2= (2,2)(1,0)x= (0,2)(1,0)x=0.$ 
\end{remark}
\begin{definition}
    In a polynomial ring $R[x_1,\dots,x_n]$, \textit{the partial derivative of $f$ with respect to $x_i$} is the polynomial $D_{x_i}f$, where 
    $$D_{x_i}: R[x_1,\dots,x_n]=R[x_1,\dots, x_{i-1},\widehat{x_i},x_{i+1},x_{n}][x_i]\rightarrow R[x_1,\dots,x_n]=R[x_1,\dots, x_{i-1},\widehat{x_i},x_{i+1},x_{n}][x_i],$$
'    is differentiation with respect to $x_i$. We sometimes denote it by $\frac{\partial f}{\partial x_i}:= \frac{\partial}{\partial x_i}f:= f_{x_i}$.
\end{definition}
\begin{lemma}\label{TranslationAndPartialDerivativesCommute}
    Let $T$ be translation of one variable, $x_1$ say, by some element $a\in K$. Then $D_{x_{i}}T=TD_{x_i}$. Hence in general translation commutes with partial derivatives.  
\end{lemma}
\begin{proof}
    Indeed,  
    $$(D_{x_1}T)f = D_{x_1}f(x_1+a,x_2,\dots,x_n) = (D_{x_1}(x_1+a))\cdot (D_{x_1}f)(x_1+a,x_2,\dots,x_n) =(TD_{x_1})f.$$
    (Here we implicitly use that differentiation commutes with permutation of variables).
\end{proof}
\begin{lemma}\label{EulersTheorem}(Euler's theorem)
    Let $R$ be an integral domain, $f\in R[x_1,\dots,x_n]\setminus0 $ be homogeneous of degree $d>0$. Then
    $$\sum_1^n x_i\frac{\partial f}{\partial x_i}=df.$$
\end{lemma}
\begin{proof}
    Let $a_v\mathbf{x}^v$ be a term of $f$. Then
    $$\frac{\partial }{\partial x_i} a_v\mathbf{x}^v = \begin{cases}
        v_ia_v\mathbf{x}^{v-e_i} & \mathrm{if} \ v_i>0\\
        0 & \mathrm{otherwise}
    \end{cases}$$
    $$\sum_1^n x_i\frac{\partial }{\partial x_i} a_v\mathbf{x}^v = \sum_{i: v_i>0} v_ia_vx_i \mathbf{x}^{v-e_i} = \left(\sum_{i : v_i>0} v_i\right) a_vx^{v} = da_v\mathbf{x}^v.$$
    It thus follows that 
    $$\sum_1^n x_i\frac{\partial f}{\partial x_i}= \sum_1^n x_i\sum_{v\in\N^n}\frac{\partial}{\partial x_i}a_v\mathbf{x}^v = \sum_{v\in\N^n} \sum_1^n x_i\frac{\partial}{\partial x_i}a_v\mathbf{x}^v=d\sum_{v\in\N^n} a_v\mathbf{x}^v=d f. $$
\end{proof}
We provide a lemma which alleviate some ugly cases in characteristic $p>0$ cases
\begin{lemma}\label{UsefulPartialDerivativeLemmaForPositiveCharacteristic}
    Let $R$ be an integral domain of characteristic $p>0$. Let $f\in R[x_1,\dots,x_n]$ be a non-constant polynomial such that $f_{x_i}=0$ for each $i$. Then $f=g(x_1^p,\dots,x_n^p)$ for some $g$. 
\end{lemma}
\begin{proof}
    For any $k\in \Z$, let $k_R$ denote the image of $k$ in $R$. Consider the case where $n=1$. Write $f=\sum_0^d a_ix^i$. Then $0=f_x = \sum_1^{d} i_Ra_ix^{i-1}.$ For each $i$, we then get that $i_Ra_i=0$, hence $a_i=0$ or $i_R=0$. If $i < p$, then $i_R\neq 0$, hence $a_i =0$. We furthermore have that if $a_i\neq 0$ then $p\mid i$ It thus follows that $f = \sum_{1}^k a_{jp} x^{jp}$, where $k$ is chosen such that $d = kp$. Hence picking $g = \sum_1^k a_{jp} x^{j}$ we are done. We prove the general case by induction using the fact that $\Char\ R[x_1,\dots,x_n] = p$, $R[x_1,\dots,x_{n+1}]\simeq R[x_1,\dots,x_n][x_{n+1}]$ in conjunction with the validity of the $1$-variable case.   
\end{proof}
\subsection{Ring Extensions and Algebras over Rings}
We proceed with considerations of the theory of algebras over rings in conjunction with some of the theory modules already developed. We could have noted this earlier, but it is more relevant to note it now. 
\subsubsection{Finitely Generated Ring Extensions}
\begin{definition}
    A ring extension $S\supset R$ is said to be \textit{module-finite (over $R$)} if $S$ is finitely generated as an $R$-module.
\end{definition}
\begin{definition}
    A ring extension $S\supset R$ is a said to be \textit{finitely generated $R$-algebra} or \textit{ring-finite} if $S=R[s_1,\dots,s_n]$ for suitable $s_1,\dots,s_n\in S$.
\end{definition}
\begin{proposition}\label{FinitelyGeneratedAlgebrasArePolynomialQuotientRings}
    Let $S\supset R$ be a ring-finite ring extension. Then
    $$S\simeq R[x_1,\dots,x_n]/I,$$
    for some $n\geq 1$ and some ideal $I\subset R[x_1,\dots,x_n]$.
\end{proposition}
\begin{proof}
    For suitable $s_1,\dots,s_n\in S$, $S=R[s_1,\dots,s_n]$, hence $\ev_{s_1,\dots,s_n} : R[x_1,\dots,x_n]\rightarrow S$ is a surjective $R$-algebra homomorphism. Then by the first isomorphism theorem $S\simeq K[\mathbf{x}]/\ker \ \ev_{s_1,\dots,s_n}$. 
\end{proof}
\begin{proposition}\label{ModuleFiniteImpliesRingFinite}
    Let $S\supset R$ is a ring extension that is also a finitely generated $R$-module. Then $S$ is a finitely generated $R$-algebra.
\end{proposition}
\begin{proof}
    For suitable $s_1,\dots,s_n\in S$, $S = \sum_1^n Rs_i$. We prove that $S = R[s_1,\dots,s_n]$. We already know that $S\supset R[s_1,\dots,s_n]$. Let $s\in S$. Then $s = \sum_1^n r_is_i$ for suitable $r_1,\dots,r_n \in R$, hence $s\in R[s_1,\dots,s_n]$. 
\end{proof}
\begin{example}
    The converse implication of the above proposition is clearly not true. Consider for instance $R[x_1,\dots,x_n]\supset R$. given $f_1,\dots,f_m\in R[\mathbf{x}]$. If these are all $0$, clearly $R[\mathbf{x}]\supsetneq \sum_1^m Rf_i$. Otherwise putting $D = \max_{i\in\{1,\dots,m\}}\ \{\deg\ f_i \}$, we see that for $r_1,\dots, r_m\in R$, 
    $$\deg \ \sum_1^m r_if_i \leq D < D+1 = \deg \ x_1^{D+1},$$
    hence $x_1^{D+1}\notin \sum_1^m Rf_i$, hence $R[\mathbf{x}]\supsetneq \sum_1^m Rf_i.$
\end{example}
\begin{definition}
    A ring extension $L\supset K$ is called a \textit{field extension (over $K$)} if both $L$ and $K$ are fields.
\end{definition}
Let $S\supset R$ be a ring extension where $S$ is an integral domain. For $s_1,\dots,s_n\in S$, $R[s_1,\dots,s_n]$ is also an integral domain. We denote the fraction field of $R[s_1,\dots,s_n]$ by $R(s_1,\dots,s_n)$
\begin{definition}
    A field extension $L\supset K$ is said to be \textit{finite}, if there exist $a_1,\dots,a_n$ such that $L = K(a_1,\dots,a_n)$.
\end{definition}
\begin{lemma}
    Let $K$ be a field. Consider $K[x_1,\dots,x_n]$ as a subring of $K[\mathbf{x},y_1,\dots,y_m]$ Then $K(x_1,\dots,x_n)(y_1,\dots,y_m) = K(x_1,\dots,x_n,y_1,\dots,y_m).$
\end{lemma}
\begin{proof}
    Clearly $K(\mathbf{x})\subset K(\mathbf{x},\mathbf{y})$ and one easily verifies that this is a subfield of $K(\mathbf{x},\mathbf{y})$. To be very precise, this means $K(\mathbf{x},\mathbf{y})\supset K(\mathbf{x})$ is a ring extension. Hence $K(\mathbf{x},\mathbf{y})\supset K(\mathbf{x})[\mathbf{y}]$. This means
    $K(\mathbf{x},\mathbf{y})= Q(K(\mathbf{x},\mathbf{y}))\supset Q(K(\mathbf{x})[\mathbf{y}])=K(\mathbf{x})(\mathbf{y})$.
\end{proof}
\begin{remark}
    Of course this statement {\Large What?}
\end{remark}
\begin{lemma}\label{CriterionForCheckingMembershipInAlgebra}
    Consider $L := K(x_1,\dots,x_n)$ and $R:= K\left[\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right]$ for $\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\in K(\mathbf{x})$. Then there is a $b\in K[\mathbf{x}]$ such that $b^dz\in K[\mathbf{x}]$ for every $z\in R$ for some $d\geq 0$.
\end{lemma}
\begin{proof}
    If $\lcm(b_1,\dots,b_m)=1$ the statement is trivial. Set $b := \lcm(b_1,\dots,b_m)$ and assume $\deg b > 0$. Let $z\in R$. If $z=0$, then $b^0z\in K[\mathbf{x}]$. Suppose $z\neq 0$. For some $f\in K[y_1,\dots,y_m]\setminus 0$, $z = f\left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right)$. Set $d := \deg \ f$. Let $v\in \N^m$ with $\vert v \vert \leq d$. Then 
    $$\prod_1^m b_i^{v_i}\mid \prod_1^m b^{v_i}= b^{\vert v\vert } \ \mathrm{and}\ b^{\vert v\vert} \mid b^d \implies \prod_1^m b_i^{v_i}\mid b^d \implies b^d\prod_1^m \left(\frac{a_i}{b_i}\right)^{v_i} \in K[\mathbf{x}]\implies b^dz\in K[\mathbf{x}].$$
\end{proof}
\begin{proposition}\label{PolynomialQuotientFieldIsFieldFiniteNotRingFinite}
    Consider $L:= K(x_1,\dots,x_n) \supset K$. Then $L$ is a finite field extension over $K$, but not a finitely generated $K$-algebra.
\end{proposition}
\begin{proof}
    The first statement is obvious as $K(x_1,\dots,x_n)$ is finitely generated as field extension over $K$ by $x_1,\dots,x_n\in K(x_1,\dots,x_n)$. To prove the second statement, let $\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\in K(\mathbf{x})$. Set $b:=\lcm(b_1,\dots,b_m)$. Then there is a $c\in K[\mathbf{x}]$ such that $c\nmid b^d$ for any $d\geq 0$, since {\Large There are infinitely many irreducible pol. over $K$} and $K[\mathbf{x}]$ is a UFD, hence $b^d \frac{1}{c}\notin K[x]$ for any $d\geq 0$. By lemma~\ref{CriterionForCheckingMembershipInAlgebra}, it follows that $\frac{1}{c}\notin K\left[\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right]$, hence $K(\mathbf{x})\supsetneq K\left[\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right]$. This means $K(\mathbf{x})$ is not a finitely generated $K$-algebra. 
\end{proof}
\begin{lemma}
    These finiteness conditions are transitive, i.e. the following three statements are true:
    \begin{enumerate}
        \item Let $T\supset S$,$S\supset R$ be module-finite. Then $T\supset R$ is module-finite.
        \item Let $T\supset S$, $S\supset R$ be ring-finite. Then $T\supset R$ is ring-finite.
        \item Let $M\supset L$, $L\supset K$ be finite field extensions. Then $M\supset K$ is a finite field extension. 
    \end{enumerate}
\end{lemma}
\begin{proof}
    1. We can find $s_1,\dots,s_n\in S$ such that $S = \sum_1^n Rs_i$ and $t_1,\dots,t_m\in T$ such that $T=\sum_1^m St_i$. Let $t \in T$. Then there are $a_1,\dots,a_m\in S$ such that 
    $t = \sum_1^m a_it_i$.
    For each $i$, $a_i = \sum_{j=1}^n b_{ij}s_j$ for suitable $b_{ij}\in R$, hence 
    $$t=\sum_1^m \sum_1^n b_{ij}t_is_j\in \sum_1^n \sum_1^m Rt_is_j.$$
    Hence $T$ is finitely generated as an $R$-module by the elements of $\left\{t_is_j : i\in\{1,\dots,m\},j\in\{1,\dots,n\}\right\}$.\\
    2. We can find $s_1,\dots,s_n\in S$ such that $S = R[s_1,\dots,s_n]$ and $t_1,\dots,t_m\in T$ such that $T=S[t_1,\dots,t_m]$. Let $t \in T$.  Then there are $a_v\in S$ such that 
    $$t = \sum_{v\in \N^m} a_vt_1^{v_1}\cdots  t_m^{v_m}.$$
    For each $v\in \N^m$, $$a_v = \sum_{w\in \N^n} b_{vw}s_1^{w_1}\cdots s_n^{w_n}a_v $$ for suitable $b_{vw}\in R$, hence 
   $$t=\sum_{v\in \N^m} \sum_{w\in \N^n} b_{vw}t_1^{v_1}\cdots t_m^{v_m}s_1^{w_1}\cdots s_n^{w_n}\in R[s_1,\dots,s_n,t_1,\dots,t_m].$$
    Hence $T$ is finitely generated as an $R$-algebra by the elements of $s_1,\dots,s_n,t_1,\dots,t_m\in T$.\\
    3. There are $\alpha_1,\dots,\alpha_m\in M$ such that $M=L(\alpha_1,\dots,\alpha_m)$ and $\beta_1,\dots,\beta_n\in L$ such that $L= K(\beta_1,\dots,\beta_n)$. Then 
    $$M=L(\alpha_1,\dots,\alpha_m)=K(\beta_1,\dots,\beta_n)(\alpha_1,\dots,\alpha_m) = K(\alpha_1,\dots,\alpha_m,\beta_1,\dots,\beta_n).$$
\end{proof}

\subsubsection{Integral- \& Algebraic Extensions}
\begin{definition}
    Let $S\supset R$ be a ring extension. An element of $s\in S$ is said to be \textit{integral (over $R$)} if it is algebraically dependent over $R$, i.e. there is a monic $f\in R[x]\setminus 0$ such that $f(s)=0$.\\
    Let $L\supset K$ be a field extension. An element of $L$ is \textit{algebraic (over $K$)} if it is integral over $K$. An element that is not algebraic over $K$ is transcendental over $K$. 
\end{definition}
\begin{remark}\label{MinimalPolynomialDefinitionAndTechnicalRemarks}
    In the case $R$ is a field, consider $d = \min (\{ k>0 : \exists f\in R[x]\setminus 0, f(s) =0\})$. Let $f_s$ be a polynomial of degree $d$ vanishing on $s$. Consider another polynomial $g\in R[x]\setminus 0$ vanishing on $s$. We can write $g = qf_s+r$, where $r = 0$ or $\deg \ r < d$. Then 
    $$r(s) =g(s)- q(s)f_s(s)=0.$$
    By minimality $r = 0$, hence $f_s\mid g$. There $f_s$ is the unique non-zero polynomial vanishing on $s$ of minimal degree. We call this polynomial the \textit{defining polynomial of $s$ over $K$}. Note that $\ker \ \ev_s = \langle f_s\rangle$, hence $R[s]\simeq R[x]/\langle f_s\rangle$. We refer to $\deg \ f_s$ as \textit{the degree of $s$ over $R$}. We can extend this result to the case where $R$ is a UFD. By the same argument as before $f_s\mid g$ in $Q(R)[x]$, hence since $f$ is primitive, $f_s\mid g$ in $R[x]$ by Lemma~\ref{PrimitivePolynomialOverFractionFieldDivisorImpliesDivisorInBaseRing} 
\end{remark}
\begin{remark}
    Note that if $a_1,\dots,a_n\in L\supset K$ are algebraic over $K$ ($L\supset K$ is a field extension), then 
    $$K[a_1,\dots,a_n] = K(a_1,\dots,a_n).$$
    Indeed, in the case $n=1$, $K[a]\simeq K[x]/\langle f_a\rangle$. Then $K[a]$ is a subfield of $K(a)$, and since $K(a)$ is the smallest subfield containing $K[a]$, $K[a]=K(a)$. By induction $K[a_1,\dots,a_n]=K(a_1,\dots,a_n)$, so it is sufficient to prove that $K(a_1,\dots,a_n)[a_{n+1}]=K(a_1,\dots,a_n)(a_{n+1})$. Since $a_{n+1}$ is algebraic over $K$ it is algebraic over $K(a_1,\dots,a_n)$, hence by the base case, $K(a_1,\dots,a_n)[a_{n+1}]=K(a_1,\dots,a_n)(a_{n+1})$.
\end{remark}
\begin{example}
    Let $R$ be an integral domain. Then $Q(R)\supset R$ is integral. Indeed consider $\alpha=\frac{a}{b}\in Q(R)$. Then $\alpha$ vanishes on $bx-a\in R[x]\setminus 0$ 
\end{example}
\begin{lemma}\label{EquivalentFormulationsOfIntegralElements}
    Let $S\supset R$ be ring extension where $S$ is an integral domain. Furthermore, let $s\in S$. The following are equivalent
    \begin{enumerate}
        \item $s$ is integral over $R$.
        \item $R[s]\supset R$ is module-finite.
        \item There is a subring of $S$ containing $R[s]$, $R'$ say, which is finitely generated as an $R$-module.   
    \end{enumerate}
\end{lemma}
\begin{proof}
    "1. $\implies$ 2.": We may find $a_0,\dots,a_{n-1}\in R$ such that 
    $$s^n+a_{n-1}s^{n-1}+\dots+ a_1s+ a_0 = 0.$$
    It follows that $s^n \in \sum_1^{n-1} Rs^i$. By a simple induction argument it follows that $s^{n+j}\in \sum_1^{n-1} Rs^i$ for every $j\geq 0$. Let $\sum_1^m b_js^j\in R[s]$. Then by the considerations prior to this, $\sum_1^m b_js^j\in \sum_1^{n-1} Rs^i$, hence $R[s]=\sum_1^{n-1} Rs^{i}$.\\
    "2. $\implies$ 3.": Putting $R' = R[s]$ we have such a subring of $S$.\\
    "3. $\implies$ 1.": We can write $R'=\sum_1^n a_it_i$ for suitable $t_1,\dots,t_n\in R[s]\setminus 0$. Then 
    $$st_i = \sum_1^n a_{ij}t_i$$
    for suitable $a_{ij}\in R$. Note then that 
    $$s\begin{pmatrix} t_1\\ \vdots \\ t_n\end{pmatrix} = (a_{ij}) \begin{pmatrix} t_1\\ \vdots \\ t_n\end{pmatrix}.$$
    This implies that $s$ is a root of the characteristic polynomial 
    $$\det\left( x\mathbbm{1} - (a_{ij})\right) \in R[x]\setminus 0$$
    which is monic{\LARGE ref.}. It thus follows that $s$ is integral over $R$. 
\end{proof}
\begin{lemma}
    Consider a tower of ring extensions $T\supset S\supset R$ where $T$ is a domain. Suppose $T$ is integral over $S$ and $S$ is integral over $R$, then $T$ is integral over $R$. 
\end{lemma}
\begin{proof}
    Let $t\in T$, then there is a monic $f=x^n+\sum_0^{n-1} a_ix^i\in S[x]\setminus 0$ such that $f(t)=0$. By the above lemma $R' := R[a_1,\dots,a_n,t]=R[a_1,\dots,a_n][t] \supset R[a_1,\dots,a_n]$ is module-finite. By the above lemma we also find that $R[a_1]\supset R$ is module-finite. Recursive usage of the above and the transitivity of module-finiteness thus implies that $R[a_1,\dots,a_{n-1}]\supset R$ is module-finite and hence that $R' = R[a_1,\dots,a_{n-1},t]\supset R$ is module-finite. We have thus found a subring of $T$ containing $R[t]$, which is module-finite over $R$. Hence using the above lemma yet another time it follows that $t$ is integral over $R$, hence $T$ is integral over $R$. 
\end{proof}
\begin{proposition}\label{IntegralElementsIsSubring}
    Let $S\supset R$ be an integral domain and a ring extension. Then 
    $$\left\{ s\in S : s \ \text{is integral over} \ R\right\}$$
    is a subring $S$.
\end{proposition}
\begin{proof}
    Let $a,b\in S$ be integral over $R$. We repeatably use Lemma~\ref{EquivalentFormulationsOfIntegralElements}. Note that $b$ is integral over $R[a]$ hence $R[a,b]\subset R$ is module-finite. Since $a + b, ab\in R[a,b]$, it follows that $R[a,b]$ is a ring contained in $S$, containing $R[a+b]$ and $R[ab]$ that is module-finite over $R$, meaning $a+b$ and $ab$ are integral over $R$. 
\end{proof}
\begin{lemma}\label{ModuleFiniteImpliesAlgebraic}
    If $S\supset R$ is module-finite then $S\supset R$ is integral. 
\end{lemma}
\begin{proof}
    Let $s\in S$. The ring $S$ is a subring of $S$ containing $R[s]$ which is finitely generated as an $R$-module. It thus follows by Lemma~\ref{EquivalentFormulationsOfIntegralElements} that $s$ is integral over $R$. Hence $S\supset R$ is integral.
\end{proof}
\begin{lemma}\label{ForRingFiniteExtensionsModuleFiniteIsTheSameAsAlgebraic}
    Let $S\supset R$ be a ring extension with $S$ an integral domain. Then $S\supset R$ is module-finite if and only if $S=R[s_1,\dots,s_n]$ where $s_i\in S$ is integral over $R$. 
\end{lemma}
\begin{proof}
    "$\implies$": This follows from Lemma~\ref{ModuleFiniteImpliesAlgebraic}
    "$\impliedby$": By assumption there are $s_1,\dots,s_n\in S$ such that $S = R[s_1,\dots,s_n]$. Since $s_i\in S$ is integral over $R$, it follows by Lemma~\ref{EquivalentFormulationsOfIntegralElements} that $R[s_1] \supset R$ is module-finite and by induction $S=R[s_1,\dots,s_n]\supset R$ is module-finite. 
\end{proof}
\begin{lemma}\label{TechnicalLemmaAboutFieldExtOverAlgClosedFields}
    Let $L\supset K$ be a field extension with $K$ algebraically closed. 
    \begin{enumerate}
        \item Every $f\in K[x]\setminus 0$ with $n:=\deg \ f >0$ has exactly $n$ roots all in $K$.  
        \item If $a\in L$ is algebraic over $K$, then $a\in K$.
        \item If $L\supset K$ is module-finite, then $L=K$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    1. We proceed by induction in $n$. For $n=1$, $f$ has a root $a\in K$, and since $f$ has exactly one root, this is the only root.\\ 
    Suppose now $f$ has degree $n+1$, then $f$ has a root $a\in K$, hence $f = (x-a)g$ for a polynomial $g\in K[x]\setminus0$ with $\deg \ g = n$. By induction $g$ has exactly $n$ roots in $K$. Using that $K$ is an integral domain it follows that $f$ has $n+1$ roots.\\
    2. If $a\in L$ is algebraic over $L$, then there is a polynomial $f\in K[x]\setminus 0$ such that $a$ is a root $f$. Since $V(f)\subset K$ by 1. it follows that $a\in K$.\\
    3. If $L\supset K$ is module-finite, then it is algebraic by Lemma~\ref{ModuleFiniteImpliesAlgebraic}. Let $a\in L$. Then $a$ is algebraic over $K$, hence $a\in K$ by 2. We thus get that $L=K$.
\end{proof}
\begin{lemma}\label{TechnicalPolynomialFieldExtensionLemma}
    Let $K$ be a field and set $L:= Q(K[x])=K(x)$. Then
    \begin{enumerate}
        \item $a\in L$ is integral then $a\in K[x]$.
        \item There is no $f\in K[x]\setminus 0$ such that for every $a\in L $, $F^na$ is integral over $K[x]$ for some $n>0$. 
    \end{enumerate}
\end{lemma}
\begin{proof}
    1. We may write $a = \frac{f}{g}$ for $f,g\in K[x]$ with $g\neq 0$ and $\gcd(f,g)=1$. We can then find $a_0,\dots,a_{n-1}\in K[x]$ such that 
    $$\frac{f^n}{g^n}+\sum_0^{n-1} a_i\frac{f^i}{g^i} = 0 \implies f^n = \sum_0^{n-1} a_ig^{n-i}f^i = g\sum_0^{n-1} a_ig^{n-(i+1)}f^i,$$
    hence $g\mid f^n$, meaning $g\mid f$, hence $g\in K\setminus 0$. This implies that $a = \frac{f}{g} = g^{-1}f\in K[x].$\\
    2. By Proposition~\ref{PolynomialQuotientFieldIsFieldFiniteNotRingFinite} $K(x)\supsetneq R:=K[z_1,\dots,z_m]$ for any $z_1,\dots,z_m\in K(x)$. Recall that we proved this by showing that for any $f\in K[x]\setminus 0$ there is some $c\in K[x]$ such that $f^d\frac{1}{c}\notin K[x]$ for any $d>0$. By 1. this implies that $f^d\frac{1}{c}$ is not integral over $K$ for $a$.
\end{proof}
\begin{proposition}
    Let $L\supset K$ be a field extension. The set 
    $$\{a\in L: a \ \mathrm{is}\ \mathrm{integral}/K\}$$
    is a subfield of $L$.
\end{proposition}
\begin{proof}
    We already know that it is a subring by Proposition~\ref{IntegralElementsIsSubring}. Let $a\in L\setminus 0$ be integral over $K$. Then there are $a_0,\dots,a_{n-1}\in K$ such that 
    $$0= a^n+\sum_0^{n-1} a_ia^{i},$$
    where choosing $n$ minimal implies, $a_0 \neq 0$, hence 
    $a\left(a^{n-1}(-a_0)^{-1}\sum_1^{n-1} a_ia^{i}\right) = 1$, implying $a$ is a unit. 
\end{proof}
\begin{proposition}\label{CannotSqueezeNonFieldBetweenFiniteFieldExtension}
    Let $L\supset K$ be a module-finite field extension. Consider a subring $R$ of $L$ containing $K$ as a subring. Then $R$ is a field. 
\end{proposition}
\begin{proof}
    By Lemma~\ref{ModuleFiniteImpliesAlgebraic}, $L\supset K$ is algebraic. Let $r\in R\setminus 0$. Then $r$ has a multiplicative inverse $r^{-1}\in L\setminus 0$. We can thus find $a_0,a_1,\dots, a_{n-1}\in K$ such that 
    $$r^{-n}+\sum_0^{n-1} a_ir^{-i} = 0,$$
    This implies that 
    $$r^{-1}=r^{n-1}r^{-n} = -\sum_0^{n-1}a_i r^{n-1}r^{-i} = -\sum_0^{n-1} a_ir^{n-1-i}\in R,$$
    hence $R$ is a subfield of $L$. 
\end{proof}
\begin{theorem}
    Let $L\supset K$ be a ring-finite field extension generated by $a_1,\dots,a_n\in L$. Then $L\supset K$ is module finite and hence also algebraic.   
\end{theorem}
\begin{proof}
    We use induction in $n$. For $n=1$, suppose $L=K[a]$ for some $a\in L$. Consider $\rho = \ev_a : K[x]\rightarrow L$. Since $K[x]$ is a PID {\LARGE add result!}, we find that $\ker\ \rho = \langle g \rangle$ for some $g\in K[x]$. Then $K[x]/\langle g\rangle \simeq K[a] = L$. We claim that $g\neq 0$\\
    \textbf{Proof of the claim:} Suppose $K[x]\simeq K[a]$, then $K(x) \simeq K(a)$, but then $L$ is not ring-finite over $K$, by Proposition~\ref{PolynomialQuotientFieldIsFieldFiniteNotRingFinite} leading to a contradiction.\\
    So we may WLOG assume $g$ is monic. Then $a$ is algebraic over K.\\
    Assume the statement is true for some $n\geq 1$. Suppose $L=K[a_1,\dots,a_n]$. Set $K' = K(a_1)$. Then by induction $L=K'[a_2,\dots,a_{n+1}]$ is algebraic. Suppose $a_1$ is algebraic over $K$. Then $K'\supset K$ is algebraic, hence $L\supset K$ is algebraic. Suppose for a contradiction that $a_1$ is not algebraic over $K$. We note that this implies that $K[a_1]\overset{\sigma}{\simeq} K[x]$ {\LARGE add reference!}. We have identities 
    $$a_i^{n_i}+\sum_{j=0}^{n_i-1} \alpha_{ij}a_i^{j}=0,$$
    for each $i\geq 2$ for suitable $n_i\geq 1$, $\alpha_{ij}\in K'$. Let $\alpha\in K[a_1]$ be the common denominator of the $\alpha_{ij}$. Let $M\geq \max_{i\in\{2,\dots,n+1\}}\ n_i$. Then 
    $$\left(\alpha a_i\right)^{M}+\sum_{j=0}^{n_i-1} \alpha^{M-j}\alpha_{ij} \left(\alpha a_i\right)^j=0,$$
    hence $\alpha^Ma_i$ is integral over $K[a_1]$.
    Let $z = \sum_{v\in \N^{n+1}} c_va_1^{v_1}\cdots a_{n+1}^{v_{n+1}}\in L$. Then taking $N\geq 0$ sufficiently large we get that $\alpha^N z$ is integral over $K[a_1]$ by Proposition~\ref{IntegralElementsIsSubring}. However taking $z\in K(a_1)$, this implies that $\sigma(z)\in K(x)$ is a polynomial such that $\sigma(\alpha_1)^N\sigma(z)$ is integral over $K[x]$, leading to a contradiction by Lemma~\ref{TechnicalPolynomialFieldExtensionLemma}.
\end{proof}

\begin{corollary}\label{IfExistsSurjectiveAlgHomThenFieldExtOverAlgClosedFieldIsTrivial}
    Let $L\supset K$ be a field extension where $K$ is algebraically closed. Suppose also that there is a surjective $K$-algebra homomorphism from $K[x_1,\dots,x_n]$ to $L$ for some $n>0$. Then $K=L$. 
\end{corollary}
\begin{proof}
    Let $\sigma: K[x_1,\dots,x_n] \rightarrow L$ be a surjective $K$-algebra map. By Corollary~\ref{AlgebraHomomorphismFromPolynomialRingIsJustEvaluation} there are $a_1,\dots,a_n\in L$ such that $\sigma = \ev_{a_1,\dots,a_n}$. It thus follows that $L = \sigma(K[\mathbf{x}])=\ev_{a_1,\dots,a_n}(K[\mathbf{x}])=K[a_1,\dots,a_n]$, hence by the above theorem $L$ is module-finite over $K$. It follows from Lemma~\ref{TechnicalLemmaAboutFieldExtOverAlgClosedFields} that $L=K$. 
\end{proof}
\begin{corollary}\label{ClassificationOfMaximalPolynomialIdeals}
    Let $K$ be algebraically closed and $I\subset K[x_1,\dots,x_n]$ be a maximal ideal. Then $K[x_1,\dots,x_n]/I=K$ (thinking about $K$ as the canonical embedding of $K$ in $K[\mathbf{x}]/I$). This implies $I=\langle x_1-a_1,\dots,x_n-a_n\rangle$. 
\end{corollary}
\begin{proof}
    The quotient map $\pi : K[\mathbf{x}]\rightarrow K[\mathbf{x}]/I, f\mapsto f+I$ is a canonically a surjective $K$-algebra homomorphism. It follows by the above corollary that $K[\mathbf{x}]/I=K$. Then for each $i$, $x_i+I = a_i+ I$ for some $a_i\in K$, this means $J:=\langle x_1-a_1,\dots,x_n-a_n\rangle\subset I$. $J$ is maximal by Corollary~\ref{MaximalIdealsOfPolynomialRings}, hence $J=I$. 
\end{proof}
\subsubsection{Field Extensions}
    \begin{lemma}
        Let $L\supset K$ be a field extension and $a\in L$. Then $a$ is module finite if and only if $\dim_K \ K[a] <\infty$. If $a$ is algebraic, let $d$ denote $\deg \ f_a$. Then $\{1,a,\dots,a^{d-1}\}$ is a basis for $K[a]$. 
    \end{lemma}
    \begin{proof}
        The first statement follows immediately from Proposition~\ref{EquivalentFormulationsOfIntegralElements}. Set $I=\langle f_a\rangle$. By Lemma~\ref{DimensionOfQuotientSpace}, $\{1+ I,x+I,\dots,x^{d-1}+I\}$ is a basis of $K[x]/I \overset{\overline{\ev_a}}{\simeq} K[a]$, since $\ev_a$ is a $K$-algebra homomorphism, it is in particular a $K$-linear map, hence $\{1,a,\dots,a^{d-1}\}$ is a basis of $K[a]$
    \end{proof}
    \begin{definition}
        If $L\supset K$ is a module finite field extension we define define $[L:K]:= \dim_K\ L$ to be \textit{the degree of $L$ over $K$} 
    \end{definition}
    \begin{lemma}\label{ModuleFiniteFieldExtensions}
        Let $L\supset K$ be a field extension. Then $L\supset K$ is module finite if and only if $L\supset K$ is a finite field extension generated by some $a_1,\dots,a_n\in L$ that are algebraic over $K$. 
    \end{lemma}
    \begin{proof}
        "$\implies$": By Lemma~\ref{ForRingFiniteExtensionsModuleFiniteIsTheSameAsAlgebraic} $L = K[a_1,\dots,a_n]$ for suitable $a_1,\dots,a_n\in L$ that are algebraic over $K$, hence $L\subset K[a_1,\dots,a_n]\subset K(a_1,\dots,a_n) \subset L$, hence $L=K(a_1,\dots,a_n)$.\\
        "$\impliedby$": Suppose $L=K(a_1,\dots,a_n)$ for some $a_i\in L$. If $n=1$, then $L=K(a_1)=K[a_1]$ which is module finite over $K$ due to Proposition~\ref{EquivalentFormulationsOfIntegralElements}. Set $L'=K(a_1,\dots,a_n)$ which by induction is module finite over $K$. Note that $a_{n+1}$ is algebraic over $L'$, hence applying Proposition~\ref{EquivalentFormulationsOfIntegralElements}, $L\supset L'$ is module finite. By the transitive property of module finite extensions, it follows that $L\supset K$ is module finite.  
    \end{proof}
    \begin{lemma}
        Let $L\supset K$ be a field extension and $f\in K[x]$ irreducible. Suppose there is an $a\in L$ such that $f(a)=0$. Then $L\simeq  K[x]/I$ where $I:= \langle f\rangle$.
    \end{lemma}
    \begin{proof}
        Consider the $K$-algebra map
        \begin{gather*}
            \sigma := \ev_a : K[x]\rightarrow L
        \end{gather*}
        This induces an isomorphism 
        \begin{gather*}
            \overline {\sigma}: K[x]/\ker \ \sigma \simeq \im \ \sigma\\
            \mu + \ker \ \sigma \mapsto \mu(a)
        \end{gather*}
        Since $K[x]$ is a PID, $\ker \ \sigma = \langle f'\rangle$ for some $f'$ and since $f\in \ker \ \sigma$ it follows that $f'\mid f$, hence $\langle f\rangle =\langle f'\rangle$ by the irreducibility of $f$. Note that $K[x]/I$ is a field (cf. Lemma~\ref{OverFieldQoutientRingWithRespectToIrreduciblePolynomialIsAField}).
        Let $z=\frac{g(a)}{h(a)}\in K(a)$. Then since $h(a)\neq 0$, $f\nmid h$, hence $h+I\neq 0$. Then 
        $$z = \frac{g(a)}{h(a)} = \frac{\sigma(g)}{\sigma(h)}=\frac{\overline{\sigma}(g+I)}{\overline{\sigma}(h+I)}= \overline{\sigma}\left((g+I)(h+I)^{-1}\right)\in \im \ \overline{\sigma}=\im\ \sigma.$$ 
    \end{proof}
    \begin{lemma}\label{SplittingFieldLemma}
        Let $L\supset K$ be a field extension and $f\in K[x]$ an irreducible, monic polynomial. Suppose there is an $a\in L$ such that $f(a)=0$. Set $L':= K[x]/I\simeq K(a)$, where $I:=\langle f\rangle$ 
        \begin{enumerate}
            \item Suppose there is a $g\in k[x]$ that also vanishes on $a$. Then $f\mid g$.
            \item  identifying $K$ canonically with a subfield of $L'$ and $K(a)$ with $L'$ we find $f=(y-(x+I))f_1$ for some $f_1\in L[y]$.
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        1. From the proof of the last lemma we learned that $\ev_a(g)=0$ if and only if $g\in I$, hence $f\mid g$.\\
        2. Since $x+I$ is a zero of $f$ in $L$ the result follows.  
    \end{proof}
    \begin{theorem}\label{ExistenceTheoremForSplittingField}(Existence Theorem for Splitting Fields) 
        Let $K$ be a field and $f\in K[x]$. There is a field $L$ extending $K$ such that $f$ can be written as a product of linear polynomials over $L$
    \end{theorem}
    \begin{proof}
        When $\deg \ f = 1$ the statement is trivial by taking $K=L$. If $f$ is of of degree $d+1$ for some $d\geq 1$, pick a monic irreducible factor of $f$, $g$ say. Then over $L' = K[x]/\langle g\rangle$, $g =(y - (x+I))g_1 $ for some $g_1\in L'[y]$. The $f=qg=qg_1(y-(x+I))$ for some $q\in L'[y]$ and the result follows by induction in the degree.
    \end{proof}
    \begin{definition}
        The above $L$ is called \textit{the splitting field of $f$ over $K$}.
    \end{definition}
    \begin{lemma}
        Let $K$ be a characteristic $0$ field and $f\in K[x]$ irreducible monic. Let $L$ be the splitting field of $f$ over $K$ and write $f=\prod_1^d (x-\alpha_i)$ for suitable $\alpha_i\in L$. 
    \end{lemma}
    \begin{proof}
        Suppose $L$ is a field extension over $K$, and suppose there is an $\alpha \in L$ such that $(x-\alpha^2)\mid f$. Then $g:=D f$ also has $\alpha$ as a root. Then $g\nmid f$, hence by Lemma~\ref{SplittingFieldLemma} $f$ cannot be irreducible. In particular if $L$ was the splitting field, and $f$ has a multiple linear factor, then $f$ is not irreducible.  
    \end{proof}
\subsubsection{Theorem of the Primitive Element}
    \begin{theorem}
        Let $K$ be a characteristic $0$ field and $L\supset K$ a module-finite extension. Then there is a $c\in L$ such that $L=K(c)$.
    \end{theorem}
    \begin{proof}
        Suppose $L=K(a,b)$. Then there are monic irreducible polynomials $f,g\in K[x]$ such that $f(a)=0$ and $g(b)=0$. Let $S$ be the splitting field of $f$ and $g$. Write $f=(x-a)\prod_1^l (x-\alpha_i)$ and $g=(x-b)\prod_1^k(x-\beta_i)$. We may pick $\lambda \neq 0$ such that $c:=\lambda a+b\neq \lambda \alpha_i+ \beta_j$ for any $i,j$, since $V_{ij}:=V(\alpha_it+\beta_j-(at+b))$ can have at most finitely many points. So pick any $\lambda\notin \{0\}\cup\bigcup V_{ij}$. Set $K':= K(c)$ and $h:=g(c-\lambda x)\in K'[x]$. Note that $h(a)=g(b)=0$ and $h(\alpha_i)=g(\lambda a + b-\lambda\alpha_i)$ and since $\lambda a + b-\lambda\alpha_i\neq \beta_j$ for any $j$, $h(\alpha_i)\neq 0$. Then $\gcd(f,h)=x-a\in K'[x]$, implying $a\in K'$, and so $b=c-\lambda a\in K'$. In conclusion, $L=K(c)$.\\
        Suppose $L=K(a_1,\dots,a_{n+1})$ for some $n\geq 1$. By induction there are $\lambda_1,\dots,\lambda_n\in K\setminus 0$, so that upon defining $c=\sum_1^n \lambda_ia_i$,  $K(a_1,\dots,a_n)=K(c)$, hence $L=K(c,a)=K(c')$ by the first case.
    \end{proof}
\subsubsection{Transcendence Degree \& Transcendence Bases}
\begin{definition}
    Let $L\supset K$ be a field extension. We say that $L$ has \textit{transcendence degree $d$ over $K$} if there is a set $X=\{ a_1,\dots,a_d\}\subset L$ such that $X$ is algebraically independent over $K$ and every other set $Y\subset L$ with more than $n$ elements is algebraically dependent over $K$. We define 
    $$\trdeg_K\ L:=\trdeg \ L =d.$$
    If there is not such $d$ we write $\trdeg_K \ L = \infty.$\\
    A finite field extension over $K$ of transcendence degree $n$ is called \textit{an algebraic function field (over $K$) in $n$ variables} 
\end{definition}
\begin{remark}
    If $\delta<d$ is a positive integer such that there are $b_1,\dots,b_\delta$ that are algebraically independent, then $a_1,\dots,a_{\delta}$ are algebraically independent over $K$, hence the transcendence degree of $L$ over $K$ is unique. 
\end{remark}
\begin{definition}
    Let $L\supset K$ be a field extension. A set $X = \{a_1,\dots,a_d\}\subset L$ is a \textit{transcendence basis of $L$ over $K$} if $X$ is algebraically independent over $K$ and $K(a_1,\dots,a_d)\supset K$ is algebraic.  
\end{definition}
\begin{remark}
    When $L\supset K$ is algebraic, then $\emptyset$ is a transcendence basis of $L$ over $K$.
\end{remark}
\begin{lemma}\label{EquivalentDefinitionOfTranscendenceBasis}
    Let $L\supset K$ be a field extension and $X= \{a_1,\dots,a_d\}\subset L$ be algebraically dependent. Consider an element $a\in L$. $X\cup\{a\}$ is algebraically dependent over $K$ if and only if $a$ is algebraic over $K(a_1,\dots,a_d)$. Therefor $a_1,\dots,a_d\in L$ forms a transcendence basis of $L$ over $K$ if and only if $a_1,\dots,a_d$ are algebraically independent over $K$ and every $a\in L$ is algebraic over $K(a_1,\dots,a_d)$. 
\end{lemma}
\begin{proof}
    "$\implies$": Let $f\in K[x_1,\dots,x_{d+1}]\setminus 0$ be given such that $f(a_1,\dots,a_d,a)=0$. Then $f = \sum_0^m f_ix_{d+1}^i$, with $f_m \neq 0$. Since $X$ is algebraically independent over $K$, this implies $f_m(a_1,\dots,a_d)\neq 0$. Then 
    $$g:= f_m(a_1,\dots,a_d)^{-1}f(a_1,\dots,a_d,x)$$
    is non-zero, monic and has $a$ as a root, implying $a$ is algebraic over $K(a_1,\dots,a_d)$.\\
    "$\impliedby$": There is some $f=y^m+\sum_0^{m-1} b_iy^i\in K(a_1,\dots,a_d)[y]\setminus 0$ such that $f(a)=0$. Then $g:=cf\in K[a_1,\dots,a_d][y]\setminus 0$, where $c$ is a common denominator of the $b_i$'s, hence 
    $$g = \sum_0^m g_i(a_1,\dots,a_d)y^{i},$$
    for suitable $g_i\in K[x_1,\dots,x_d]$, with $g_m=c\neq 0$. It follows that 
    $$h := \sum_0^m g_iy^i\in K[x_1,\dots,x_d,y],$$
    such that $h(a_1,\dots,a_d,a)= g(a)=0,$ hence $X\cup \{a\}$ is algebraically dependent over $K$.
\end{proof}
\begin{lemma}\label{SurjectiveHomInducesIneqOfTrdegs}
    Let $L\supset K$ and $L' \supset K$ be field extensions. Suppose there is a surjective $K$-algebra homomorphism $\sigma: L\rightarrow L'$. Then $\trdeg_K\ L \geq \trdeg_K\ L'$.
\end{lemma}
\begin{proof}
    Let $\alpha \in L'$ and set $n:= \trdeg \ L$. Pick $\alpha_1,\dots,\alpha_n\in L'$ and pick $\beta,\beta_1,\dots,\beta_n\in L$ such that $\sigma(\beta),\sigma(\beta_i)=\alpha$. Pick $f\in K(\beta_1,\dots,\beta_n)[x]\setminus 0$ be monic such that $f(\beta)=0$. Let $\overline{\sigma}$ denote the restriction of $L[x]\rightarrow L'[x]$, the induced $K$-algebra map induced by $\sigma$ to a $K$-algebra map $K(\beta_1,\dots,\beta_n)\rightarrow K(\alpha_1,\dots,\alpha_n)$. Then 
    $$\overline{\sigma}(f)(\alpha)=\overline{\sigma}(f)(\sigma(\beta))=\sigma(f(\beta))=\sigma(0)=0,$$
    and since $\overline{\sigma}(f)$ is monic, this shows that $\alpha$ is algebraic over $K(\alpha_1,\dots,\alpha_n)$. So every sequence of $n$ elements in $L'$ are algebraically dependent over $K$ by the prior lemma. It follows that $\trdeg_K \ L' \leq n = \trdeg_K \ L$.
\end{proof}
\begin{remark}\label{ImportantTranscendenceBasisRemark}
    Note that the assumption that $X$ is algebraically independent over $K$ is not necessary to prove "$\impliedby$".
\end{remark}
\begin{lemma}\label{TechnicalTranscendenceBasisLemma}
    Let $L\supset K$ be a finite field extension generated by $a_1,\dots,a_d\in L$. 
    \begin{enumerate}
        \item There is a subset of $X:= \{a_1,\dots,a_r\}$ that is a transcendence basis for $L$ over $K$.
        \item Let $Y=\{ a \in X : a \text{ is transcendental over } K(b_1,\dots,b_s)\}$. If $b_1,\dots,b_s\in L$ are algebraically independent over $K$, then for some $Z\subset Y$, $\{b_1,\dots, b_s\}\cup Y$ is a transcendence basis of $L$ over $K$ 
    \end{enumerate}
\end{lemma}
\begin{proof}
    1. If no subset of $X$ is algebraically independent over $K$, then each $a_i$ is algebraic over $K$, hence $L$ is algebraic over $K$. This means that $\emptyset$ is a transcendence basis for $L$ over $K$.\\ 
    Suppose now that there is a subset of $\{a_1,\dots,a_r\}$ whose elements are algebraically independent over K.  Let $Y\subset\{a_1,\dots,a_r\}$ be a maximal subset of elements that are algebraically independent over $K$. After a permutation, we can write $Y=\{a_1,\dots,a_k\}$ for some $k<r$. Then $a_1,\dots,a_k,a_i$ are algebraically dependent over $K$ for each $i\in\{k+1,\dots,r\}$. Then by Lemma~\ref{EquivalentDefinitionOfTranscendenceBasis}, $a_i$ is algebraic over $K(a_1,\dots a_k)$ for each $i\in\{k+1,\dots, r\}$. Then $L$ is algebraic over $K(a_1,\dots,a_k)$, implying $Y$ is a transcendence basis of $L$ over $K$.\\
    2. We proceed by induction in $k$: if $a_i$ is algebraic over $K(b_1,\dots,b_s)$ for every $i\in\{1,\dots,r\}$, then $L$ is algebraic over $K(b_1,\dots,b_s)$, hence $\{b_1,\dots,b_s\}$ is a transcendence basis for $L$ over $K$ by Lemma~\ref{EquivalentDefinitionOfTranscendenceBasis}.\\
    Suppose the statement is true for some $k<n$. Consider WLOG $Y= \{a_1,\dots,a_{k+1}\}$. By Lemma~\ref{EquivalentDefinitionOfTranscendenceBasis} $b_1,\dots,b_s,a_1$ are algebraically independent over $K$. Every element in $X\setminus Y$ is algebraic over $K(b_1,\dots,b_s, a_1)$, hence $Y':= \{ a \in X : a \text{ is transcendental over } K\}\subset Y$. It thus follows by induction that for some $Z'\subset Y'$, 
    $$\{b_1,\dots,b_s\}\cup \underbrace{\{a_1\}\cup Z'}_{=:Z}$$ 
    is a transcendence basis of $L$ over $K$.
\end{proof}
\begin{theorem}\label{CardinalityOfTranscendenceBasisIsEqualToTranscendenceDegree}
    Let $L\supset K$ be a field extension. $L$ has a transcendence basis $X=\{a_1,\dots, a_d\}$ if and only if $\trdeg_K \ L = d$
\end{theorem}
\begin{proof}
    "$\implies$": 
    \textbf{To prove this we make the following claim}: $L$ is algebraic over $K(b_1,\dots,b_{k}, a'_{k+1},\dots,a'_{d})$ for any $k\in \{0,\dots,d\}$ for some subset $\{a'_{k+1},\dots,a'_{d}\}\subset \{a_1,\dots,a_d\}$ with $d-k$ elements.  We prove this using induction in $r$. For $k=0$, we have that $L\supset K(a_1,\dots,a_d)$ is algebraic by the assumption that $\{a_1,\dots,a_d\}$ is a transcendence basis for $L$ over $K$. Suppose that we the statement holds for some $k\in\{0,\dots,d-1\}$. Then by induction hypothesis $L$ is algebraic over $M := K(b_1,\dots,b_k,a_{r+1}',\dots,a'_d)$ for a suitable subset $\{a_{k+1}',\dots,a_{d}'\}\subset \{a_1,\dots,a_d\}$. This means $b_{k+1}$ is algebraic over $M$, hence by Lemma~\ref{EquivalentDefinitionOfTranscendenceBasis} $b_1,\dots,b_{k+1},a_{r+1}',\dots,a_d$ are algebraically dependent over $K$. By Lemma~\ref{TechnicalTranscendenceBasisLemma} there is an integer $r\leq s<d$ and a subset $\{a''_{k+1},\dots,a''_{s+1}\}\subset \{a'_{k+1},\dots,a_{d}'\}$ such that $b_1,\dots,b_{k+1},a''_{k+2},\dots,a_{s+1}''$ are algebraically independent over $K$ and $b_1,\dots,b_{r+1},a''_{k+1},\dots,a_{s+1}''$ are algebraically dependent over $K$. Again by Lemma~\ref{EquivalentDefinitionOfTranscendenceBasis}, we have that $a''_{k+1}$ is algebraic over $M':= K(b_1,\dots,b_{k+1},a''_{k+2},\dots,a''_d)$. Now $L\supset M(b_{r+1})$ is algebraic and $M'(a''_{r+1})\supset M'$ is algebraic. Since $M(b_{r+1})=M'(a''_{r+1})$ we find that $L$ is algebraic over $M'$, finishing the proof of the claim.
    \textbf{Application of the claim:} Suppose for a contradiction that there are $b_1,\dots,b_{d+1}\in L$ that are algebraically independent over $K$. Then $b_1,\dots,b_d$ are algebraically independent over $K$. But then by the claim $L\supset K(b_1,\dots,b_d)$ is algebraic, hence $b_{d+1}$ is algebraic over $K(b_1,\dots,b_d)$, but then $b_1,\dots,b_{d+1}$ are algebraically dependent over $K$.\\ 
    "$\impliedby$": There are algebraically independent $a_1,\dots,a_d \in L$ and for every $a\in L$ $a,a_1,\dots,a_d$ are algebraically dependent, hence by Lemma~\ref{EquivalentDefinitionOfTranscendenceBasis} $\{a_1,\dots,a_d\}$ is a transcendence basis of $L$ over $K$.
\end{proof}
\begin{remark}
    Note the claim of "$\impliedby$", simply proves that every algebraically independent $b_1,\dots,b_d$ forms a transcendence basis of $L$ over $K$ 
\end{remark}
\begin{corollary}
    Two transcendence bases have the same cardinality. 
\end{corollary}
\begin{example}\label{TranscendenceDegreeOfQuotientFieldOfPolynomialRing}
    Consider $L=K(x_1,\dots,x_n)=Q(K[x_1,\dots,x_n])$. The elements $x_1,\dots,x_n\in K[x_1,\dots,x_n]$ are algebraically independent over $K$, hence $x_1,\dots,x_n\in K(x_1,\dots,x_n)$ are algebraically independent over $K$. It follows that $\{x_1,\dots,x_n\}$ is a transcendence basis of $K(\mathbf{x})\supset K$, hence $\trdeg_K \ K(\mathbf{x}) = n$. 
\end{example}
\begin{corollary}\label{TrcBasisExtendsWhenExtFinGen}
    Consider a tower of field extensions $M\supset L\supset K$. Let $X=\{a_1,\dots,a_n\}\subset L$ be a transcendence basis for $L$ over $K$. Suppose $M$ is finitely generated $L$-module (in other words a finite dimensional vector space over $L$). Then $X$ is transcendence basis for $M$ over $K$ and hence $\text{trdeg}_K\ M = \text{trdeg}_K\ L$.
\end{corollary}
\begin{proof}
    We need to check that $M$ is algebraic over $K(a_1,\dots,a_n)$. Note that $M\supset L$ being module-finite, implies $M\supset L$ is algebraic (cf. Lemma~\ref{ForRingFiniteExtensionsModuleFiniteIsTheSameAsAlgebraic}), hence $M\supset K(a_1,\dots,a_n)$ is algebraic. 
\end{proof}
\begin{lemma}\label{InjectiveKAlgebraHomomorphismImpliesSmallerTranscendenceDegree}
    Let $L\supset K$ and $M\supset K$ be field extension with an injective $K$-algebra homomorphism $\sigma: L\hookrightarrow M$. If $a_1,\dots,a_n\in L$ are algebraically independent over $K$, then so are $\sigma(a_1),\dots,\sigma(a_n)$. It follows that $\trdeg_K\ L \leq \trdeg_K\ M$.\\
\end{lemma}
\begin{proof}
    Let $a_1,\dots,a_n\in L$ be algebraically independent over $K$.  Let $f\in K[x_1,\dots,x_n]\setminus 0$. Note that $\sigma(k)=k$ for every $k\in K$ and $f(a_1,\dots,a_n)\neq 0$ by the assumption that $a_1,\dots, a_n$ are algebraically independent over $K$. It follows that
    $$f(\sigma(a_1),\dots,\sigma(a_n))= \sigma(f(a_1,\dots,a_n))\neq 0,$$
    hence $\sigma(a_1),\dots,\sigma(a_n)$ are algebraically independent. Hence for any $n\leq \trdeg \ L$ there are algebraically independent $b_1,\dots,b_n\in M$, hence $\trdeg\ L \leq \trdeg\ M$.
\end{proof}
\begin{remark}
    Note that $\trdeg_K$ is an integer invariant (cf. Example~\ref{IntegerInvariants}) of the category with objects being field extensions over $K$ and morphisms being injective $K$-algebra homomorphism.
\end{remark}
\begin{lemma}
    Let $M\supset L\supset K$ be a tower of field extensions.
    $$\trdeg_K \ M = \trdeg_K\ L+ \trdeg_L \ M.$$
    If $L$ has transcendence basis $X=\{a_1,\dots,a_d\}$ over $K$ and $M$ has transcendence basis $Y=\{b_1,\dots,b_\delta\}$ over $L$, then $X\cup Y$ is a transcendence basis for $M$ over $K$. It follows that 
\end{lemma}
\begin{proof}
    If $\trdeg_L \ M = \infty$, then $\trdeg_K \ M = \infty$. Suppose this is not the case. Then we may assume that $L$ has transcendence basis $X=\{a_1,\dots,a_d\}$ over $K$ and $M$ has transcendence basis $Y=\{b_1,\dots,b_\delta\}$ over $L$. Then $M\supset L(b_1,\dots,b_\delta)$ is algebraic and $L\supset K(a_1,\dots,a_d)$ is algebraic, hence $$L(b_1,\dots,b_\delta)\supset K(a_1,\dots,a_d)(b_1,\dots,b_\delta)=K(a_1,\dots,a_d,b_1,\dots,b_\delta)$$
    is algebraic. It follows that $M\supset K(a_1,\dots,a_d,b_1,\dots,b_\delta)$ is algebraic. Let $f\in K[x_1,\dots,x_d,y_1,\dots,y_\delta]\setminus 0$. Write 
    $$f = \sum_{v\in\N^\delta} f_v \mathbf{y}^v.$$
    For some $v$, $f_v\neq 0$, hence since $a_1,\dots,a_d$ are algebraically independent over $K$, $f_v(a_1,\dots,a_d)\neq 0$. Then 
    $$g := f(a_1,\dots,a_d,\mathbf{y}) = \sum_{v\in\N^\delta} f_v(a_1,\dots,a_d)\mathbf{y}^v\in L[\mathbf{y}]\setminus 0.$$
    Since $b_1,\dots,b_\delta$ are algebraically independent over $L$, it follows that 
    $$f(a_1,\dots,a_d,b_1,\dots,b_\delta) = g(b_1,\dots,b_\delta)\neq 0,$$
    hence $a_1,\dots,a_d,b_1,\dots,b_\delta$ are algebraically independent over $K$. Hence $X\sqcup Y$ is a transcendence basis for $M$ over $K$, hence 
    $$\trdeg_K\ M = d+\delta = \trdeg_K\ L + \trdeg_L \ M.$$
\end{proof}
\begin{lemma}\label{AlgebraicFuntionFieldsInOneVariable}
    Let $L\supset K$ be an algebraic function field in one variable with $K$ algebraically closed. Let $a\in L\setminus K$. Then 
    \begin{enumerate}
        \item $L\supset K(a)$ is algebraic
        \item Suppose $\Char\ K =0$. Then there is a $b\in L$ such that $L=K(a,b)$.
        \item Consider an integral domain $R$ with $Q(R)=L$, $K\subset R$ algebraically closed. Suppose there is a non-trivial prime ideal $I\subset R$. Then $\sigma: K\rightarrow R/I, a\mapsto a+ I$ is an isomorphism.
    \end{enumerate}
\end{lemma}
\begin{proof}
    1. $L$ is algebraic over $K(t)$ for some $t\in L$. Then $a$ is algebraic over $K(t)$, hence we may find $f\in K[x,y]\setminus 0$ such that $f(a,t)=0$. Note that since $a\notin K$, $a$ cannot be algebraic over $K$ (Lemma~\ref{TechnicalLemmaAboutFieldExtOverAlgClosedFields}). Then $\deg_y \ f >0$, hence $g=f(a,y)\neq 0$ is polynomial that vanishes on $t$, hence $K(a,t)\supset K(a)$ is algebraic and since $L\supset K(a,t)$ is algebraic, it follows that $L\supset K(a)$ is algebraic.\\
    2. Since $L\supset K(a)$ is algebraic it is finite, hence by the Theorem of the Primitive Element $L=K(a,b)$ for some $b$.\\
    3. We prove the contrapositive: Let $I\supset R$ be any prime ideal. Suppose $\sigma: K\hookrightarrow R/I$ is not surjective. Pick $a\in R$ such that $a+I\in R/I$ is not in $K$ and pick $b\in I$. Note that $b=0$ or $b\in R\setminus K$, since otherwise $1\in I$. Then since $L\supset K(a)$ is algebraic, we can find a $f=\sum_0^d g_i(a)y^i\in K[a][y]\setminus 0$ of minimal degree such that $f(b)=0$. By Lemma~\ref{TechnicalLemmaAboutFieldExtOverAlgClosedFields} $g_0=0$ or $g_0(a)\neq 0$. In the first case we clearly have that $b=0$, since then $f=y(\sum_1^d g_i(a)y^{i-1})$, hence $f=y$ by minimality. In the second case $f_0(a)\in I$, hence $a+I$ is integral over $K$ which would imply that $a+I\in K$ by Lemma~\ref{TechnicalLemmaAboutFieldExtOverAlgClosedFields}, leading to a contradiction. Since $b$ was chosen arbitrarily it follows that $I=0$.
\end{proof}
\subsubsection{Graph Ideals \& Algebraic Dependence of Polynomials} 
\begin{definition}
    Consider a ring $R$. The \textit{graph ideal} for polynomials $f_1,\dots,f_m\in R[x_1,\dots,x_n]$ is defined to be ideal $\langle y_1-f_m,\dots,y_m-f_m\rangle \subset R[\mathbf{x},y_1,\dots,y_m]$
\end{definition}
\begin{remark}
    Note that the graph ideal of $f_1,\dots,f_m$ is just the point ideal (cf. Proposition~\ref{AZeroIffPolynomialIsInPointIdeal}) of $(x_1,\dots,x_n,f_1,\dots,f_m) \in R[\mathbf{x},\mathbf{y}]^{n+m}$. Hence a polynomial $f\in K[\mathbf{x},\mathbf{y}]$ is in the graph ideal of $f_1,\dots,f_m$ if and only if $f(\mathbf{x},f_1,\dots,f_m)=0$.
\end{remark}
\begin{lemma}\label{GraphIdealIsProperIdeal}
   Let $K$ be a field. Consider $f_1,\dots,f_m\in K[x_1,\dots,x_n]$ and denote the graph ideal of $f_1,\dots,f_m$ by $I$. Then $I$ is a proper ideal by Lemma~\ref{IdealIsNonProperIff1IsInIdeal} 
\end{lemma}
\begin{proof}
    Since $1$ doesn't vanish on $(\mathbf{x},f_1,\dots,f_m)$ it follows that $1\notin I$. Hence $I$ is proper by Lemma~\ref{IdealIsNonProperIff1IsInIdeal}
\end{proof}
\begin{proposition}
    Consider $f_1,\dots,f_{n+1}\in K[x_1,\dots,x_n]$ and let $I$ denote the graph ideal in $K[\mathbf{x},y_1,\dots,y_{n+1}]$ for these polynomials. Let $G\subset K[\mathbf{x},\mathbf{y}]$ be a Gröbner basis for $I$ with respect to the lexicographic term order with $x_1>\dots>x_n>y_1>\dots>y_{n+1}$. Then there is a non-zero polynomial $g\in G\cap K[\mathbf{x}]$ such that $g(f_1,\dots,f_{n+1})=0$.   
\end{proposition}
\begin{proof}
    By Example~\ref{TranscendenceDegreeOfQuotientFieldOfPolynomialRing} there is a polynomial $h\in K[\mathbf{y}]\setminus 0$ such that $h(f_1,\dots,f_{n+1})=0$. Then $h\in I$ and in particular $h\in I\cap K[\mathbf{y}]$. By Proposition~\ref{GBElimTheorem}, $G':= G\cap K[\mathbf{y}]$ is a Gröbner basis for $I\cap K[\mathbf{y}]$ with respect to the lexicographic term order with $y_1<\dots< y_{n+1}$. Then $h^{G'} = 0$ by Proposition~\ref{GBCriterionForCheckingMembership}, meaning $G'$ must contain a non-zero polynomial $g$. Since $g\in I$ we again by Lemma~\ref{GBElimTheorem} get that $g(f_1,\dots,f_{n+1}) = 0$. 
\end{proof}
\subsubsection{Finite Algebra Homomorphisms}
    \begin{definition}
        Let $S,T$ be $R$-algebras. An $R$-algebra homomorphism, $\sigma: S\rightarrow T$ is called \textit{finite} if $T\supset \sigma(S)$ is module finite.   
    \end{definition}
    \begin{lemma}\label{CompositionOfFiniteAlgebraHomsIsFinite}
        Let $S,T,Q$ be $R$-algebras and $\sigma: S\rightarrow T$ and $\omega: T\rightarrow Q$ be finite $R$-algebra homomorphisms. Then $\omega\sigma : S\rightarrow Q$ is finite. 
    \end{lemma}
    \begin{proof}
        For some  $t_1,\dots,t_m\in T$ and $q_1,\dots,q_n\in Q$  we have $S=\sum_1^m \sigma(S)t_i$ and $Q=\sum_1^k \omega(T)q_i$. We Then get that 
        \begin{align*}
            = \sum_1^k \omega(T)q_i = \sum_1^k \omega\left(\sum_1^m \sigma(S)t_j\right)q_i =\sum_1^k\sum_1^m (\omega\circ\sigma)\left(S\right)\omega(t_j)q_i,  
        \end{align*}
        hence $Q$ is a finitely generated over $(\alpha\circ \beta)(R)$ with generators 
        $$\omega(t_j)q_i,\ (1\leq i\leq k, 1\leq j\leq m).$$
        Therefor, we can conclude that $\omega\circ \sigma$ is finite. 
    \end{proof}
    \begin{lemma}\label{SurjectiveAlgebraHomomorphismIsFinite}
        Let $S,T$ be $R$-algebras and $\sigma : S\rightarrow T$ be a surjective $R$-algebra homomorphism. Then $\sigma$ is finite. 
    \end{lemma}
    \begin{proof}
        Trivial since $T = \sigma(S)$.
    \end{proof}
\subsubsection{Perron's Theorem of Effective Algebraic Dependence of Polynomials}
    \begin{lemma}\label{Satz58}
        Let $K$ be any field and $d_1,\dots,d_n>0$, $\pazocal{S}\subset \N^n$ containing $d_ie_i\in \pazocal{S}$ for each $i$. Set $L := K(y_v^{[i]} : v\in \pazocal{S})= Q(K[y_v^{[i]} : v \in \pazocal{S})$. For each $i\in \{1,\dots,n\}$, set 
        $$g_i := \sum_{v\in\pazocal{S}} y_v^{[i]}\mathbf{x}^v \in L[x_1,\dots,x_n]$$
        and $d_i := \deg\ g_i$
        Let $N\geq 0$ be given. Define $\Delta := \{ v\in \N^n : \vert v\vert \leq N\}$. Then 
        $$B := \{ g_1^{q_1}\cdots g_n^{q_n}x_1^{r_1}\cdots x_n^{r_n} : 0\leq r_i<d_i, \sum_1^n q_id_i+r_i \leq N\}$$
        is a basis for $L[\mathbf{x}]_{\leq N}$ over $K$.
    \end{lemma}
    \begin{proof}
        For each $v=(v_1,\dots,v_n)\in \Delta$ there are unique pair of tuples $$(q_1(v_1),\dots,q_n(v_n)),(r_1(v_1),\dots,r_n(v_n))\in \N^n$$
        such that for each $i\in\{1,\dots,n\}$, $0\leq r_i < d_i$ and $v=(q_1d_1+r_1,\dots,q_nd_n+r_n)$. We $\nabla = \{ (q_1,\dots,q_n),(r_1,\dots,r_n)\in \N^n : 0\leq r_i<d_i, \sum_1^n (q_id_i+r_i) \leq N\}$. We thus have that 
        \begin{gather*}
            (q,r): \Delta \rightarrow \nabla\\
            v=(v_1,\dots,v_n)\mapsto ((q_1(v_1),\dots,q_n(v_n)),(r_1(v_1),\dots,r_n(v_n)))
        \end{gather*}
        defines a bijection. We define for each $v\in \Delta$, 
        $$\Lambda_v := \Lambda_{q(v),r(v)} := \left(\prod_1^n g_i^{r_i(v_i)}\right)\left(\prod_1^n x_i^{r_i(v_i)}\right)\in K[\mathbf{x}][y_v^{[i]} : v\in \pazocal{S}].$$
        We thus have that $B=\{\Lambda_v : v\in \Delta\}$. Note that $\deg \ \Lambda_v = \vert v\vert$ for each $v\in \Delta$, which for one means that $\Lambda_v\in L[\mathbf{x}]_{\leq N}$.  Let $\sigma : K[y_v^{[i]} : v\in  \pazocal{S}]\rightarrow K$ be the unique $K$-algebra homomorphism defined by $y_v^{[i]}\mapsto 0$ when $(v,i)\neq (d_1e_1,1)\dots,(d_ne_n,n)$  and $y_{d_ie_i}^{[i]}\mapsto 1$. This map naturally extends to a $K[\mathbf{x}]$-algebra homomorphism which we also denote $\sigma$. Then $\sigma(g_i) = \mathbf{x_i}^{d_i}$ and $\sigma(\mathbf{x}^v)=\mathbf{x}^v$, hence 
        \begin{equation}\label{LambdaMapToRightBasisElement} 
            \sigma(\Lambda_v)=x_1^{q_1(v_1)d_1+r_1(v_1)}\cdots x_n^{q_n(v_n)d_n+r_n(v_n)}=\mathbf{x}^v.
        \end{equation}
        Write for each $v\in \Delta$
        $$\Lambda_v= \sum_{v\in \Delta} c_{vw}\mathbf{x}^w.$$
        By (\ref{LambdaMapToRightBasisElement}) 
        $$
            \sigma(c_{vw})=
            \begin{cases}
                1 & \text{if } w=v\\
                0 & \text{if } w \neq v
            \end{cases}
        $$
        Let $D$ denote $\# \Delta = \frac{N(N+1)}{2}$. $\sigma$ naturally induces a homomorphism 
        $$\sigma:M_{D}(K[y_v : v\in\pazocal{S}]) \rightarrow M_D(K)\subset M_D(K[y_v : v\in\pazocal{S}])$$ 
        defined by entry-wise application for which $\sigma((c_{vw}))= (\sigma(c_{vw}))=(e_{vw})=I_D$. Set $\pazocal{V}$ to be equal to $\{\mathbf{x}^v : v\in \Delta\}$; i.e. the standard basis for $L[\mathbf{x}]_{\leq N}$ over $L$. Then $_\pazocal{V}T_B = (c_{vw})\in M_D([y_v : v\bigcup \pazocal{S}_i])$. Moreover, 
        $$\sigma(\det\ _\pazocal{V}T_B)=\det\ \sigma(_\pazocal{V}T_B)=\det \ I_D = 1\neq 0\implies \det \ _\pazocal{V}T_B\neq 0.$$
        This means $_\pazocal{V}T_B$ is invertible in $M_D(L)$, hence $B$ is a basis by Theorem~\ref{BasisTransformationTheorem}.
    \end{proof}
    \begin{remark}
        In the above setup we can therefor given any $f\in K[\mathbf{x}]$ find a family of polynomials 
        $$f_{r_1,\dots,r_n}\in L[z_1,\dots,z_n] \quad (0\leq r_i < d_i),$$
        such that
        \begin{align*}
            &\text{1.}\ f = \sum_{r_1,\dots,r_n} f_{r_1,\dots,r_n}(g_1,\dots,g_n)x_1^{r_1}\cdots x_n^{r_n}\\
            &\text{2.}\ \deg\ f_{r_1,\dots,r_n}(z_1^{d_1},\dots,z_n^{d_n})+\sum_1^n r_i\leq \deg\ f.
        \end{align*}
        Indeed set $N:=\deg f$ and write 
        $$
            f = \sum_{v\in \Delta} a_v\Lambda_v = \sum_{(r_1,\dots,r_n)\in \N^n: r_i<d_i} \left[\sum_{(q_1,\dots,q_n): \sum_1^n (q_id_i+r_i)\leq N} a_{(q_1d_1+r_1,\dots,q_nd_n+r_n)}g_1^{q_1}\cdots g_n^{q_n}\right]x_1^{r_1}\cdots x_n^{r_n}.
        $$
        Setting 
        $$
            f_{r_1,\dots,r_n} := \sum_{(q_1,\dots,q_n)\in \N^n : \sum_1^n (q_id_i+r_i)\leq N} a_{(q_1d_1+r_1,\dots,q_nd_n+r_n)} z_1^{q_1}\cdots z_n^{q_n} \quad (0\leq r_i< d_i)
        $$
        These polynomials will satisfy property 1. Secondly,
        \begin{align*}
            \deg\ f_{r_1,\dots,r_n}(z_1^{d_1},\dots,z_n^{d_n})+\sum_1^n r_i &\leq \max_{(q_1,\dots,q_n) : \sum_1^n (q_id_i+r_i)} \ \deg \ z_1^{q_1d_1}\cdots z_n^{q_nd_n} + \sum_1^n r_i\\
            &= \max_{(q_1,\dots,q_n) : \sum_1^n (q_id_i+r_i)} \sum_1^n (q_id_i+r_i) \leq N = \deg \ f.
        \end{align*}
    \end{remark}
    \begin{lemma}\label{PerronInSpecialCase}
        Let $K$ be any field and $d_1,\dots,d_n>0$, $\pazocal{S}\subset \N^n$ containing $d_ie_i\in \pazocal{S}$ for each $i$. Set $L := K(y_v^{[i]} : v\in \pazocal{S})= Q(K[y_v^{[i]} : v \in \pazocal{S})$. For each $i\in \{1,\dots,n\}$, set 
        $$g_i := \sum_{v\in\pazocal{S}} y_v^{[i]}\mathbf{x}^v \in L[x_1,\dots,x_n]$$
        and $d_i := \deg\ g_i$ Then for every $g_{n+1}\in L[\mathbf{x}]$ with $d_{n+1} := \deg \ g_{n+1}$ there is polynomial $P\in L[z_1,\dots,z_{n+1}]$ that is monic in $L[z_1,\dots,z_n][z_{n+1}]$ satisfying 
            \begin{align*}
                &\text{1. } P(g_1,\dots,g_{n+1})=0\\
                &\text{2. } \deg \ P(z_1^{d_1},\dots,z_{n+1}^{d_{n+1}}) \leq \prod_1^{n+1}d_i.
            \end{align*}    
    \end{lemma}
    \begin{proof}
        There is $d:= \prod_1^n d_i$ elements in $\Omega=\{v\in \N^n : v_i<d_i\}$. Denote the elements in $\{\mathbf{x}^v : v\in \Omega\}=\{M_1,\dots,M_{d}\}$. Then by Lemma~\ref{Satz58} for each $i\in\{1,\dots,d\}$ there are polynomials 
        $$P_{ij}\in L[\mathbf{x}]\quad (j\in\{1,\dots d\})$$
        such that 
        \begin{align*}
            &\text{a. } M_ig_{n+1}=\sum_1^d P_{ij}(g_1,\dots,g_n)M_i\\
            &\text{b. } \deg\ P_{ij}(z_1^{d_1},\dots,z_n^{d_n}) + \deg M_i\leq \deg \ g_{n+1}M_i = d_{n+1}+\deg \ M_i.
        \end{align*}
        Property a. shows that 
        $$
            g_{n+1}
            \begin{pmatrix}
                M_1\\ \vdots\\ M_d
            \end{pmatrix} 
            =
            (P_{ij}(g_1,\dots,g_n)) \begin{pmatrix}
                M_1\\ \vdots\\ M_d
            \end{pmatrix}, 
        $$
        I.e. $g_{n+1}$ is an eigenvalue of $(P_{ij}(g_1,\dots,g_n))$, hence by Cramer's rule,  
        $$\det(P_{ij}(g_1,\dots,g_n)-\delta_{ij}g_{n+1}) =  0$$. Then $P: = (-1)^d\det(P_{ij}-\delta_{ij}g_{n+1})= \sum_{\pi\in S_d} \prod_1^d (P_{i\pi(i)}-\delta_{ij}g_{n+1})\in L[\mathbf{x}]\setminus 0$ satisfies $P(g_1,\dots,g_{n+1})=0.$ From b. we find that 
        $$\deg \ P_{ij}(z_1^{d_1},\dots,z_n^{d_n}) - \delta_{ij}z_{n+1}^{d_{n+1}}\leq d_{n+1}+\deg \ M_i - \deg \ M_j.$$
        For an arbitrary permutation $\pi \in S_d$,
        \begin{align*}
            \deg \ \prod_1^d (P_{i\pi(i)}(z_1^{d_1},\dots,z_d^{d_n})-\delta_{i\pi(i)}z_{n+1}^{d_{n+1}})&\leq \sum_1^d (d_{n+1} + \deg\ M_{\pi(i)}-\deg \ M_{i})\\
            &= \sum_1^d d_{n+1} + \sum_1^d \deg \ M_i -\sum_1^d\deg \ M_i\\ &= d d_{n+1} = \prod_1^{n+1} d_i. 
        \end{align*}
    \end{proof}
    \begin{lemma}
        Let $K$ be some field. For $f_1,\dots,f_n\in K[x_1,\dots,x_n]$ with $d_i := \deg\ f_i>0$, and 
        $$f_i = \sum_{v\in \N^n} a_v^{[i]}\mathbf{x}^v.$$
        Define $L:= K\left( y_v^{[i]} : v\in \N^n, a_v^{[i]}\neq 0 \text{ or } v=d_ie_i\right)$ and set $Y:=\left\{y_v^{[i]} : v\in \N^n, a_v^{[i]}\neq 0 \text{ or } v=d_ie_i\right\}$. Lastly define 
        $$g_i := \sum_{v\in \N^n} y_v^{[i]}\mathbf{x}^v$$
        for every $i$. Then there is a $K[\mathbf{x}]$-algebra homomorphism $\sigma: K[Y][\mathbf{x}]\rightarrow K[\mathbf{x}]$ such that $\sigma(g_i)=f_i$
    \end{lemma}
    \begin{proof}
        Indeed, take $\sigma$ to be the $K$-algebra homomorphism such that $y_v^{[i]}\mapsto a_v^{[i]}$. This trivially extends to a $K[\mathbf{x}]$-algebra homomorphism. 
    \end{proof}
    \begin{theorem}(Perron's Theorem) Let $K$ be any field and let $f_1,\dots,f_{n+1}\in K[x_1,\dots,x_{n}]$ and put $d_i := \deg \ f_i$ for each $i$. Then there is a $P\in K[z_1,\dots,z_{n+1}]\setminus 0$ satisfying 
    \begin{align*}
        &\text{1. } P(f_1,\dots,f_{n+1}) = 0,\\
        &\text{2. } \deg \ P(z_1^{d_1},\dots,z_{n+1}^{d_{n+1}}) \leq \prod_1^{n+1} d_i.
    \end{align*}
    \end{theorem}
    \begin{proof}
        First a slight reformulation. Let $M$ be some field and consider $H=\{h_1,\dots,h_{n+1}\} \subset M[x_1,\dots,x_n]$, $\delta_i:=\deg\ h_i$. Set $$\Delta_{\delta_1,\dots,\delta_{n+1}}:=\left\{ v\in \N^{n+1} : \sum_1^{n+1} v_i\delta_i \leq \prod_1^{n+1} \delta_i\right\}.$$
        Then define 
        $$B(H) := \left\{ h_1^{q_1}\cdots h_{n+1}^{q_{n+1}} : (q_1,\dots,q_{n+1})\in \Delta_{\delta_1,\dots,\delta_{n+1}}\right\}.$$
        And let $\pazocal{V}$ be the standard basis of $\{\mathbf{z}^v : v\in \Delta_{\delta_1,\dots,\delta_n}$. If $_\pazocal{V}T_{B(H)}$ is not invertible if and only if for suitable $a_v\in M$
        $$\sum_{v\in \Delta } a_vh_1^{v_1}\cdots h_{n+1}^{v_{n+1}} = \ev_{h_1,\dots,h_{n+1}}\left(\underbrace{\sum_{v\in\Delta} a_v\mathbf{z}^v}_{P_H}\right),$$
        where 
        \begin{align*}
        \deg \ P_H(z_1^{\delta_1},\dots,z_{n+1}^{\delta_{n+1}}) &\leq \max_{v\in \Delta} \deg \ \mathbf{z}^{(\delta_1v_1,\dots,\delta_{n+1}v_{n+1})} = \max_{v\in\Delta}\ \sum_1^{n+1} \delta_iv_i \leq \prod_1^{n+1}\delta_i.
        \end{align*}
        Set $F=\{f_1,\dots,f_{n+1}\}$ and $B := \{\mathbf{z}^v : v\in \Delta_{d_1,\dots,d_{n+1}}\}$. To prove the theorem we can equivalently prove that $\det \ _BT_{B(F)}=0$. With this in mind, we proceed with the proof of the theorem. Write $f_i = \sum_{v\in\N^n} a_v\mathbf{x}^v$. Define $L$, $g_i$ and $\sigma$ as in the prior lemma. By Lemma~\ref{PerronInSpecialCase}, there is a $Q\in L[\mathbf{x}]\setminus 0$ such that 
        \begin{align*}
            &\text{1. } Q(g_1,\dots,g_n,f_{n+1}) =0,\\
            &\text{2. } \deg \ Q(z_1^{d_1},\dots,z_{n+1}^{d_{n+1}}) \leq \prod_1^{n+1} d_i.
        \end{align*}
        Set $G := \{g_1,\dots,g_n,f_{n+1}\}$. By {\Large small easy lemma} $\sigma(_BT_{B(G)}) = {_B}T_{B(F)}$. It thus follows that 
        $$\det \ {_B}T_{B(F)} = \det \ \sigma(_BT_{B(G)}) = \sigma(\det\ _BT_{B(G)})= \sigma(0)=0.$$ 
    \end{proof}
\subsubsection{Noether Normalizations}
    \begin{lemma}\label{MainTechnicalLemmaForNNT}
        Let $K$ be a field and let $f=\sum_{v\in \N^n} a_v\mathbf{x}^v\in K[x_1,\dots,x_n]$ be given with $d :=\deg \ f >0$. Then we have the following
        \begin{enumerate}
            \item There are elements $y_1,\dots,y_{n-1}\in K[x_1,\dots,x_n]$  such that $x_i = y_i + x_n^{r_i}$ for $i\in\{1,\dots,n-1\}$ for suitable $r_i>0$ and
            $$f= ax_n^m +\sum_i^{m-1} G_ix_n^i,$$
            for some $a\in K\setminus \{0\}$, $m>0$ and $G_i\in  K[y_1,\dots,y_{n-1}]$.
            \item If $\# K =\infty$ we get the same result as in (a) with $x_i = y_i+a_ix_n$ for
        \end{enumerate}
    \end{lemma}
    \begin{proof}
    1. Set $k = d+1$ and put $r_i = k^i$ for $i\in\{1,\dots, n-1\}$. Then for $v,w\in \N^n$ with $\vert v\vert ,\vert w\vert \leq d$  and $v \neq w$ we get 
    $$v_n+\sum_1^{n-1} v_ir_i = v_n+\sum_1^{n-1} v_ik^i \neq w_n+\sum_1^k w_ik^i = w_n+\sum_1^n w_ir_i,$$
    by the uniqueness of $k$-adic expansions. This mean that we can define\\ $m:= \max_{v\in\N^n : \vert v\vert \leq d, a_v\neq 0} \left\{v_n + \sum_1^{n-1} v_i r_i\right\}$, $v_m = \text{argmax}_{v\in\N^n : \vert v\vert \leq d, a_v\neq 0} \left\{v_n + \sum_1^{n-1} v_i r_i\right\} $. We thus pick $y_i = x_i-x_n^{r_i}$ and see that 
    \begin{align*} 
        f &=f\left(y_1+x_n^{r_1},\dots, y_{n-1}+x_n^{r_{n-1}}, x_n\right) = \sum_{v\in \N^n : \vert v\vert \leq d} 
        a_v\left(\prod_1^{n-1}\left( y_i + x_n^{r_i}\right)^{v_i} \right) x_n^{v_n}\\ 
        &= \sum_{v\in \N^n : \vert v\vert \leq d} a_v\left(x_n^{v_n+\sum_1^{n-1} v_i r_i} + \underbrace{\dots}_{\text{lower degree terms}}\right)
        = \sum_{v\in \N^n : \vert v\vert \leq d} a_v x_n^{v_n+\sum_1^{n-1} v_i r_i} + \underbrace{\dots}_{\text{lower degree terms}}\\  &= a_{v_m}x_n^m + \underbrace{\dots}_{\text{lower degree terms}}.
    \end{align*}
    We can write the lower order terms on the form $\sum_1^{m-1} G_ix_n^i$ for suitable $G_i\in K[y_1,\dots,y_{n-1}]$.\\
    2. Write $f= \sum_0^d f_i$ with $f_d \neq 0$ for homogeneous $f_i\in K[x_1,\dots,x_n]$ of degree $i$. By Lemma~\ref{TechnicalLemmaForNoetherLemma}, $f_d(x_1,\dots,x_{n-1},1) \neq 0$. Since $\# K = \infty$ we get that there are $a_1,\dots, a_{n-1} \in K$ such that $f_d(a_1,\dots, a_{n-1},1) \neq 0$. We now pick $y_i = x_i-a_ix_i$ for $i\in\{1,\dots,n-1\}$. 
    Note that 
    $$\sum_{v\in \N^n : \vert v \vert = d} a_v(a_1x_n)^{v_1}\cdots (a_{n-1}x_n)^{v_{n-1}}x_n^{v_n} = \left[\sum_{v\in \N^n : \vert v \vert = d} a_va_1^{v_1}\cdots a_{n-1}^{v_{n-1}}\cdot1^{v_n}\right] x_n^d=f_d(a_1,\dots,a_{n-1},1)x_n^d. $$
    From this it follows that 
    \begin{align*}
        f &= f(y_1+a_1x_n,\dots,y_{n-1}+a_{n-1}x_n,x_n)\\
        &= \sum_{v\in \N^n : \vert v \vert = d} a_v(y_1+a_1x_n)^{v_1}\cdots (y_{n-1}+a_{n-1}x_n)^{v_{n-1}}x_n^{v_n} + \sum_1^{m-1}\sigma(F_i)\\
        &\overset{(\ast)}{=} \left[\sum_{v\in \N^n : \vert v\vert = d} a_va_1^{v_1}\cdots a_{n-1}^{v_{n-1}}\cdot 1^{v_n}\right]x_n^d + \dots=f_d(a_1,\dots,a_{n-1},1)x_n^d+\dots.
    \end{align*}
    We then set $a = f_d(a_1,\dots,a_{n-1},1)$. The $\dots$ in the expressions following $(\ast)$ in the above signify remaining terms. One readily verifies that these have $x_n$-degree strictly smaller than $d$. Again these terms can clearly be written on the form $\sum_1^{d-1}G_ix_n^i$ for suitable $G_i\in K[y_1,\dots,y_{n-1}]$. 
    \end{proof}
    \begin{theorem}\label{NoetherNormalizationLemma}
        (Noether Normalization Theorem)  Let $A = K[x_1,\dots,x_n]/J$ for some field $K$ and some ideal $J \subsetneq K[x_1,\dots,x_n]$. Let also $I\subsetneq A$ be an ideal. 
        \begin{enumerate}[(a)] 
            \item Then there are elements $y_1,\dots, y_d\in A$, which are algebraically independent such that $A$ is a finitely generated $K[y_1,\dots,y_d]$-module. Furthermore, for some $\delta \leq d$, $I\cap K[y_1,\dots,y_d] = \langle y_{\delta+1},\dots,y_d \rangle$. 
            \item In addition if $\#K = \infty$, we have that $y_i = \sum_{j=i}^n a_{ij}x_j$ for suitable $a_{ij} \in K$.
        \end{enumerate}
    \end{theorem}
    \begin{proof}
        1. \textbf{case 1:} We first consider the case where $A= K[\mathbf{x}]$ and $I=\langle f\rangle$ for some $f\in A$ with $\deg\ f>0$. We put $y_n = f$ and apply Lemma~\ref{MainTechnicalLemmaForNNT} 1. to obtain $y_i=x_i-x_n^{r_i}\in A$ for suitable $r_i>0$ for $i\in\{1,\dots,n-1\}$, such that
        $$y_n=f=ax_n^m+\sum_1^{m-1} G_i(y_1,\dots,y_{n-1})x_n^i \iff y_n-ax_n^m+\sum_1^{m-1} G_i(y_1,\dots,y_{n-1})x_n^i=0,$$
        for some $a\in K\setminus\{0\}$, $m>0$, $G_i\in K[y_1,\dots,y_{n-1}]$. Then $x_n$ is integral over $K[y_1,\dots,y_{n}]$. Since $x_i=y_i+x_n^{r_i}\in A$ we get that $A=K[y_1,\dots,y_n][x_n]$, hence $A$ is a finitely generated $K[y_1,\dots,y_n]$-module.\\
        We now claim that $y_1,\dots, y_n$ are algebraically independent over $K$. Suppose for contradiction that this is not the case. Then $Y:=\left\{y_1,\dots,y_n\right\}$ is not a transcendence basis of $K(y_1,\dots,y_n)$. However, by Lemma~\ref{TechnicalTranscendenceBasisLemma} (a) there is a subset of $Y$, say $y_{l_1},\dots y_{l_k}$ for $k<n$, which constitutes a transcendence basis for $K(y_1,\dots,y_n)$. Then by Corollary~\ref{TrcBasisExtendsWhenExtFinGen} 
        $$
            k=\trdeg K(y_1,\dots,y_n)=\trdeg K(x_1,\dots,x_n)=n>k,
        $$ 
        leading to a contradiction.\\
        Let $\lambda\in I\cap K[y_1,\dots,y_n]$. Then $\lambda = g f=g y_n$ for some $g\in A$. $g$ is integral over $K[y_1,\dots,y_n]$, hence for suitable $h_1,\dots,h_{k-1}\in K[y_1,\dots,y_n]$,
        $$g^k+\sum_{i=1}^{k-1}h_ig^i=0\implies \lambda^k=f^kg^k=-\sum_{i=1}^{k-1}h_if^kg^i=-\sum_{i=1}^{k-1}h_i\lambda^iy_n^{k-i}.$$
        This means $y_n \mid \lambda^k$, implying $y_n \mid \lambda$. From this we conclude $I\cap K[y_1,\dots,y_n] =\langle y_n \rangle$.\\

        \noindent\textbf{Case 2:} We now prove the statement for $A = K[x_1,\dots,x_n]$ and an arbitrary ideal $I\subsetneq A$. For $I=0$, we are done after choosing $y_i = x_i$ and $\delta = n$. We prove the statement for $I\neq 0$ by induction in $n\geq 1$. For $n=1$, $A$ is a PID, so $I$ is generated by some non-zero polynomial. Then the statement follows from case 1.\\ 
        Suppose now that $n>1$ and let $f\in I\setminus\{0\}$. Again using Lemma~\ref{MainTechnicalLemmaForNNT} we find $y_1,\dots,y_n\in A$ that are algebraically independent over $K$ with $y_n = f$. Then $y_1,\dots,y_{n-1}$ are also algebraically independent over $K$. By the induction hypothesis, we can find elements $t_1,\dots, t_{d-1} \in K[y_1,\dots, y_{n-1}]$ algebraically independent over $K$ such that $K[y_1,\dots,y_{n-1}]$ is a finitely generated $K[t_1,\dots,t_{d-1}]$-module and $I \cap K[t_1,\dots, t_{d-1}] = \left\langle t_{\delta+1},\dots, t_{d-1} \right\rangle$ for some $\delta <d$. We then get that $K[y_1,\dots, y_n]$ is a finitely generated $K[t_1,\dots, t_{d-1},y_n]$-module, and hence $A$ is a finitely generated $K[t_1,\dots, t_{d-1},y_n]$-module. Thus by a similar contradiction argument to that of case 1 {\Large I feel there is an argument that captures the fact better}, we conclude that $d=n$ and $t_1,\dots,t_{n-1},y_n$ are algebraically independent over $K$.\\ 
        Let $\lambda\in I\cap K[t_1,\dots,t_{n-1},y_n]$. Then $\lambda= g+hy_n$ for some $g\in I\cap K[t_1,\dots, t_{n-1}]=\langle t_{\delta+1},\dots,t_{n-1}\rangle$ and $h\in K[t_1,\dots, t_{n-1},y_n]$, then $I\cap K[t_1,\dots,t_{n-1},y_n]=\langle t_{\delta+1},\dots,t_{n-1},y_n\rangle$.\\

        \noindent \textbf{case 3:} We now generalize to the case where $A = K[x_1,\dots, x_n]/J$ and $I\subsetneq A$ for an ideal $J\subsetneq K[x_1,\dots,x_n]$. We apply case 2 to $J$ and find $y_1,\dots, y_n\in A$ algebraically independent over $K$ such that $K[x_1,\dots, x_n]$ is a finitely generated $K[y_1,\dots,y_n]$-module and $J\cap K[x_1,\dots,x_n]=\langle y_{d+1},\dots,y_{n}\rangle$ for some $d\leq n$. Consider the embedding $\iota : K[y_1,\dots, y_n] \hookrightarrow A$. By construction we have that $A$ is a finitely generated $\iota(K[y_1,\dots,y_n])$-module. It is easy to check that $$\iota(K[y_1,\dots,y_n])\simeq \frac{K[y_1,\dots,y_n]}{(J\cap K[y_1,\dots,y_n])} = \frac{K[y_1,\dots,y_n]}{\langle y_{d+1},\dots,y_n\rangle} \simeq K[y_1,\dots, y_d].$$  
        From which it follows that $A$ is a finitely generated $K[y_1,\dots, y_d]$-module.\\ 
        Let $I'=I\cap K[y_1,\dots,y_d]$. Then using case 2 we find $t_1,\dots,t_d\in K[y_1,\dots, y_d]$ algebraically independent over $K$ such that $K[y_1,\dots,y_d]$ is a finitely generated $K[t_1,\dots,t_d]$-module and $I'\cap K[t_1,\dots,t_d] = \langle t_{\delta +1},\dots, t_d\rangle$ for some $\delta \leq d$. It then also follows that $A$ is a finitely generated $K[t_1,\dots, t_d]$-module.\\ 
         
        \noindent 2.Suppose now that $\#K=\infty$. In case 1 the construction is also valid with $y_i =x_i-a_ix_n$ for suitable $a_i\in K$ by Lemma~\ref{MainTechnicalLemmaForNNT} 2.\\ 
        In case 2 we can choose $t_1,\dots, t_{n-1}$ and $y_1,\dots, y_{n-1}$ in the same way. In case 3 we can again choose $y_i = x_i-a_ix_n$ for suitable $a_i\in K$. It follows from case 2 that we can choose $$t_j = y_i-b_jy_d = x_i-x_d-(a_i-b_ja_d)x_n + J,$$ which is of the desired form.  
    \end{proof}
    \begin{definition}
        Let $A$ be a finitely generated $K$-algebra. A sequence of elements $y_1,\dots,y_d \in A$ with the properties specified in the above theorem is called a \textit{Noether normalization} of $A$. 
    \end{definition}
    \begin{corollary}\label{FinitelyGeneratedFieldAlgebrasHaveFiniteTranscendenceDegree}
        Consider $A = K[x_1,\dots,x_n]/I$, with $I\subset K[\mathbf{x}]$ a prime ideal.
        \begin{enumerate}
            \item  If $y_1,\dots,y_d$ is a Noether normalization of $A$, then $X=\{y_1,\dots, y_d\}$ defines a transcendence basis of $L:=Q(A)\supset K$.
            \item If $\trdeg_K\ Q(A)=d$, then $A$ has a Noether normalization $y_1,\dots, y_d$
        \end{enumerate} 
    \end{corollary}
    \begin{proof}
        1. By assumption $y_1,\dots,y_d$ are algebraically independent over $K$. Secondly $A$ is a finitely generated $K[\mathbf{y}]$-module, hence $A$ is integral over $K[\mathbf{y}]$. Then $L$ is integral over $K[\mathbf{y}]$. It follows that since $L\supset K(\mathbf{y})\supset K[\mathbf{y}]$ $K(\mathbf{y})$ must be algebraic {\LARGE Lemma not yet written}.\\
        2. NNT there is a Noether normalization $y_1,\dots,y_\delta \in A$. By 1. $\delta = \trdeg_K Q(A) = d$.
    \end{proof}
    \begin{corollary}\label{NoetherNormalizationGivesRiseToFiniteAlgebraHomomorphism}
        Let $A$ be finitely generated $K$-algebra and $y_1,\dots,y_d\in A$ be its Noether normalization. Then $\iota : K[y_1,\dots,y_d]\hookrightarrow A$ is finite $K$-algebra homomorphism.
    \end{corollary}